{"pages":[{"title":"About","text":"Blog SetupThis Blog is setup with Hexo and theam Icarus","link":"/about/index.html"}],"posts":[{"title":"VMware Centos7 Network Setup","text":"背景没想到搞虚拟机搞了这么久，终于配置成功了，不过踩坑也是难免的，记录一下。 目前要做的是搭建 Hadoop 集群环境，但是在配置多台 linux 虚拟机时网络一直没有弄好，远程无法访问，出现各种问题。主要原因是自己这块基础不牢，网上的资料也太杂。 更重要的是，我使用的是带 GUI 的 CentOS，导致了有两套 Network 管理，所以冲突了，尴尬。 环境 VMWare 12 CentOS7 主机 Win10 NAT 模式 NAT 模式因为采用 NAT 模式对虚拟机进行网络配置，所以需要将主机网络共享给虚拟机 NAT 网卡，如下： 启用之后, 则 VMnet8 网卡的配置要如下所示，固定 ip 网段为 192.168.137.* CentOS7 网络配置下面以克隆的一台机子为例（被克隆的机子为 hadoop00/192.168.137.100），配置新的网络。 Mac Address当刚克隆的时候，机子的 Mac 地址也是一样的，所以要在启动前先重新生成（00:50:56:24:A9:10 就是新生成的 mac 地址）： hostname原来机器的 hostname 是 hadoop00，所以我们需要使用hostnamectl set-hostname hadoop01将名字改成hadoop01 /etc/hosts下面是增加 hosts 配置，在之后建立集群时，需要将多台机子的 ip，hostname 信息都要添加进来。 123127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 hadoop01::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.137.101 hadoop01 修改网卡配置首先我们可以使用ifconfig先查看一下网卡信息（这是我已经配置好了），我们可以看到网卡的名字为eno16777736, Mac 地址和我们生成的是一致的。 1234567891011121314151617181920212223242526[root@hadoop01 ~]# ifconfigeno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.137.101 netmask 255.255.255.0 broadcast 192.168.137.255 inet6 fe80::250:56ff:fe24:a910 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:24:a9:10 txqueuelen 1000 (Ethernet) RX packets 405 bytes 45790 (44.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 245 bytes 42636 (41.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 0 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 ether 52:54:00:d8:7c:eb txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 所以我们需要配置/etc/sysconfig/network-scripts/ifcfg-eno16777736文件，这里便是核心的配置。 内容如下所示： 123456789101112131415161718DVICEE=eno16777736NAME=eno16777736TYPE=EthernetIPADDR=192.168.137.101 # ip地址PREFIX=24NETMASK=255.255.255.0 # 子网掩码NETWORK=192.168.137.0 # ip段GATEWAY=192.168.137.2 # 网关地址BROADCAST=192.168.137.255 # 广播地址, 网关地址最后一位换成255DEFROUTE=yesONBOOT=yesUSERCTL=yesBOOTPROTO=staticIPV4_FAILURE_FATAL=yesHWADDR=00:50:56:24:A9:10 # 这里填执行ifconfig命令后, ens33(这里名称可能不同)的mac地址IPV6INIT=noDNS1=8.8.8.8DNS2=114.114.114.114 之后有一些资料中说要删除/etc/udev/rules.d/70-persistent-ipoib.rules,但我查看过，里面是空的，可能是我使用的系统关系，想来不删除也没有关系的。 重启网卡服务在这里，一般使用: 1systemctl restart network.service 也可以用: 12service network stopservice network start 而我像开头说的，因为带了GUI，所以我这边还需要停掉一个NetworkManager服务，使用: 12service NetworkManager stopchkconfig NetworkManager off 此外，为了方便，防火墙可以关闭掉： 1234//临时关闭systemctl stop firewalld//禁止开机启动systemctl disable firewalld 这样子的话，下次开机，就能直接通过ssh访问了。 最后主要是自己不熟悉一些概念，所以遇到问题没有办法找到根源，只能有时间再好好啃啃鸟哥的私房菜了。 参考： VMware 虚拟机中 Centos7 网络配置及 ping 不通思路 RTNETLINK answers: File exists 错误解决方法 VMware 下克隆 centos7 后的网络配置及主机名问题 CentOS 7 开机 network service 不启动的问题 Centos 怎么关闭 NetworkManager 服务 VMware克隆CentOS7，解决网络配置问题","link":"/2019/01/09/2019/01/2019-01-09-VMLinuxNetworkSetup/"},{"title":"Setup Hadoop Cluster","text":"背景最近终于在虚拟上搭好了 Hadoop 的集群环境，记录一下。 资源准备 jdk-8u40-linux-x64.gz hadoop-2.7.3.tar.gz CentOS Linux release 7.4.1708 (Core) 四台机子如下，hadoop00 为mater, 其余为slave 1234192.168.137.100 hadoop00192.168.137.101 hadoop01192.168.137.102 hadoop02192.168.137.103 hadoop03 配置环境 jdk: /usr/local/apps/jdk1.8.0_40 hadoop: /usr/local/apps/hadoop-2.7.3 编辑/etc/profile, 将 jdk 和 hadoop 相关 key 添加到环境变量中。 12345678910111213141516export JAVA_HOME=/usr/local/apps/jdk1.8.0_40export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport HADOOP_HOME=/usr/local/apps/hadoop-2.7.3export HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/libexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;#export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_PREFIX/lib:$HADOOP_PREFIX/lib/native&quot;export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native 然后执行source /usr/profile 使配置生效。 创建为 Hadoop 之后使用的目录 123mkdir -p /usr/local/hadoop/tmpmkdir -p /usr/local/hadoop/hdfs/namemkdir -p /usr/local/haddop/data 设置 ssh 免密登录下面是生成密钥对的方式。生成公钥： 1ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 将公钥加入验证： 1cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys 尝试登录验证： 1ssh localhost 最后我们需要每一台都能互相访问，所以需要将所有的公钥加入authorized_keys 那我们就要每次都通过scp将私钥传递下去，最后再将最后的authorized_keys传回每一台： 123scp /root/.ssh/authorized_key hadoop00:/root/.sshscp /root/.ssh/authorized_key hadoop01:/root/.sshscp /root/.ssh/authorized_key hadoop02:/root/.ssh Hadoop 配置与运行Hadoop 的解压路径为 /usr/local/apps/hadoop/hadoop-2.7.3 在/usr/local/apps/hadoop/hadoop-2.7.3/hadoop/etc目录下有许多配置文件需要我们配置。 hadoop-env.sh主要是修改使用我们自己的 jdk 路径 1export JAVA_HOME=/usr/local/apps//jdk1.8.0_40 yarn-env.sh同样是修改 JAVA_HOME 路径： 1export JAVA_HOME=/usr/local/apps/jdk1.8.0_40 core-site.xml12345678910111213141516&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop00:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/apps/hadoop-2.7.3/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; --&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/apps/hadoop-2.7.3/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/apps/hadoop-2.7.3/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop00:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt; &lt;value&gt;hadoop00:10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml这个文件从 mapred-site.xml.template 拷过来改： 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop00:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop00:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop00&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop00:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop00:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;hadoop00:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop00:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop00:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;~ 格式化 namenode1hdfs namenode -format 启动将 HDFS 和 YARN 都起来： 1234# start HDFSstart-dfs.sh# start YARNstart-yarn.sh 然后使用 jps 查看 java 进程jps,可以看到我们想要的都起来了。 12345[root@hadoop00 hadoop]# jps9057 ResourceManager9318 Jps8631 NameNode8860 SecondaryNameNode 1234[root@hadoop01 ~]# jps7844 NodeManager7689 DataNode7978 Jps 通过网址访问如果启动成功，我们便能成功访问下列的网址： （注意）我们需要将防火墙关闭（当然也可以把用到的端口配置一下） http://hadoop00:8088 http://hadoop00:50070 最后目前搭的只是最基础的环境，后继用到其它组件时还需要继续添加。 参考：《Hadoop构建数据仓库实践》等","link":"/2019/01/15/2019/01/2019-01-15-SetupHadoopCluster/"},{"title":"Update CSV Column Values","text":"背景在一些 csv 文件中会有日期类型，而如果我们想按日期类型导入到数据库中，那么就需要在 insert 前以 date(datetime)的类型传入。 假设简单的 csv 文件(sample.csv)如下： 123456789HolidayDate,Region2019-01-01,China2019-01-02,China2019-04-01,China2019-05-01,China2019-07-07,China2019-10-01,China2019-01-05,US2019-06-01,US 直接使用 cvs 库读取到 csv 后，直接遍历完成： 12345678910111213141516import csvfrom datetime import datetimedef read_csv_rows_list(file_path): csv_row_list = [] with open(file_path, newline='') as csvfile: csvReader = csv.reader(csvfile, delimiter=',', quotechar='|') for row in csvReader: csv_row_list.append(row) return csv_row_listcsv_row_list = read_csv_rows_list('sample.csv')csv_row_list = csv_row_list[1:]for row in csv_row_list: row[0] = datetime.strptime(row[0], '%Y-%m-%d')[print(row) for row in csv_row_list] output: 12345678[datetime.datetime(2019, 1, 1, 0, 0), 'China'][datetime.datetime(2019, 1, 2, 0, 0), 'China'][datetime.datetime(2019, 4, 1, 0, 0), 'China'][datetime.datetime(2019, 5, 1, 0, 0), 'China'][datetime.datetime(2019, 7, 7, 0, 0), 'China'][datetime.datetime(2019, 10, 1, 0, 0), 'China'][datetime.datetime(2019, 1, 5, 0, 0), 'US'][datetime.datetime(2019, 6, 1, 0, 0), 'US'] 使用 pandas如果使用 pandas 就很方便： 1234567891011import pandas as pdimport numpy as npdata = pd.read_csv('sample.csv',encoding='utf-8',)data[u'HolidayDate'] = pd.to_datetime(data[u'HolidayDate'])# data[u'HolidayDate'].astype(str)# data[u'HolidayDate'] = data[u'HolidayDate'].apply(lambda x :datetime.strptime(x, '%Y-%m-%d'))data = np.array(data)#np.ndarray()data_list = data.tolist()[print(row) for row in data_list]#data.to_csv('sample.output.csv',index=False, encoding='utf-8') 12345678[Timestamp('2019-01-01 00:00:00'), 'China'][Timestamp('2019-01-02 00:00:00'), 'China'][Timestamp('2019-04-01 00:00:00'), 'China'][Timestamp('2019-05-01 00:00:00'), 'China'][Timestamp('2019-07-07 00:00:00'), 'China'][Timestamp('2019-10-01 00:00:00'), 'China'][Timestamp('2019-01-05 00:00:00'), 'US'][Timestamp('2019-06-01 00:00:00'), 'US'] 参考： pandas DataFrame数据转为list pandas处理时间和日期类型数据 Pandas修改csv文件某一列的值","link":"/2019/03/14/2019/03/2019-03-14-PyCSVColumns/"},{"title":"Convert EST&#x2F;EDT to GMT datetime","text":"背景最近在导数据的时候，需要把日期型字符串转换成 datetime 类型再导入数据库，其中一些带时区的数据，不能直接使用 strftime 格式化。 数据主要带 EST 和 EDT，需要先转成 GMT 再处理。 122019/05/14 03:20:10 EDT2019/05/14 03:20:10 EST 时区ST: Eastern Standard Time 东部（美国）标准时间 EDT: Eastern Daylight Time 东部（美国）夏令时时间 ET: Eastern Time 东部（美国）时间 GMT: Greenwich Mean Time 格林尼治标准时间 其中，EST 和 EDT 是 ET 在不同时间段的两种说法。 简单来说，ET 在夏季月份（summer months) 采用 EDT, 在其他月份采用 EST 时区，因此 EDT 和 EST 是不会同时存在的。 在美国，EST 时区的采用时间段：开始时间：当年 11 月份的第一个星期天凌晨 2.00（如 2018.11.4， 周日， 2.00）结束时间：次年 3 月份的第二个星期天凌晨 2.00（如 2019.3.10，周日， 2.00）其余时间采用 EDT 时区。 时区转换EDT 比 GMT 时间慢 5 个小时，即 EDT=GMT-5 EST 比 GMT 时间慢 4 个小时，即 EST=GMT-4 北京时区比 GMT 快 8 个小时，即北京时区=GMT+8 因此，可以得到： EDT 比北京时间慢 13 个小时，即 EDT=北京时间-13 EST 比北京时间慢 12 个小时，即 EST=北京时间-12 使用 pytzpytz 是 python 时间处理的一个模块，可以使用它进行时区的转换 1234567891011121314151617181920212223import timeimport datetime as dtimport pytzutc=pytz.utceastern=pytz.timezone('US/Eastern')format_without_tz ='%Y/%m/%d %H:%M:%S'format_with_tz = '%Y/%m/%d %H:%M:%S %Z%z'dt_time_str_full_list = ['2002/10/27 01:20:00 EDT', '2002/10/27 01:20:00 EST']for dt_time_str_full in dt_time_str_full_list: dt_time_str = dt_time_str_full[0:-4] dt_time_tz = dt_time_str_full[-3:] if dt_time_tz == 'EDT': is_dst_flag = True else: is_dst_flag = False dt_time = dt.datetime.strptime(dt_time_str, format_without_tz) dt_time_with_tz = eastern.localize(dt_time, is_dst=is_dst_flag) dt_time_with_tz2uct = dt_time_with_tz.astimezone(utc) print('{}=&gt;{}'.format(dt_time_with_tz.strftime(format_with_tz), dt_time_with_tz2uct.strftime(format_with_tz))) output: 122002/10/27 01:20:00 EDT-0400=&gt;2002/10/27 05:20:00 UTC+00002002/10/27 01:20:00 EST-0500=&gt;2002/10/27 06:20:00 UTC+0000 参考: EST，EDT 和 GMT 时区转换 格林尼治时间官网","link":"/2019/05/13/2019/05/2019-05-13-ESTEDT2GMT/"},{"title":"Compare Excel","text":"背景下周可能有一个小任务是由于系统升级，需要保证生成的 excel 是一致的，所以写了一个非常简单的对比脚本。 主要代码下面直接上代码，主要的思路就是从上到下依次对比，从 sheet name 开始，最后对比 cell 的值，数据类型，数据格式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# author: xiche# create at: 01/20/2019# description:# simple script to compare excel# Change log:# Date Author Version Description# 01/20/2019 xiche 1.0 init# 01/21/2019 xiche 1.1 add compare csvsimport openpyxlimport copyimport csvdef compare_cell(cell01, cell02): if(cell01.value != cell02.value): print('{}{}: {} / {}'.format(cell01.column, cell01.row, cell01.value, cell02.value)) elif(cell01.data_type != cell02.data_type): print('{}{}: {} / {}'.format(cell01.column, cell01.row, cell01.data_type, cell02.data_type)) elif(cell01.number_format != cell02.number_format): print('{}{}: {} / {}'.format(cell01.column, cell01.row, cell01.number_format, cell02.number_format))def compare_sheet(sheet01, sheet02): if(sheet01.max_row != sheet02.max_row): print('row is not matched! {} / {}'.format(sheet01.max_row, sheet02.max_row)) if(sheet01.max_column != sheet02.max_column): print('column is not matched! {} / {}'.format(sheet01.max_column, sheet02.max_column)) max_row = max(sheet01.max_row, sheet02.max_row) max_column = max(sheet01.max_column, sheet02.max_column) for index_row in range(max_row): for index_col in range(max_column): compare_cell(sheet01.cell(column=index_col+1, row=index_row+1), sheet02.cell(column=index_col+1, row=index_row+1))def compare_excels(excel_file01, excel_file02): print('------compare {} &amp; {}------'.format(excel_file01, excel_file02)) wb01 = openpyxl.load_workbook(excel_file01) wb02 = openpyxl.load_workbook(excel_file02) sheet_names_01 = wb01.get_sheet_names() sheet_names_02 = wb02.get_sheet_names() if(len(sheet_names_01) != len(sheet_names_02)): print('sheets number are not matched!') return for i in range(len(sheet_names_01)): if(len(sheet_names_01[i]) != len(sheet_names_02[i])): print('sheets name not matched!\\n{}:{} / {}:{}'.format(excel_file01, sheet_names_01[i], excel_file02, sheet_names_02[i])) return sheet01 = wb01.get_sheet_by_name(sheet_names_01[i]) sheet02 = wb02.get_sheet_by_name(sheet_names_02[i]) print('compare {}:{} &amp; {}:{}'.format(excel_file01, sheet01.title, excel_file02, sheet02.title)) compare_sheet(sheet01, sheet02)def compare_csv(csv_file01, csv_file02): print('------compare {} &amp; {}------'.format(csv_file01, csv_file02)) is_diff = False with open(csv_file01, newline='') as csv_01, open(csv_file02, newline='') as csv_02: csv_reader_01 = csv.reader(csv_01, delimiter=',', quotechar='|') csv_reader_02 = csv.reader(csv_02, delimiter=',', quotechar='|') csv_list_01 = list(csv_reader_01) csv_list_02 = list(csv_reader_02) ccsv_list_02_copy = copy.deepcopy(csv_list_02) for line in csv_list_01: if line not in csv_list_02: is_diff = True print('{}:{}'.format(csv_file01, line)) else: ccsv_list_02_copy.remove(line) for line in ccsv_list_02_copy: print('{}:{}'.format(csv_file02, line)) if not is_diff: print('No difference between these two csv') Compare Excel12345if __name__ == '__main__': compare_excels('test01.xlsx', 'test01.copy.xlsx') compare_excels('test01.xlsx', 'test02.xlsx') compare_excels('test01.xlsx', 'test03.xlsx') compare_excels('test01.xlsx', 'test04.xlsx') 目前的输出结果如下： 123456789101112131415161718192021222324C:\\Users\\mayn\\Desktop\\GitSpace\\PowerScript\\Python3\\openpyxl\\compare_excels&gt;python compare_excels.py------compare test01.xlsx &amp; test01.copy.xlsx------compare test01.xlsx:Sheet1 &amp; test01.copy.xlsx:Sheet1------compare test01.xlsx &amp; test02.xlsx------compare test01.xlsx:Sheet1 &amp; test02.xlsx:Sheet1row is not matched! 4 / 5B2: mm-dd-yy / [$-409]d\\-mmm\\-yy;@C4: Test03 / Test04B5: None / 10/21/2010C5: None / Test05------compare test01.xlsx &amp; test03.xlsx------sheets name not matched!test01.xlsx:Sheet1 / test03.xlsx:NotMatch------compare test01.xlsx &amp; test04.xlsx------compare test01.xlsx:Sheet1 &amp; test04.xlsx:sheet1row is not matched! 4 / 6column is not matched! 3 / 4B2: mm-dd-yy / [$-409]d\\-mmm\\-yy;@C4: Test03 / Test04B5: None / 10/21/2010C5: None / Test05B6: None / testC6: None / testD6: None / test Compare CSV123if __name__ == '__main__': compare_csv('demo01.csv', 'demo01.copy.csv') compare_csv('demo01.csv', 'demo02.csv') demo01.csv和demo02.csv有两行数据不一值： 123456c:\\Users\\mayn\\Desktop\\GitSpace\\PowerScript\\Python3\\openpyxl\\compare_excels&gt;python compare_excels.py------compare demo01.csv &amp; demo01.copy.csv------No difference between these two csv------compare demo01.csv &amp; demo02.csv------demo01.csv:['R1', ' R1', ' R1']demo02.csv:['Z5', ' R4', ' R4'] 最后不知道有没有第三方库直接提供对比的方法，自己的这个之后可以添加到自己的工具库中。 代码和测试的文件在：compare_excels","link":"/2019/01/20/2019/01/2019-01-20-CompareExcel/"},{"title":"Hadoop HDFS Operation","text":"背景之前搭好了 Hadoop 环境，但是在使用的过程中还是有一些问题，现在终于解决了，至少最基础的环境没有问题了。 基本环境Hadoop-2.7.3 / Java7 四台机子如下，hadoop00 为mater, 其余为 slave 1234192.168.137.100 hadoop00192.168.137.101 hadoop01192.168.137.102 hadoop02192.168.137.103 hadoop03 HDFS 的相关命令123456789101112131415-mkdir 在HDFS创建目录 hdfs dfs -mkdir /data-ls 查看当前目录 hdfs dfs -ls /-ls -R 查看目录与子目录-put 上传一个文件 hdfs dfs -put data.txt /data/input-moveFromLocal 上传一个文件，会删除本地文件：ctrl + X-copyFromLocal 上传一个文件，与put一样-copyToLocal 下载文件 hdfs dfs -copyToLocal /data/input/data.txt-get 下载文件 hdfs dfs -get /data/input/data.txt-rm 删除文件 hdfs dfs -rm /data/input/data.txt-getmerge 将目录所有的文件先合并，再下载-cp 拷贝： hdfs dfs -cp /data/input/data.txt /data/input/data01.txt-mv 移动： hdfs dfs -mv /data/input/data.txt /data/input/data02.txt-count 统计目录下的文件个数-text、-cat 查看文件的内容 hdfs dfs -cat /data/input/data.txt-balancer 平衡操作 使用 Java 进行 HDFS 基本操作下面是一个非常简单的例子，需要说明的是，”root”是那台 Linux 上有权限的用户名，不然会报权限错误，有多种解决方案，可以参考：HDFS JAVA 客户端的权限错误：Permission denied 12345678910111213141516171819202122232425262728package org.bearfly.fun.hadooplearn;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;/*** @Description: App* @author bearfly1990* @date Feb 17, 2019 10:18:42 PM*/public class App { public static void main(String[] args) throws IOException, InterruptedException, URISyntaxException { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop00:9000&quot;), conf, &quot;root&quot;); fs.mkdirs(new Path(&quot;/folder1&quot;)); Path src = new Path(&quot;d:/test.txt&quot;); Path dst = new Path(&quot;/folder1&quot;); fs.copyFromLocalFile(src, dst); fs.close(); }} 网上有更多详细的例子。 最后的运行结果如下（加上上面创建的文件）： 12345678[root@hadoop00 ~]# hdfs dfs -ls -R /drwxr-xr-x - root supergroup 0 2019-02-17 21:23 /datadrwxr-xr-x - root supergroup 0 2019-02-17 21:25 /data/input-rw-r--r-- 3 root supergroup 68 2019-02-17 21:25 /data/input/word-count-data.txtdrwxr-xr-x - root supergroup 0 2019-02-17 21:23 /data/outputdrwxr-xr-x - root supergroup 0 2019-02-17 22:10 /folder1-rw-r--r-- 3 root supergroup 20 2019-02-17 22:10 /folder1/test.txt WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable在使用的过程中一直有报这样的 Warning,虽然可以正常运行，但是看着很烦。 网上有许多的问题和对应解决方案，而我这个编译的版本什么都是一样的，只要在环境变量中添加 HADOOP_OPTS 即可。 具体分析的时候可以把 Hadoop 的 debug log 打开$ export HADOOP_ROOT_LOGGER=DEBUG,console，找到对应的信息。 12# /etc/profileexport HADOOP_OPTS=&quot;-Djava.library.path=${HADOOP_HOME}/lib/native&quot; DataNode Crashed在使用的过程中，我还遇到了DataNode失效了。我找不到原因，所以stop hdfs 和 yarn 之后重新format了一下就好了。 注：在这过程中我关闭了防火墙，删除原始的目录，排除了可能的影响。 更多信息可以参考最后的链接。 参考： HDFS JAVA 客户端的权限错误：Permission denied Hadoop 出现错误：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable，解决方案 Hadoop之—— WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… Hadoop之——重新格式化HDFS的方案 java使用FileSystem上传文件到hadoop文件系统","link":"/2019/02/17/2019/02/2019-02-17-HDFSOperation/"},{"title":"Generate Business Days","text":"背景与需求这两天有个小的需求，想要使用 Stored Procedure 根据 Holiday 的信息，将新的一年每月的 Business Day 数量计算出来并插入一张表中。 这个功能我已经实现，现在就记录一下一些觉得有意思的信息，简单的介绍下自己的思路。 简化需求假设有一张Holiday表，表示每个国家的假期计划，其中假日可能和周末两天重合，那么那天假期其实是没用了，等于没放假。 现在假设需要构造 2019 年每个月的 Business Days(每个月除了周末和假期，假期与周末重合就算一天) 现有的 Holiday 表如下： 1234create table Holiday(HolidayDate date,Region varchar(15)) 123456789HolidayDate Region2019-01-01 China2019-01-02 China2019-04-01 China2019-05-01 China2019-07-07 China2019-10-01 China2019-01-05 US2019-06-01 US 并有一张 Region 表： 123create table Region(Region varchar(15)) 123RegionChinaUS 构造 2019 月份数据下面使用临时表来构造 2019 年的月份数据，每年的每个月的一号是确定的。 1234567891011121314declare @Table_WorkDays table (StartDate Date, EndDate Date, WorkDayNums int);insert into @Table_WorkDays(StartDate) values('2019-01-01');insert into @Table_WorkDays(StartDate) values('2019-02-01');insert into @Table_WorkDays(StartDate) values('2019-03-01');insert into @Table_WorkDays(StartDate) values('2019-04-01');insert into @Table_WorkDays(StartDate) values('2019-05-01');insert into @Table_WorkDays(StartDate) values('2019-06-01');insert into @Table_WorkDays(StartDate) values('2019-07-01');insert into @Table_WorkDays(StartDate) values('2019-08-01');insert into @Table_WorkDays(StartDate) values('2019-09-01');insert into @Table_WorkDays(StartDate) values('2019-10-01');insert into @Table_WorkDays(StartDate) values('2019-11-01');insert into @Table_WorkDays(StartDate) values('2019-12-01'); 12345678910111213StartDate EndDate WorkDayNums2019-01-01 NULL NULL2019-02-01 NULL NULL2019-03-01 NULL NULL2019-04-01 NULL NULL2019-05-01 NULL NULL2019-06-01 NULL NULL2019-07-01 NULL NULL2019-08-01 NULL NULL2019-09-01 NULL NULL2019-10-01 NULL NULL2019-11-01 NULL NULL2019-12-01 NULL NULL 接下来计算 EndDate，通过加一个月的时间再减去一天，即月末。 1update @Table_WorkDays set EndDate = DATEADD(day, -1, DATEADD(month, 1, StartDate)) 如果 sql server 版本 &gt;= SQL Server 2012，可以使用EOMonth函数 1update @Table_WorkDays set EndDate = EOMonth(StartDate) 12345678910111213StartDate EndDate WorkDayNums2019-01-01 2019-01-31 NULL2019-02-01 2019-02-28 NULL2019-03-01 2019-03-31 NULL2019-04-01 2019-04-30 NULL2019-05-01 2019-05-31 NULL2019-06-01 2019-06-30 NULL2019-07-01 2019-07-31 NULL2019-08-01 2019-08-31 NULL2019-09-01 2019-09-30 NULL2019-10-01 2019-10-31 NULL2019-11-01 2019-11-30 NULL2019-12-01 2019-12-31 NULL 下面再计算出 workday 的天数，把一个月中的周末两天去掉： 12345update @Table_WorkDays set WorkDayNums = (DATEDIFF(dd, StartDate, EndDate) + 1) - (DATEDIFF(wk, StartDate, EndDate) * 2) - (case when DATENAME(dw, StartDate) = 'Sunday' then 1 else 0 end) - (case when DATENAME(dw, EndDate) = 'Saturday' then 1 else 0 end) 12345678910111213StartDate EndDate WorkDayNums2019-01-01 2019-01-31 232019-02-01 2019-02-28 202019-03-01 2019-03-31 212019-04-01 2019-04-30 222019-05-01 2019-05-31 232019-06-01 2019-06-30 202019-07-01 2019-07-31 232019-08-01 2019-08-31 222019-09-01 2019-09-30 212019-10-01 2019-10-31 232019-11-01 2019-11-30 212019-12-01 2019-12-31 22 结合 Region 表简单 join，得到每个 Region 2019 年的初始 WorkDayNums: 1select * from @Table_WorkDays, Region 12345678910111213141516171819202122232425StartDate EndDate WorkDayNums Region2019-01-01 2019-01-31 23 China2019-01-01 2019-01-31 23 US2019-02-01 2019-02-28 20 China2019-02-01 2019-02-28 20 US2019-03-01 2019-03-31 21 China2019-03-01 2019-03-31 21 US2019-04-01 2019-04-30 22 China2019-04-01 2019-04-30 22 US2019-05-01 2019-05-31 23 China2019-05-01 2019-05-31 23 US2019-06-01 2019-06-30 20 China2019-06-01 2019-06-30 20 US2019-07-01 2019-07-31 23 China2019-07-01 2019-07-31 23 US2019-08-01 2019-08-31 22 China2019-08-01 2019-08-31 22 US2019-09-01 2019-09-30 21 China2019-09-01 2019-09-30 21 US2019-10-01 2019-10-31 23 China2019-10-01 2019-10-31 23 US2019-11-01 2019-11-30 21 China2019-11-01 2019-11-30 21 US2019-12-01 2019-12-31 22 China2019-12-01 2019-12-31 22 US 优化 Holiday 数据一个月中可能有许多天假期，所以我们需要最后算一个月的有效假期（不和周末重合）。 首先得出月份 1select CONVERT(date, DATEADD(month, DATEDIFF(month, 0, HolidayDate), 0)) as TheMonth, * from Holiday; 123456789TheMonth HolidayDate Region2019-01-01 2019-01-01 China2019-01-01 2019-01-02 China2019-04-01 2019-04-01 China2019-05-01 2019-05-01 China2019-07-01 2019-07-07 China2019-10-01 2019-10-01 China2019-01-01 2019-01-05 US2019-06-01 2019-06-01 US 然后 group by TheMonth 和 Region的时候计算有效假期（不和周末重合）： 1234567--#HolidayNewselect TheMonth, Region, sum(case when DATENAME(dw, HolidayDate) in ('Saturday', 'Sunday') then 0 else 1 end) as ValidHolidayDaysfrom (select CONVERT(date, DATEADD(month, DATEDIFF(month, 0, HolidayDate), 0)) TheMonth,* from Holiday) as HolidayNewgroup by TheMonth, Regionorder by Region, TheMonth; 12345678TheMonth Region ValidHolidayDays2019-01-01 China 22019-04-01 China 12019-05-01 China 12019-07-01 China 02019-10-01 China 12019-01-01 US 02019-06-01 US 0 合并 WorkDay 与 Valid Holiday Day将上面的#HolidayNew和最上面的初始workday结合一下，就可以得到最后的Business Day Nums。 123456select temp.startDate as TheMonth, temp.Region, WorkDayNums - isnull(hn.ValidHolidayDays, 0) as BusinessDaysfrom (select * from @Table_WorkDays, Region) as temp left join #HolidayNew hn on temp.StartDate = hn.TheMonth and temp.Region = hn.Region 12345678910111213141516171819202122232425TheMonth Region BusinessDays2019-01-01 China 212019-01-01 US 232019-02-01 China 202019-02-01 US 202019-03-01 China 212019-03-01 US 212019-04-01 China 212019-04-01 US 222019-05-01 China 222019-05-01 US 232019-06-01 China 202019-06-01 US 202019-07-01 China 232019-07-01 US 232019-08-01 China 222019-08-01 US 222019-09-01 China 212019-09-01 US 212019-10-01 China 222019-10-01 US 232019-11-01 China 212019-11-01 US 212019-12-01 China 222019-12-01 US 22 最后这次又回顾学习了下一些日期函数的学习，包括类型转换等，还可以。","link":"/2019/02/25/2019/02/2019-02-25-BusinessDays/"},{"title":"Difference between Python2 and Python3","text":"Python2.x 与 3​​.x 版本区别Python 的 3​​.0 版本，常被称为 Python 3000，或简称 Py3k。相对于 Python 的早期版本，这是一个较大的升级。 为了不带入过多的累赘，Python 3.0 在设计的时候没有考虑向下相容。 许多针对早期 Python 版本设计的程式都无法在 Python 3.0 上正常执行。 为了照顾现有程式，Python 2.6 作为一个过渡版本，基本使用了 Python 2.x 的语法和库，同时考虑了向 Python 3.0 的迁移，允许使用部分 Python 3.0 的语法与函数。 新的 Python 程式建议使用 Python 3.0 版本的语法。 除非执行环境无法安装 Python 3.0 或者程式本身使用了不支援 Python 3.0 的第三方库。目前不支援 Python 3.0 的第三方库有 Twisted, py2exe, PIL 等。 大多数第三方库都正在努力地相容 Python 3.0 版本。即使无法立即使用 Python 3.0，也建议编写相容 Python 3.0 版本的程式，然后使用 Python 2.6, Python 2.7 来执行。 Python 3.0 的变化主要在以下几个方面: print 函数print 语句没有了，取而代之的是 print()函数。 Python 2.6 与 Python 2.7 部分地支持这种形式的 print 语法。在 Python 2.6 与 Python 2.7 里面，以下三种形式是等价的： 123print &quot;fish&quot;print (&quot;fish&quot;) #注意print后面有个空格print(&quot;fish&quot;) #print()不能带有任何其它参数 然而，Python 2.6 实际已经支持新的 print()语法： 12from __future__ import print_functionprint(&quot;fish&quot;, &quot;panda&quot;, sep=', ') UnicodePython 2 有 ASCII str() 类型，unicode() 是单独的，不是 byte 类型。 现在， 在 Python 3，我们最终有了 Unicode (utf-8) 字符串，以及一个字节类：byte 和 bytearrays。 由于 Python3.X 源码文件默认使用 utf-8 编码，这就使得以下代码是合法的： 123&gt;&gt;&gt; 中国 = 'china'&gt;&gt;&gt;print(中国)china Python 2.x 123456&gt;&gt;&gt; str = &quot;我爱北京天安门&quot;&gt;&gt;&gt; str'\\xe6\\x88\\x91\\xe7\\x88\\xb1\\xe5\\x8c\\x97\\xe4\\xba\\xac\\xe5\\xa4\\xa9\\xe5\\xae\\x89\\xe9\\x97\\xa8'&gt;&gt;&gt; str = u&quot;我爱北京天安门&quot;&gt;&gt;&gt; stru'\\u6211\\u7231\\u5317\\u4eac\\u5929\\u5b89\\u95e8' Python 3.x 123&gt;&gt;&gt; str = &quot;我爱北京天安门&quot;&gt;&gt;&gt; str'我爱北京天安门' 除法运算Python 中的除法较其它语言显得非常高端，有套很复杂的规则。Python 中的除法有两个运算符，/和// 首先来说/除法: 在 python 2.x 中/除法就跟我们熟悉的大多数语言，比如 Java 啊 C 啊差不多，整数相除的结果是一个整数，把小数部分完全忽略掉，浮点数除法会保留小数点的部分得到一个浮点数的结果。 在 python 3.x 中/除法不再这么做了，对于整数之间的相除，结果也会是浮点数。 Python 2.x: 1234&gt;&gt;&gt; 1 / 20&gt;&gt;&gt; 1.0 / 2.00.5 Python 3.x: 12&gt;&gt;&gt; 1/20.5 而对于//除法，这种除法叫做 floor 除法，会对除法的结果自动进行一个 floor 操作，在 python 2.x 和 python 3.x 中是一致的。 python 2.x: 12&gt;&gt;&gt; -1 // 2-1 python 3.x: 123&gt;&gt;&gt; -1 // 2-1注意的是并不是舍弃小数部分，而是执行 floor 操作，如果要截取整数部分，那么需要使用 math 模块的 trunc 函数 python 3.x: 12345&gt;&gt;&gt; import math&gt;&gt;&gt; math.trunc(1 / 2)0&gt;&gt;&gt; math.trunc(-1 / 2)0 异常在 Python 3 中处理异常也轻微的改变了，在 Python 3 中我们现在使用 as 作为关键词。 捕获异常的语法由 except exc, var 改为 except exc as var。 使用语法 except (exc1, exc2) as var 可以同时捕获多种类别的异常。 Python 2.6 已经支持这两种语法。 在 2.x 时代，所有类型的对象都是可以被直接抛出的，在 3.x 时代，只有继承自 BaseException 的对象才可以被抛出。 2.x raise 语句使用逗号将抛出对象类型和参数分开，3.x 取消了这种奇葩的写法，直接调用构造函数抛出对象即可。在 2.x 时代，异常在代码中除了表示程序错误，还经常做一些普通控制结构应该做的事情，在 3.x 中可以看出，设计者让异常变的更加专一，只有在错误发生的情况才能去用异常捕获语句来处理。 xrange在 Python 2 中 xrange() 创建迭代对象的用法是非常流行的。比如： for 循环或者是列表/集合/字典推导式。 这个表现十分像生成器（比如。”惰性求值”）。但是这个 xrange-iterable 是无穷的，意味着你可以无限遍历。 由于它的惰性求值，如果你不得仅仅不遍历它一次，xrange() 函数 比 range() 更快（比如 for 循环）。尽管如此，对比迭代一次，不建议你重复迭代多次，因为生成器每次都从头开始。 在 Python 3 中，range() 是像 xrange() 那样实现以至于一个专门的 xrange() 函数都不再存在（在 Python 3 中 xrange() 会抛出命名异常）。 12345678910import timeitn = 10000def test_range(n): return for i in range(n): passdef test_xrange(n): for i in xrange(n): pass Python 2 123456789101112131415print 'Python', python_version()print '\\ntiming range()'%timeit test_range(n)print '\\n\\ntiming xrange()'%timeit test_xrange(n)Python 2.7.6timing range()1000 loops, best of 3: 433 µs per looptiming xrange()1000 loops, best of 3: 350 µs per loop Python 3 123456789print('Python', python_version())print('\\ntiming range()')%timeit test_range(n)Python 3.4.1timing range()1000 loops, best of 3: 520 µs per loop 1234567print(xrange(10))---------------------------------------------------------------------------NameError Traceback (most recent call last)&lt;ipython-input-5-5d8f9b79ea70&gt; in &lt;module&gt;()----&gt; 1 print(xrange(10))NameError: name 'xrange' is not defined 八进制字面量表示八进制数必须写成 0o777，原来的形式 0777 不能用了；二进制必须写成 0b111。 新增了一个 bin()函数用于将一个整数转换成二进制字串。 Python 2.6 已经支持这两种语法。 在 Python 3.x 中，表示八进制字面量的方式只有一种，就是 0o1000。 python 2.x 1234&gt;&gt;&gt; 0o1000512&gt;&gt;&gt; 01000512 python 3.x 1234567&gt;&gt;&gt; 01000 File &quot;&lt;stdin&gt;&quot;, line 1 01000 ^SyntaxError: invalid token&gt;&gt;&gt; 0o1000512 不等运算符Python 2.x 中不等于有两种写法 != 和 &lt;&gt; Python 3.x 中去掉了&lt;&gt;, 只有!=一种写法，还好，我从来没有使用&lt;&gt;的习惯 去掉了 repr 表达式``Python 2.x 中反引号``相当于 repr 函数的作用 Python 3.x 中去掉了``这种写法，只允许使用 repr 函数，这样做的目的是为了使代码看上去更清晰么？不过我感觉用 repr 的机会很少，一般只在 debug 的时候才用，多数时候还是用 str 函数来用字符串描述对象。 12def sendMail(from_: str, to: str, title: str, body: str) -&gt; bool: pass 多个模块被改名（根据 PEP8） 旧的名字 新的名字 _winreg winreg ConfigParser configparser copy_reg copyreg Queue queue SocketServer socketserver repr reprlib StringIO 模块现在被合并到新的 io 模组内。 new, md5, gopherlib 等模块被删除。 Python 2.6 已经支援新的 io 模组。 httplib, BaseHTTPServer, CGIHTTPServer, SimpleHTTPServer, Cookie, cookielib 被合并到 http 包内。 取消了 exec 语句，只剩下 exec()函数。 Python 2.6 已经支援 exec()函数。 数据类型1）Py3.X 去除了 long 类型，现在只有一种整型——int，但它的行为就像 2.X 版本的 long 2）新增了 bytes 类型，对应于 2.X 版本的八位串，定义一个 bytes 字面量的方法如下： 123&gt;&gt;&gt; b = b'china'&gt;&gt;&gt; type(b)&lt;type 'bytes'&gt; str 对象和 bytes 对象可以使用.encode() (str -&gt; bytes) or .decode() (bytes -&gt; str)方法相互转化。 123456&gt;&gt;&gt; s = b.decode()&gt;&gt;&gt; s'china'&gt;&gt;&gt; b1 = s.encode()&gt;&gt;&gt; b1b'china' 3）dict的.keys()、.items 和.values()方法返回迭代器，而之前的iterkeys()等函数都被废弃。同时去掉的还有 dict.has_key()，用 in替代它吧. 打开文件原： 123file( ..... )或 open(.....) 改为只能用 1open(.....) 从键盘录入一个字符串 原: 1raw_input( &quot;提示信息&quot; ) 改为: 1input( &quot;提示信息&quot; ) 在python2.x中raw_input()和input( )，两个函数都存在，其中区别为： raw_input()—将所有输入作为字符串看待，返回字符串类型 input()—–只能接收”数字”的输入，在对待纯数字输入时具有自己的特性，它返回所输入的数字的类型（int, float ）在python3.x中raw_input()和input( )进行了整合，去除了raw_input()，仅保留了input()函数，其接收任意任性输入，将所有输入默认为字符串处理，并返回字符串类型。 map、filter 和 reduce这三个函数号称是函数式编程的代表。在 Python3.x 和 Python2.x 中也有了很大的差异。 首先我们先简单的在 Python2.x 的交互下输入 map 和 filter,看到它们两者的类型是 built-in function(内置函数): 12345&gt;&gt;&gt; map&lt;built-in function map&gt;&gt;&gt;&gt; filter&lt;built-in function filter&gt;&gt;&gt;&gt; 它们输出的结果类型都是列表: 12345&gt;&gt;&gt; map(lambda x:x *2, [1,2,3])[2, 4, 6]&gt;&gt;&gt; filter(lambda x:x %2 ==0,range(10))[0, 2, 4, 6, 8]&gt;&gt;&gt; 但是在Python 3.x中它们却不是这个样子了： 123456789&gt;&gt;&gt; map&lt;class 'map'&gt;&gt;&gt;&gt; map(print,[1,2,3])&lt;map object at 0x10d8bd400&gt;&gt;&gt;&gt; filter&lt;class 'filter'&gt;&gt;&gt;&gt; filter(lambda x:x % 2 == 0, range(10))&lt;filter object at 0x10d8bd3c8&gt;&gt;&gt;&gt; 首先它们从函数变成了类，其次，它们的返回结果也从当初的列表成了一个可迭代的对象, 我们尝试用 next 函数来进行手工迭代: 12345678910&gt;&gt;&gt; f =filter(lambda x:x %2 ==0, range(10))&gt;&gt;&gt; next(f)0&gt;&gt;&gt; next(f)2&gt;&gt;&gt; next(f)4&gt;&gt;&gt; next(f)6&gt;&gt;&gt; 对于比较高端的 reduce 函数，它在 Python 3.x 中已经不属于 built-in 了，被挪到 functools 模块当中。 参考： Python2.x与3​​.x版本区别 Python2和Python3的差异 python3和Python2的区别（被坑太久了）","link":"/2019/02/19/2019/02/2019-02-19-Py2DifferPy3/"},{"title":"Tinkter Demo","text":"背景最近在做数据的交互与导入导出，刚开始还是按照原先的方式，把所有的参数写在配置文件中。 在运行前需要把配置改掉，这样很容易遗漏。 所以想到了写点简单的界面来配置一些选项，相对来说所见即所得。 Python 里可以写界面的选择很多，也有很强大和专业的库(e.g. PyQt)，但是我们这种程度的使用，用原生的 tinkter 就足够了。 tinkter 基本信息现在基本上都是基于 python 3.0 在开发，所以 tkinter 的组件命名也发生的一些变化。 1234567# Python 2.x使用这行#from Tkinter import *# Python 3.x使用这行from tkinter import * ttk程序都是直接使用 tkinter 模块下的 GUI 组件的，这些组件看上去特别“复古”，也就是丑，仿佛是从 20 年前的程序上抠出来的组件。 为了弥补这点不足，Tkinter 后来引入了一个 ttk 组件作为补充（主要就是简单包装、美化一下），并使用功能更强大的 Combobox 取代了原来的 Listbox，且新增了 LabeledScale（带标签的 Scale）、Notebook（多文档窗口）、Progressbar（进度条）、Treeview（树）等组件。 ttk 作为一个模块被放在 tkinter 包下，使用 ttk 组件与使用普通的 Tkinter 组件并没有多大的区别，只要导入 ttk 模块即可 布局UI 组件需要我们摆放位置，tinkter 提供了三种布局方式： Pack Grid Place 相对来说 Pack 和 Grid 相对多一些，Pack 更加灵活，而 Grid 适方方正正的类表格布局，而在同一个 Frame 中，布局方式只能有一种。 Demo在我下面列的参考文档中有很详细的组件与细节介绍，下面我直接放一个自己写的简单Demo。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import webbrowserfrom PIL import ImageTkfrom tkinter import ttkfrom tkinter import messageboxfrom tkinter import *def gamerun(): print('run game...')def init_menu(root, imgGame, imgHelp): menubar =Menu(root) root.config(menu = menubar) #实例化菜单1，创建下拉菜单，调用add_separate创建分割线 menu1 =Menu(menubar,tearoff = 0) menubar.add_cascade(label = &quot;Edit&quot;,menu = menu1) menu1.add_command(label = &quot;Do Nothing&quot;) menu1.add_separator() menu1.add_command(label = &quot;Quit&quot;,command = root.quit) menu2 =Menu(menubar,tearoff = 0) menubar.add_cascade(label = &quot;More&quot;,menu = menu2) menu2.add_command(label = &quot;New Job&quot;,image = imgGame ,compound= &quot;left&quot;,command = lambda:gamerun()) menu2.add_command(label = &quot;Tkinter&quot;,image = imgHelp,compound = &quot;left&quot;,command = lambda:webbrowser.open(&quot;http://effbot.org/tkinterbook/tkinter-index.htm&quot;))def init_userinfo(root): frame_user_info = ttk.LabelFrame(root, text='UserInfo:') frame_user_info.pack(side=TOP, fill=X) # myLabel= Label(frame_user_info, text='UserInfo:', font=&quot;Helvetica 10 bold&quot;) # myLabel[&quot;relief&quot;]=tk.SOLID#设置label的样式 # myLabel[&quot;width&quot;]=10 # myLabel[&quot;height&quot;]=5 # myLabel.pack(side=LEFT) # myLabel.grid(row=0, sticky=W) Label(frame_user_info, text=&quot;Username&quot;).grid(row=1, sticky=W) Label(frame_user_info, text=&quot;Password&quot;).grid(row=2, sticky=W) username = Entry(frame_user_info).grid(row=1, column=1, sticky=E) password = Entry(frame_user_info, show='*').grid(row=2, column=1, sticky=E) # Button(frame_user_info, text=&quot;Login&quot;).grid(row=2, column=1, sticky=E)def init_radio(root): frame_radio = ttk.Labelframe(root, text='Radio Test',padding=20) frame_radio.pack(fill=BOTH, expand=YES, padx=10, pady=10) books = ['C++', 'Python', 'Linux', 'Java'] i = 0 books_radio = [] for book in books: intVar = IntVar() books_radio.append(intVar) Radiobutton(frame_radio, text=book,value=i,variable=intVar,command=changed).pack(side=LEFT) i = i + 1 def changed(): print('value changed')def init_checkbutton(root): frame_checkbutton = ttk.Labelframe(root, text='Checkbutton Test',padding=20) frame_checkbutton.pack(fill=BOTH, expand=YES, padx=10, pady=10) books = ['C++', 'Python', 'Linux', 'Java'] i = 0 books_checkbox = [] for book in books: strVar = StringVar() books_checkbox.append(strVar) cb = ttk.Checkbutton(frame_checkbutton, text = book, variable = strVar, onvalue = i, offvalue = 'None', command = changed) cb.pack(anchor=W) i += 1def init_combobox(root): frame_combobox = ttk.Labelframe(root, text='Combobox Test',padding=20) frame_combobox.pack(fill=BOTH, expand=YES, padx=10, pady=10) strVar = StringVar() # 创建Combobox组件 cb = ttk.Combobox(frame_combobox, textvariable=strVar, # 绑定到self.strVar变量 postcommand=changed) # 当用户单击下拉箭头时触发self.choose方法 cb.pack(side=TOP) # 为Combobox配置多个选项 cb['values'] = ['Python', 'Ruby', 'Kotlin', 'Swift']def show_it(): messagebox.showinfo(title='Alert', message=&quot;Please try again!&quot;)def init_showinfo(root): frame_showinfo = ttk.LabelFrame(root, text='ShowInfoTest:') frame_showinfo.pack(side=TOP, fill=X) Button(frame_showinfo, text=&quot;Click Me&quot;, command=show_it).pack(side=LEFT, fill=Y) def init_ui(): root = Tk() # root.geometry('580x680+200+100') root.resizable(width = False, height = False) root.title(&quot;Test&quot;) root.iconbitmap('login.ico') imgGame = PhotoImage(file='game.png') imgHelp = ImageTk.PhotoImage(file=&quot;help.png&quot;) init_menu(root, imgGame, imgHelp) init_userinfo(root) init_showinfo(root) init_radio(root) init_checkbutton(root) init_combobox(root) root.mainloop()if __name__ == '__main__': init_ui() 参考: Python Tkinter 教程（GUI 图形界面开发教程） Python2.7 Tkinter 创建简单登录注册界面","link":"/2019/06/25/2019/06/2019-06-23-TkinterDemo/"},{"title":"Compare data by pandas","text":"背景最近的项目会根据一些数据的值来得到一定的结果，用户原来使用 python 直接生成 excel 文件，我们相当于移植到数据库中，使用 SP 去做业务逻辑。 所以理论上最后的结果需要一致，之前没有做全部数据的对比，这次相关 features 由我在改动，所以无论如何还是要保证数据能对比上，所以就写了相关的脚本来处理。 需求情况是这样的，假设： User 有一份 output 文件(output_user.xlsx)，数据库中有我们处理后对应的 output 数据。 User 的 output 文件是有多余的数据，我们在数据库中已经剔除，所以对比前需要把 output 文件的冗余数据剔除掉。 需要把数据库中我们的 output 数据也输出到文件中(output_db.xlsx) 将 output_user.xlsx 与 output_db.xlsx 中的需要对比的字段进行比较，最后将不同的数据行出来到 compare_result.xlsx 文件中。 假设User Output 文件中的 output_user.xlsx 数据为： id UserName Age Country Status 1 cx 29 China Success 2 xm 27 China Pending 3 ll 30 China Failed 4 zz 20 China Success 5 yy 21 USA Success 5 yy -21 USA Success 6 xx -21 USA Success 数据库中的数据为： id UserName Age Country Status 1 cx 29 China Success 2 xm 27 China Success 3 ll 30 China Failed 4 zz 20 China Pending 剔除的 logic: id 不能重复 Age 不能为负数(value&lt;0) 所以在 output_user.xlsx 中 id 为 5，6 的需要被去除 step01 剔除冗余数据1234567891011121314151617181920212223import pandas as pddef get_exception_indexes(df): indexes_removed = set() df_age = df[df['Age'].apply(lambda x: x &lt; 0 or str(x) in ('', 'nan'))]['Age'] indexes_removed.update(list(df_age.index.values)) df_duplicated_ids = df[df.duplicated(subset=['id'], keep=False)] df_duplicated_group = df_duplicated_ids.groupby('id') for id in df_duplicated_group.groups.keys(): indexes_removed.update(df_duplicated_group.groups[id]) return list(indexes_removed)if __name__=='__main__': df_output_user = pd.read_excel('output_user.xlsx', sheet_name='Data') index_removed = get_exception_indexes(df_output_user) df_output_user_removed = df_output_user.iloc[index_removed] df_output_user_keeped = df_output_user.drop(index=index_removed) writer = pd.ExcelWriter('output_user.removed.xlsx', engine='openpyxl') df_output_user_keeped.to_excel(writer, sheet_name='Data', index=False) df_output_user_removed.to_excel(writer, sheet_name='Removed', index=False) writer.save() 得到结果： id UserName Age Country Status 1 cx 29 China Success 2 xm 27 China Pending 3 ll 30 China Failed 4 zz 20 China Success step02 从数据库中导出到Excel中123456789101112import pandas as pdif __name__=='__main__': conn=pyodbc.connect(r'DRIVER={SQL Server};SERVER=PC-CX\\SQLEXPRESS;UID=test;PWD=test') sql_str = 'select * from [BFTest].[dbo].[Test_Output]' sql_query = pd.read_sql_query(sql_str, conn) df_output_db = pd.DataFrame(sql_query) writer = pd.ExcelWriter('output_db.xlsx', engine='openpyxl') df_output_db.to_excel(writer, sheet_name='Data', index=False) writer.save() step03 对比数据最后将没有匹配的数据分别放到sheet User和DB中。 1234567891011121314151617181920212223242526272829303132import pandas as pdimport pyodbcdef get_key_value(x): key_value = [x['id'], x['UserName'], x['Country'], x['Status']] return ','.join([str(x).replace(' ', '').lower() for x in key_value])if __name__=='__main__': df_output_user = pd.read_excel('output_user.removed.xlsx', sheet_name='Data') df_output_db = pd.read_excel('output_db.xlsx', sheet_name='Data') df_output_user['key_value'] = df_output_user.apply(lambda x: get_key_value(x), axis=1) df_output_user['source'] = 'User' df_user_compare = df_output_user[['key_value', 'source']] df_output_db['key_value'] = df_output_db.apply(lambda x: get_key_value(x), axis=1) df_output_db['source'] = 'DB' df_db_compare = df_output_db[['key_value', 'source']] df_compare = pd.concat([df_user_compare, df_db_compare]) df_compare = df_compare.drop_duplicates(subset=['key_value'], keep=False) indexes_user = df_compare.index[df_compare['source'] == 'User'].tolist() indexes_db = df_compare.index[df_compare['source'] == 'DB'].tolist() df_output_user = df_output_user.iloc[indexes_user] df_output_db = df_output_db.iloc[indexes_db] writer = pd.ExcelWriter('compare_result.xlsx', engine='openpyxl') df_output_user.to_excel(writer, sheet_name='User', index=False) df_output_db.to_excel(writer, sheet_name='DB', index=False) writer.save() 最后的代码在demo03_get_data_from_db","link":"/2019/10/13/2019/10/2019-10-13-CompareData/"},{"title":"Process data by pandas","text":"背景最近做了一些数据处理方面的事，用到了 pandas，记录一下。 python 3.6.5 pandas 0.25 xlrd 1.2.0 默认包引入： 1import pandas as pd 读取 excel假设有一张如下数据的 excel 表格： Name gender Company Salary Index_A Shawn man SS 6666 Index_B Andy man GG 5555 Index_C Richard man 3333 Index_D Rose female SS 7777 Index_E SS 6666 Index_F June female GG 8888 Index_G Richard man GG 4444 Index_G Richard mans GG 4444 使用 pandas 读取： 12df_sheet = pd.read_excel('test_file.xlsx', sheet_name='salary01')print(df_sheet) 123456789 Unnamed: 0 Name gender Company Salary0 Index_A Shawn man SS 6666.01 Index_B Andy man GG 5555.02 Index_C Richard man GG 3333.03 Index_D Rose female SS 7777.04 Index_E NaN NaN SS NaN5 Index_F June female GG 8888.06 Index_G Richard man GG 4444.07 Index_H Paul mans GG 5656.0 可以看到第一列没有列名，默认给了Unnamed: 0 index 为 6 那行Richard多了一个空格(在 excel 中故意空出来) 没有值的 cell 里，在 dateframe 中被填充成NaN 指定索引为了使我们第一列被成我们的索引，可以在读取时加参数： 12df_sheet = pd.read_excel('test_file.xlsx', sheet_name='salary01', index_col = [0])print(df_sheet) 123456789 Name gender Company SalaryIndex_A Shawn man SS 6666.0Index_B Andy man GG 5555.0Index_C Richard man NaN 3333.0Index_D Rose female SS 7777.0Index_E NaN NaN SS NaNIndex_F June female GG 8888.0Index_G Richard man GG 4444.0Index_H Paul mans GG 5656.0 数据类型当我们没有指定数据类型时，pandas 会按他的默认类型加载。 比如Salary那列默认使用 float 加载，但我们想要的是 int 的话，我们可以使用 convert： 12df_sheet = pd.read_excel('test_file.xlsx', sheet_name='salary01', converters = {'Name': str, 'Salary': int}, index_col = [0])print(df_sheet) 123456789 Name gender Company SalaryIndex_A Shawn man SS 6666Index_B Andy man GG 5555Index_C Richard man NaN 3333Index_D Rose female SS 7777Index_E NaN NaN SS NaNIndex_F June female GG 8888Index_G Richard man GG 4444Index_H Paul mans GG 5656 更多读取 excel 的详细信息可以参考最后 使用 pandas 读取 excel 处理数据得到基本信息123456789101112131415161718# 索引值# ['Index_A', 'Index_B', 'Index_C', 'Index_D', 'Index_E', 'Index_F', 'Index_G', 'Index_H']print(df_sheet.index.tolist())print(df_sheet.index.to_list())# 行数 8print(df_sheet.shape[0])print(len(df_sheet))# 列数 4# ['Name', 'gender', 'Company', 'Salary']print(df_sheet.shape[1]) reset_index在上面我们使用了第一列做为索引，有时候我们希望还是用回默认的数字做为索引，那么就可以使用reset_index 12df_sheet = df_sheet.reset_index()print(df_sheet) 123456789 index Name gender Company Salary0 Index_A Shawn man SS 66661 Index_B Andy man GG 55552 Index_C Richard man NaN 33333 Index_D Rose female SS 77774 Index_E NaN NaN SS NaN5 Index_F June female GG 88886 Index_G Richard man GG 44447 Index_H Paul mans GG 5656 如果不保留原来的索引，加上drop=True参数就可以了。 1df_sheet = df_sheet.reset_index(drop=True) 关于 nan 和 Nonedataframe中默认的空值为NaN，这是numpy中的一个类型，特殊的 float。Series和DataFrame都会自动把 None 转换成 NaN 然后 运算的时候会把 NaN 当成 0。 详细可以参考：pandas numpy 处理缺失值，none 与 nan 比较 我们最后使用的时候，比如插入数据库前，需要使用 None，所以需要转换一下。 首先 pandas 提供 fillna方法把NaN数据替换成其它值 12df_sheet['Salary'] = df_sheet['Salary'].fillna(0)print(df_sheet) 123456789 index Name gender Company Salary0 Index_A Shawn man SS 66661 Index_B Andy man GG 55552 Index_C Richard man NaN 33333 Index_D Rose female SS 77774 Index_E NaN NaN SS 05 Index_F June female GG 88886 Index_G Richard man GG 44447 Index_H Paul mans GG 5656 可以看到针对Salary这一列把Nan改成了0，但是如果直接用None却是不行的。 123df_sheet['Salary'] = df_sheet['Salary'].fillna(None) raise ValueError(&quot;Must specify a fill 'value' or 'method'.&quot;)ValueError: Must specify a fill 'value' or 'method'. 那么最直接的办法如下： 12df_sheet = df_sheet.where(df_sheet.notnull(), None)print(df_sheet) 123456789 index Name gender Company Salary0 Index_A Shawn man SS 66661 Index_B Andy man GG 55552 Index_C Richard man None 33333 Index_D Rose female SS 77774 Index_E None None SS None5 Index_F June female GG 88886 Index_G Richard man GG 44447 Index_H Paul mans GG 5656 去掉重复行假设我们多了一条重复的数据(Index_H/Index_I) 如下： 12345678910 index Name gender Company Salary0 Index_A Shawn man SS 66661 Index_B Andy man GG 55552 Index_C Richard man None 33333 Index_D Rose female SS 77774 Index_E None None SS None5 Index_F June female GG 88886 Index_G Richard man GG 44447 Index_H Paul mans GG 56568 Index_I Paul mans GG 5656 我们想要只留下一条数据，那么可以使用如下的方法： 12df_sheet.drop_duplicates(['Name', 'gender', 'Company', 'Salary'], keep='last', inplace=True)print(df_sheet) 123456789 index Name gender Company Salary0 Index_A Shawn man SS 66661 Index_B Andy man GG 55552 Index_C Richard man None 33333 Index_D Rose female SS 77774 Index_E None None SS None5 Index_F June female GG 88886 Index_G Richard man GG 44448 Index_I Paul mans GG 5656 因为使用了keep='last'，所以Index_H这行被删除了。如果想只保留最前面的那条，就可以使用keep='first' 找到有问题的行列在Index_I我们可以看到这行的gender为mans是有问题的，我们可以找到他，只允许man和female。 123df_sheet_by_gender = df_sheet[(df_sheet['gender']!='man') &amp; (df_sheet['gender']!='female')]print(df_sheet_by_gender)print(df_sheet_by_gender.index.tolist()) 1234 index Name gender Company Salary4 Index_E None None SS None8 Index_I Paul mans GG 5656[4, 8] 也可以使用下面的方式: 1234def test(x): return x not in ['man', 'female']df_sheet_by_gender = df_sheet[df_sheet['gender'].apply(lambda x: test(x))]print(df_sheet_by_gender) 123 index Name gender Company Salary4 Index_E None None SS None8 Index_I Paul mans GG 5656 当然还可以使用 iloc/loc 等其它方法找到你想要的值 新增一列12df_sheet['new_column'] = df_sheet['Salary']*3print(df_sheet) 123456789 index Name gender Company Salary new_column0 Index_A Shawn man SS 6666 199981 Index_B Andy man GG 5555 166652 Index_C Richard man None 3333 99993 Index_D Rose female SS 7777 233314 Index_E None None SS None NaN5 Index_F June female GG 8888 266646 Index_G Richard man GG 4444 133328 Index_I Paul mans GG 5656 16968 输出到新的 Excel 中1df_sheet.to_excel(&quot;output.xlsx&quot;, sheet_name='newsheet', index=False) 可以选择需要的列 1df_sheet.to_excel(&quot;output.xlsx&quot;, sheet_name='newsheet', index=False, columns=['Name', 'Company', 'Salary']) dataframe to list12print(df_sheet.index.tolist())print(df_sheet.values.tolist()) 12[0, 1, 2, 3, 4, 5, 6, 8][['Index_A', 'Shawn', 'man', 'SS', 6666, 19998], ['Index_B', 'Andy', 'man', 'GG', 5555, 16665], ['Index_C', 'Richard', 'man', None, 3333, 9999], ['Index_D', 'Rose', 'female', 'SS', 7777, 23331], ['Index_E', None, None, 'SS', None, nan], ['Index_F', 'June', 'female', 'GG', 8888, 26664], ['Index_G', 'Richard ', 'man', 'GG', 4444, 13332], ['Index_I', 'Paul', 'mans', 'GG', 5656, 16968]] 参考: Pandas 读取并修改 excel 使用 pandas 读取 excel pandas 中的空值处理 pandas numpy 处理缺失值，none 与 nan 比较 Pandas 标记删除重复记录 pandas 0.25.0 documentation","link":"/2019/08/17/2019/08/2019-08-17-Pandas/"},{"title":"Compare csv file","text":"背景最近有个需求是对比 CSV 文件，其中一个为 Source， 另一个是系统生成的，理论上要满足 mapping 关系。 现在想要用 python 来验证一下生成的文件对应关系是正确的。 需求情况如下： source_old.csv 与 source_new.csv 为需要对面的文件, 需要对比的字段为 csv 中的 Country 与 City Country 的 mapping 关系在 mapping1.txt 中，City 的 mapping 关系在 mapping2.txt 中，使用逗号相隔。 mapping_info.txt 中存储了 mapping 关系文件与对应的 csv 中字段的关系 CSV 文件source_old.csv 123456ID,Name,Country,City1,Name1,Japan,HangZhou2,Name2,China,HangZhou3,Name3,America,Other4,Name4,TaiWan,ShangHai5,Name5,English,TaiZou source_new.csv 123456ID,Name,Country,City1,Name1,JP,HZ2,Name2,CN,Other3,Name3,US,Other4,Name4,TaiWan,SH5,Name5,US,TZ mapping 文件mapping1.txt 1234China,CNAmerica,USAEnglish,USJapan,JP mapping2.txt 123HangZhou,HZShangHai,SHTaiZou,TZ mapping info12Country,mapping1.txtCity,mapping2.txt 思路读取 mapping 文件对于 mapping 文件，先定义了一个通用的方法读取构造 dictionary. 12345678def read_mapping(file): mapping = {} with open(file, 'r') as f: rows = f.readlines() for row in rows: key, value = row.split(',') mapping[key.strip()] = value.strip() return mapping 对比传入需要对比的 csv 文件及对应的 mapping 和字段，输出最后的结果 12345678910111213def compare_csv(csv1_path, csv2_path, mapping, column): df_csv1=pd.read_csv(csv1_path) df_csv2=pd.read_csv(csv2_path) for index,row in df_csv1.iterrows(): old_value = row[column] if old_value in mapping: expect_value = mapping.get(old_value) new_value = df_csv2.loc[index,column] if expect_value != new_value: print(f'File-&lt;{csv2_path}&gt; Col-&lt;{column}&gt; Row-&lt;{index+1}&gt; should be &lt;{expect_value}&gt;, but &lt;{new_value}&gt; found') else: print(f'File-&lt;{csv1_path}&gt; Col-&lt;{column}&gt; Row-&lt;{index+1}&gt; value-&lt;{old_value}&gt; is not in the mapping file') ##main 函数 先读取 mapping 文件的信息，再循环遍历，得到最后的结果： 1234567if __name__=='__main__': mapping_info = read_mapping('mapping_info.txt') for column in mapping_info.keys(): mapping_file = mapping_info[column] column_mapping = read_mapping(mapping_file) compare_csv('source_old.csv', 'source_new.csv', column_mapping, column) 输出1234File-&lt;source_new.csv&gt; Col-&lt;Country&gt; Row-&lt;3&gt; should be &lt;USA&gt;, but &lt;US&gt; foundFile-&lt;source_old.csv&gt; Col-&lt;Country&gt; Row-&lt;4&gt; value-&lt;TaiWan&gt; is not in the mapping fileFile-&lt;source_new.csv&gt; Col-&lt;City&gt; Row-&lt;2&gt; should be &lt;HZ&gt;, but &lt;Other&gt; foundFile-&lt;source_old.csv&gt; Col-&lt;City&gt; Row-&lt;3&gt; value-&lt;Other&gt; is not in the mapping file 完整的代码在demo04_compare_csv","link":"/2019/10/27/2019/10/2019-10-27-CompareDataByMapping/"},{"title":"openpyxl with big data","text":"背景最近写了一个脚本，用 openpyxl 从 sql server 数据库中读取数据，使用 template 文件，将数据填充进去，生成最后的 daily report。 使用 openpyxl 来操作 excel，很方便,但发现当要写入大量数据的时候，时间非常慢，而且非常占内存。 最直接的原因是 openpyxl 会将读写过的 cell 都加载在内存中方便后面 update，不会释放这些 cell 而会一直驻留在内存中。 所以如果单纯写的话建议使用 xlrd，而且 pandas 有 to_csv/to_excel 这样的直接的方法。 但是因为我需要使用模板，在研究后发现 openpyxl 有 read_only/write_only 的模式，可以满足我的需求。(一定要保证安装了 lxml 库，不然就算使用 write_only 模式，也一样会点用大量占存。） Note：针对 office2007 以后的版本，xlsx 文件上限行数大约为 100 多万条的样子。 需求 从数据库中得到想要的数据 读取 template 文件，保留 template 的样式 写入新的 output 文件中 数据库中的数据 Template 直接使用 oponpyxl123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdimport pyodbcimport openpyxlfrom datetime import datetimeconn=pyodbc.connect(r'DRIVER={SQL Server};SERVER=PC-CX\\SQLEXPRESS;UID=test;PWD=test')def logger_info(message): print(f'{datetime.now()}[INFO]', message)def query_data_from_db(conn): sql_str = 'select UserName, Age, Country, [Status] from [BFTest].[dbo].[Test_Output]' sql_query = pd.read_sql_query(sql_str, conn) df_output_db = pd.DataFrame(sql_query) return df_output_dbdef write_to_excel(template, report): workbook = openpyxl.load_workbook(template) ws_report = workbook['report'] row_start = 2 for idx_row, row in df_output_db.iterrows(): for idx_col in range(len(row)): ws_report.cell(column = idx_col+1, row = row_start + idx_row).value = row[idx_col] workbook.save(report)if __name__=='__main__': start_time = datetime.now() logger_info('query data from db') df_output_db = query_data_from_db(conn) logger_info('write to excel') write_to_excel('template.xlsx', 'report.xlsx') end_time = datetime.now() logger_info(f'cost time:{end_time-start_time}') 最后的结果： 增加数据量现在让我们把数据量加上去一些，再看结果： 12345678910111213141516171819202122232425def large_data(df): df_large = df.copy() for i in range(15): df = df_large.copy() df_large=pd.concat([df_large,df]) return df_largeif __name__=='__main__': start_time = datetime.now() logger_info('query data from db') df_output_db = query_data_from_db(conn) logger_info('large data') df_output_db = large_data(df_output_db) df_output_db = df_output_db.reset_index(drop=True) logger_info(f'count data:{len(df_output_db)}') logger_info('write to excel') start_time_excel = datetime.now() write_to_excel('template.xlsx', 'report.xlsx') logger_info(f'write to excel cost time:{datetime.now() - start_time_excel}') logger_info(f'cost time:{datetime.now()-start_time}') 数据量加到了 131072,一共使用了 26 秒。 1234567PS C:\\Users\\mayn\\Desktop\\GitSpace\\PowerScript\\Python3\\openpyxl\\generate_report&gt; python .\\generate_report.hugedata.py2019-11-10 10:50:02.332573[INFO] query data from db2019-11-10 10:50:02.334600[INFO] large data2019-11-10 10:50:02.370501[INFO] count data:1310722019-11-10 10:50:02.370501[INFO] write to excel2019-11-10 10:50:28.605598[INFO] write to excel cost time:0:00:26.2341272019-11-10 10:50:28.606598[INFO] cost time:0:00:26.274025 使用 write_only先写 template 的表头，再写内容： 123456789101112131415161718192021222324252627282930313233343536373839def write_to_excel(template, report): wb_tpl = openpyxl.load_workbook(template) ws_report_tpl = wb_tpl['report'] #workbook.get_sheet_by_name('report') workbook = openpyxl.Workbook(write_only=True) ws_report = workbook.create_sheet('report') for row in ws_report_tpl.rows: row_tpl = [] for cell in row: cell_tpl = openpyxl.cell.WriteOnlyCell(ws_report) cell_tpl.value = cell.value cell_tpl.font = cell.font.copy() cell_tpl.fill = cell.fill.copy() row_tpl.append(cell_tpl) ws_report.append(row_tpl) for idx_row, row in df_output_db.iterrows(): if idx_row % 10000 == 0: print(indx_row) ws_report.append(row.to_list()) workbook.save(report)if __name__=='__main__': start_time = datetime.now() logger_info('query data from db') df_output_db = query_data_from_db(conn) logger_info('large data') df_output_db = large_data(df_output_db) df_output_db = df_output_db.reset_index(drop=True) logger_info(f'count data:{len(df_output_db)}') logger_info('write to excel') start_time_excel = datetime.now() write_to_excel('template.xlsx', 'report.xlsx') logger_info(f'write to excel cost time:{datetime.now() - start_time_excel}') logger_info(f'cost time:{datetime.now()-start_time}') 1234567PS C:\\Users\\mayn\\Desktop\\GitSpace\\PowerScript\\Python3\\openpyxl\\generate_report&gt; python .\\generate_report.hugedata.writeonly.py2019-11-10 10:54:58.253268[INFO] query data from db2019-11-10 10:54:58.255265[INFO] large data2019-11-10 10:54:58.292595[INFO] count data:1310722019-11-10 10:54:58.292595[INFO] write to excel2019-11-10 10:55:18.039439[INFO] write to excel cost time:0:00:19.7458172019-11-10 10:55:18.039439[INFO] cost time:0:00:19.786171 可以看到时间上普通用法比 write_only 慢了 36%左右，在数据量大的时候更明显，内存上的消耗更不用说。 完整的代码在demo04_compare_csv","link":"/2019/11/10/2019/11/2019-11-10-OpenpyxlWithHugeData/"},{"title":"Count data rows in excel","text":"背景今天手动给用户提供了 Report，其中需要对 excel 中的数据与数据库中的进行对比。数据文件不少，我要一个个打开去数，真的很费时间。 想要写一个小脚本，但发现没那么快，还是先手动给做了，写一个还是挺快的。 需求情况是这样的，假设： 都是有效的文件 有多个 excel 文件在同一个目录中 有 xls 和 xlsx 后缀的 统计每个文件的文件名，Sheet，及 Sheet 中有多少有效数据 将结果写到 output 文件中 最后的代码在这里 读取一个 excel，并输出相关信息需求我们一个个实现，首先，读取一个 excel，遍历每个 sheet，得到数据的条数。 下面是一个简单的样例,可以得到想要的结果 123456789import pandas as pddf_sheet_map = pd.read_excel(&quot;./Test1.xlsx&quot;, None)sheets = list(df_sheet_map.keys())for sheet in sheets: df_sheet = df_sheet_map[sheet] print(sheet, len(df_sheet.index)) output: 123Sheet1 6Test 4Demo 7 遍历目录下所有的 excel 文件可以使用 glob 去遍历目录及子目录下，所有后缀为 xls 或者 xlsx 的文件，再直接读取： 12345678910111213import pandas as pdimport globfiles = glob.glob('**/*.xlsx', recursive=True)files = files + glob.glob('**/*.xls', recursive=True)print(files)for file in files: print(file) df_sheet_map = pd.read_excel(file, None) sheets = list(df_sheet_map.keys()) for sheet in sheets: df_sheet = df_sheet_map[sheet] print('--',sheet, ':', len(df_sheet.index)) output: 1234567891011121314151617['Test1.xlsx', 'Test2.xlsx', 'Test3.xls', 'sub_folder\\\\Test4.xls']Test1.xlsx-- Sheet1 : 6-- Test : 4-- Demo : 7Test2.xlsx-- Sheet1 : 16-- Test : 8-- Demo : 11Test3.xls-- Sheet1 : 10-- Test : 5-- Demo : 8sub_folder\\Test4.xls-- Sheet1 : 10-- Test : 5-- Demo : 8 将结果写到 output 文件中信息都有了，想要写到文件中，最简单写到 txt，这里我们用了 pandas，就直接写到 excel 中好了。 123456789101112131415161718192021222324252627282930313233import pandas as pdimport globdict_output = {}dict_output['fileName'] = []dict_output['sheet'] = []dict_output['rows'] = []files = glob.glob('**/*.xlsx', recursive=True)files = files + glob.glob('**/*.xls', recursive=True)print(files)for file in files: print(file) df_sheet_map = pd.read_excel(file, None) sheets = list(df_sheet_map.keys()) for sheet in sheets: df_sheet = df_sheet_map[sheet] print('--',sheet, ':', len(df_sheet.index)) dict_output['fileName'].append(file) dict_output['sheet'].append(sheet) dict_output['rows'].append(len(df_sheet.index))df_output = pd.DataFrame(dict_output)df_output.to_excel(&quot;output.xlsx&quot;, sheet_name='details', index=False)# with pd.ExcelWriter('output.xlsx') as writer: # doctest: +SKIP# ... df1.to_excel(writer, sheet_name='Sheet_name_1')# df2.to_excel(writer, sheet_name='Sheet_name_2' output in excel: fileName sheet rows Test1.xlsx Sheet1 6 Test1.xlsx Test 4 Test1.xlsx Demo 7 Test2.xlsx Sheet1 16 Test2.xlsx Test 8 Test2.xlsx Demo 11 Test3.xls Sheet1 10 Test3.xls Test 5 Test3.xls Demo 8 sub_folder\\Test4.xls Sheet1 10 sub_folder\\Test4.xls Test 5 sub_folder\\Test4.xls Demo 8 参考: python 获取 excel 文件的所有 sheet 名字 pandas DataFrame的创建方法","link":"/2019/08/29/2019/08/2019-08-29-CountRowsInExcel/"},{"title":"Recent Summary 08&#x2F;28&#x2F;2019","text":"搬家最近从物华搬到了政苑B区，就在银泰的边上，房租果断的长了一些。因为是年付，所以优惠了些，还能接受，不过来年就不知道还有没有优惠了。 还好有XM帮忙收拾，周末的时间就弄好了。如果是我自己一个人的话，很多东西都不知道要塞在袋子里多久才会收拾出来，哈哈哈。 搬家是真的折腾。原来的物化不过也只住了一年而已。被迫的搬家是租房最大的痛点之一。 摇号这两天几个比较火的楼盘又是万人摇，像和光尘樾之类的总价还是太高了。 像天都城这样的价格倒真的可以试一试，只是最近心思都没有在这上面，消息得到的晚，就没有去尝试了，可能大概率是去当分母吧。 年初前几个月倒是看了一些，总价是最大的问题。不过现在反而不是太着急，还是看着吧。 工作没有想到，之前不加班，每天开开心心的会被人嫌弃不上进。现在心思在工作上多一些，也会被吐槽，想不通，哈哈哈。 今年已经过了一大半了，目前总结的话就是“困惑”吧。对于工作，我不知道自己是不是做了没有意义的选择，没有必要的折腾。 或许做测试开发的工作最适合自己？还只是转型的必经之路？不知道，也不去想了，做好自己能做的事就好了，没必要想太多。 不管Firecall还是322，都费了自己不少心力，收获与经验的积累也是实打实的，只是真的过程有点累，心累，尤其是本地做开发的破电脑，抓狂。话说这两天这电脑还是有点乖的。 不过一直加班，如果之后还是这样累肯定是不行的，不知道有没有契机或者会习惯了？哈哈哈 家人本来预想的是平均每个月可以回家一趟，结果今年工作上牵绊太多，时间上反而不太自由了。 这几个星期一直想回去，却总没成行，就等中秋和国庆了。中秋一定要回去一趟的。 母亲一个人在家，最近小舅舅又出事，母亲心里也挺难受的吧，我也没能在身边，哎。 生活与感情今年最大的archievement或许就是XM了吧，不过之后怎样也没法预见，加油吧~ 最后Wish World Peace!","link":"/2019/08/28/2019/08/2019-08-28-RecentSummary/"},{"title":"Text Similarity","text":"背景在工作中有需要从 comment 中提取所需要的值，是通过前缀来判断的，由于注释是人为输入的，所以很多时候会有一些拼写错误。目前是通过写死前缀的字符串依次遍历来达到目的。 比如我们要的是 as of date mm/dd/yyyy，但也想要接受 as of data mm/dd/yyyy 那这样的话，就可以兼容许多拼写的错误。 字符串的相似性比较应用场合很多，像拼写纠错、文本去重、上下文相似性等。 评价字符串相似度最常见的办法就是：把一个字符串通过插入、删除或替换这样的编辑操作，变成另外一个字符串，所需要的最少编辑次数，这种就是编辑距离（edit distance）度量方法，也称为Levenshtein 距离。海明距离是编辑距离的一种特殊情况，只计算等长情况下替换操作的编辑次数，只能应用于两个等长字符串间的距离度量。 其他常用的度量方法还有 Jaccard distance、J-W 距离（Jaro–Winkler distance）、余弦相似性（cosine similarity）、欧氏距离（Euclidean distance）等。 difflibpython 有内置的 difflib 来判断相似度，非常方便。 1234567891011121314import difflibquery_str = 'as of date 11/12/2019's1 = 'as of data 11/12/2019's2 = 'as fo date 11/12/2019's3 = 'similar date 11/12/2019'print(difflib.SequenceMatcher(None, query_str, s1).quick_ratio()) # 0.9523809523809523print(difflib.SequenceMatcher(None, query_str, s2).quick_ratio()) # 1.0print(difflib.SequenceMatcher(None, query_str, s3).quick_ratio()) # 0.8181818181818182&quot;&quot;&quot;第一个参数是想要忽略的字符，可以不算在其中。seq = difflib.SequenceMatcher(lambda x:x=&quot; &quot;, a, b)&quot;&quot;&quot; fuzzywuzzyfuzzywuzzy 是一个第三方库，提供了更多的方法进行不同的字符匹配需求。 可以看到 fuzzywuzzy 的默认匹配区分度更直观点。 1234567891011from fuzzywuzzy import fuzzquery_str = 'as of date 11/12/2019's1 = 'as of data 11/12/2019's2 = 'as fo date 11/12/2019's3 = 'similar date 11/12/2019'print (fuzz.ratio(query_str, s1)) #95print (fuzz.ratio(query_str, s2)) #95print (fuzz.ratio(query_str, s3)) #77 除些外，还有 partial_ratio(), token_set_ratio(),partial_token_sort_ratio()等方法。 Levenshtein1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import Levenshteinquery_str = 'as of date 11/12/2019's1 = 'as of data 11/12/2019's2 = 'as fo date 11/12/2019's3 = 'similar date 11/12/2019'# hamming距离，str1和str2长度必须一致，描述两个等长字串之间对应位置上不同字符的个数print(Levenshtein.hamming(query_str, s1)) # 1print(Levenshtein.hamming(query_str, s2)) # 2#print(Levenshtein.hamming(query_str, s3)) #ValueError: hamming expected two unicodes of the same length# 编辑距离，描述由一个字串转化成另一个字串最少的操作次数，在其中的操作包括 插入、删除、替换print(Levenshtein.distance(query_str, s1)) # 1print(Levenshtein.distance(query_str, s2)) # 2print(Levenshtein.distance(query_str, s3)) # 7# 计算莱文斯坦比print(Levenshtein.ratio(query_str, s1)) # 0.9523809523809523print(Levenshtein.ratio(query_str, s2)) # 0.9523809523809523print(Levenshtein.ratio(query_str, s3)) # 0.7727272727272727# 计算jaro距离print(Levenshtein.jaro(query_str, s1)) # 0.9682539682539683print(Levenshtein.jaro(query_str, s2)) # 0.9841269841269842print(Levenshtein.jaro(query_str, s3)) # 0.8151023694501954# Jaro–Winkler距离print(Levenshtein.jaro_winkler(query_str, s1)) # 0.9968253968253968print(Levenshtein.jaro_winkler(query_str, s2)) # 0.9888888888888889print(Levenshtein.jaro_winkler(query_str, s3)) # 0.8151023694501954 最后回过头来说，使用字符串匹配度来看字符串是不是我们想要的，还是有误差的，理论上还是用正则, 或者说固定的字符更精准。 参考： Python 字符串相似性的几种度量方法 python比较字符串相似度 python: fuzzywuzzy学习笔记","link":"/2019/11/24/2019/11/2019-11-24-TextSimilarity/"},{"title":"My 2019","text":"又是 一年的末尾，冷热交替，雨下个不停，今天稍稍停歇。 今天是冬至，包了一些饺子，韭菜鸡蛋馅的。试着做肉圆，但不是很成功。 01今年过的很“困惑”，不是很顺心。我一直在反思，到底是哪里的问题。 来 PXO 快近一年。刚转组的那一段时间，特别的不适应，很困惑，想不通。 我是谁？我在哪里？我在干什么？ 自己选择的意义是什么呢，发现支撑自己的理由和信念都坍塌了，不存在了。 最直接的是没有了相对高自由度和性能的远程机器来支持 daily work，本地主机用的更是糟心。感觉做的事情也没有在 TE 的时候有意思。。。 回过头看，这是数据团队特性决定的，基本上大部分的任务主脉络都是有不同的源数据(Source/Reference Data)。然后通过不同的需求与逻辑，将数据整合，最终提供给客户。所以也会有 daily/weekly/monthly/…的 Report。 自己一开始做 PA 的时候，更多的精力是花费在沟通与获取各种数据的来源上。更让人难受的是，这东西没有特别明确的需求。。。最开始做的时候还是很崩溃的。。。好在后面也算是架子搭起来了，主要逻辑写在 Teradata 的 View 里，使用 python 来辅助，主要是数据的清洗与导入导出。 总而言之，不在自己的节奏里，做什么味道都不对。 之后从 GY 手上接手 SRD 这一块，摸着石头过河，一开始也犯了一些低级错误。不过不管是在所需的技术点上，还是开发流程，沟通方式，默认规则等方面，还是有一些收获与感悟。其中对于 CDT4 的使用，还有 python 数据操作，尤其是 pandas 库的使用都熟悉了一些。 所以今年工作上主要就这两块的内容，而 PA 一直说有新的需求但很多时候都是拖拖等等== 02今年还有另外重要的“收获”，那便是 XM。她是一个“傻”姑娘 很感激她，在我低谷时，能够包容我的负面情绪，倾听我的烦闷心声。 她是一个善良而敏感的姑娘。有时候她的想法会不太一样，这主要是因为生活经历的差别，每个人都有自己铠甲和软肋。 我也对她说过，我们有些地方有些像，所以才能理解对方的一些想法。 现在我和 XM 在一个组，有人觉得不妥，我们也觉得没必要，明年应该会有变动了。 开始一段感情很难，未来的路也不平坦，但求相遇相知，问心无愧。 03今年，尤其早些时候，也看了些杭州的楼盘，也想着是不是可以摇摇看，不过最后还是打消了自己的念头。 房子不是车子，价位就摆在那边，我连入门的门槛都够不着。 尤记得 14 年刚回杭州工作的时候，和刚子去看的那几个楼盘。 想来好笑，当时想着自己攒几年钱，家里凑一点，就可以上车了。 不过反过来说，如果当然房价已然像现在这么高的话，我也不会回杭州来的吧。 也许，也许父母现在的情况也会有点不一样吧。 04晚上给母亲打了个电话，这两天她的状态不是很好。下周看情况，休假的时候可以回家一趟，虽然春节也就快到了。 今年小舅舅的事，对母亲打击也不小，只能怪舅舅他自己太不争气了。 本来想着今年一两个月就回家一趟的，却也没有做到。 今年母亲受了两次伤，怕我担心，事后才和我说的。 我知道她现在最大的心愿就是我早点结婚生娃，但这些事是急不来的。 以前谈到类似话题，我还会和母亲拌嘴，现在我都不争辩了，只希望她不到太操心。 有时候手机不能联系不到母亲，我还是会着急。 家里囤了些啤酒，有两次微信视频时我看到母亲在喝，便知那天她心情不好了。 上次家里灯和马桶坏了，我去买了配件，回去修好了，母亲很开心。 今年台风，楼顶地漏堵住了，母亲自己爬上通的。。。 05日子总是一天天过去。2020 快来了。 愿尽自己最大的努力，给自己和家人更好的生活。 未来的事，未来再说。2019 倒计时。","link":"/2019/12/22/2019/12/2019-12-22-My2019/"},{"title":"pyodbc unicode issue","text":"这两天SRD这边有文件中存在乱码，导致那一行数据插入失败。 下面是我晚上在家里电脑重现的时候信息,所以是中文的： 1pyodbc.Error: ('HY090', '[HY090] [Microsoft][ODBC 驱动程序管理器] 无效的字符串或缓冲区长度 (0) (SQLBindParameter)') 我一开始以为是这列数据的字符串长度过长，有1万多个字符，但是我用别的值试了一下，是没有问题的。 然后我尝试着使用print输出来看一下，竟然直接error了。下面是例子： 123text = &quot;you are right \\udef6 thanks&quot;print(text)# UnicodeEncodeError: 'utf-8' codec can't encode character '\\udef6' in position 14: surrogates not allowed 目前连接Sql Server数据库用的是pyodbc，而现在在插数据的时候挂了。 我想到这个问题可能与pyodbc处理unicode的编码有关系，就去找官方的资料。 最后发现在pyodbc的github库的issue list里，已经存在类似的问题 #617。 (在这里吐槽一下，现在公司把github都封掉了，不让访问，怕有人把公司代码上传上去。。。现在查到一些链接只能用google的快照打开。。。) 就像issue里描述的，对于一些特殊字符(e.g. 🎥, ☯)，当使用fast_executemany，并且要插的数据类型是varchar(max)的时候， pyodbc不能很好的处理。 如果不使用这个模式(默认fast_executemany=False)，就能正常的插入到数据库中（虽然还是乱码），不会报错了。 但如果不使用这个模式，执行插入的操作就会很慢。 于是我又发现了一个孪生的库pypyodbc。可以认为他是pyodbc的python版本的实现，API使用基本一样，可以直接替换： 1import pypyodbc as pyodbc 当然， 有时候我不想换个库，只能等pyodbc更新修复，或者自己编译一个作者修复过的版本v-makouz@606b4a9, 又或者自己预处理一下字符串。 乱码是处理数据的时候常会遇到的，最好有统一的处理方式。 PS：对于纯英文环境，我们完全可以把编码值&gt;255的都当成特殊字符去掉。","link":"/2019/12/24/2019/12/2019-12-24-PyodbcUnicode/"},{"title":"Regex in python","text":"Regex in python 简介正则表达式 是在文本处理的时候非常高效有用的一种方式，一般的编程语言都会内置相应的模块。 不同的编程语言使用方式不尽相同，核心的表达式模式都是一致的。 最近使用python生成Message，有用到正则表达式，今天简单的总结下python使用的方式。对于表达式本身的使用，之后有时间再详细介绍。 re.match首先介绍下re.match(pattern, string, flags=0)。 如果能匹配上，这个方法将会返回match对象, 如果失败的话就会返回None。 看下面这个例子，我在创建message的时候，希望通过配置类似于Suffix[1-100]这样的方式来生成message中的变量。我需要得到前缀Suffix,最小值1,最大值100。 12345678910111213141516171819import reline = &quot;Suffix[1-100]&quot;matchObj = re.match( r'(.*)\\[(.*?)-(.*?)\\]', line, re.M|re.I)if matchObj: print (&quot;matchObj.group() : &quot;, matchObj.group()) print (&quot;matchObj.group(1) : &quot;, matchObj.group(1)) print (&quot;matchObj.group(2) : &quot;, matchObj.group(2)) print (&quot;matchObj.group(3) : &quot;, matchObj.group(3))else: print (&quot;No match!!&quot;)'''outputmatchObj.group() : Suffix[1-100]matchObj.group(1) : SuffixmatchObj.group(2) : 1matchObj.group(3) : 100''' 通过match方法，利用group()就能很方便的把固定格式的数据提取出来。 re.search在字符串中利用正则表达式查找匹配相可以使用re.search(pattern, string, flags=0) 那么search和match有什么区别呢？ 其实从名字上就可以看出来，search是在整个字符串里找符合匹配的就可以，而match要求整个字符串来匹配。 1234567891011121314import reline = &quot;Cats are smarter than dogs, but I like dogs:) &quot;;matchObj = re.match( r'dogs', line, re.M|re.I)if matchObj: print (&quot;match --&gt; matchObj.group() : &quot;, matchObj.group())else: print (&quot;No match!!&quot;)searchObj = re.search( r'dogs', line, re.M|re.I)if searchObj: print (&quot;search --&gt; searchObj.group() : &quot;, searchObj.group())else: print (&quot;Nothing found!!&quot;) re.subre.sub(pattern, repl, string, max=0)用来查找和替换字符串。max是可选的，表示替换的个数，默认是全部替换。 1234567891011import rephone = &quot;2004-959-559 # This is Phone Number&quot;# Delete Python-style commentsnum = re.sub(r'#.*$', &quot;&quot;, phone)print &quot;Phone Num : &quot;, num# Remove anything other than digitsnum = re.sub(r'\\D', &quot;&quot;, phone) print &quot;Phone Num : &quot;, num string就像其它编程语言一样，python中string已经内置了一些和文本有关的方法，可以直接方便使用。s.startswith(prefix[,start[,end]])s.endswith(suffix[,start[,end]])s.find(sub[,start[,end]])s.split([sep])… Flags上面的例子中，大家发现了又用到re.M|re.I,这些常量是用来定义匹配模式的， Sr.No. Modifier Description 1 re.I 大小写敏感 2 re.L 使用Local,影响\\W \\w和\\B \\b 3 re.M ^匹配行结尾 $匹配行开头 4 re.S 让.匹配任何字符，包括换行 5 re.U 使用Unicode编码 影响\\W \\w和\\B \\b 6 re.X 允许cuter语法，忽略空白(在[]或者\\）,并将#做为一般正常字符 更具体的信息参考：Python - Regular Expressions","link":"/2018/05/07/2018/05/2018-05-07-RegexInPython/"},{"title":"ACM with Python","text":"使用python编写acm题目ACM一般来说都是用C或者C++来编写，因为国内的大学入门教程就是以C和C++为主，而且编译后的执行效率也比较高。但是很多的Online Judge都支持别的语言，例如Java，Python，Perl，Pascal，PHP，FPC，C#等等。 省内我们之前常用的几个学校的OJ系统(浙大，杭电，工大)，只有浙大的才支持Python，而且只有2.7.3,不支持Python3+。不过大同小异，下面就用简单的例子来演示下用python来写acm。 1001 A+B Problem哈哈，就以这个最简单的a+b问题为例，下面是一个标准的输入输出样式，读取样例，输出结果。 123456while True: try: a, b = map(int, raw_input().strip().split()) print a + b except EOFError: break 本来还想再写几个的，但暂时没发现特别简单的，哈哈哈哈哈！ Python输入样例参考：Python - Input","link":"/2018/05/09/2018/05/2018-05-09-ACM-Python/"},{"title":"Catch CPU&#x2F;Memory Monitor","text":"根据log中的实际取得monitor中的数据。现在有一个完整的导数据的log，类似于如下ImportLog.txt： 12345678910111213141516...略...2018-06-14 00:07:58,156 [INFO ] -----START IMPORT-----2018-06-14 00:07:58,515 [INFO ] xxx2018-06-14 00:10:39,065 [INFO ] xxx2018-06-14 00:10:39,081 [INFO ] Import Time(s):160.552018-06-14 00:10:39,128 [INFO ] xxx2018-06-14 00:10:39,128 [INFO ] Finished:xxxAAxxx2018-06-14 00:10:39,128 [INFO ] Import Time(s):160.962018-06-14 00:10:39,128 [INFO ] -----END IMPORT-----2018-06-14 00:10:44,201 [INFO ] -----START IMPORT-----2018-06-14 00:10:46,590 [INFO ] xxx\\\\IMPORT.exe BBB2018-06-14 00:57:30,360 [INFO ] Finished:\\xxx_BBB_xxx2018-06-14 00:57:30,360 [INFO ] Import Time(s):2803.772018-06-14 00:57:31,125 [INFO ] -----END IMPORT-----...略...&lt;!-- more --&gt; 而整个过程有监控数据类似如下Monitor.csv： 123456789Time,CPU(%),Memory(%)2018/06/13 23:51:17,2.7,12.172018/06/13 23:51:19,0.0,12.162018/06/13 23:51:21,0.0,12.162018/06/13 23:51:23,0.0,12.142018/06/13 23:51:25,0.4,12.152018/06/13 23:51:28,0.0,12.152018/06/13 23:51:30,0.0,12.15...略... 那么现在的问题是只需要在跑BBB的时候的那段时间的监控数据，于是临时写了个简单的HardCode的过程达到目的，主要用到了datetime对时间的处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&quot;&quot;&quot;author: xichecreate at: 06/14/2018description: Catch monitor data from whole proces&quot;&quot;&quot;from datetime import datetime# datetime_object = datetime.strptime('Jun 1 2005 1:33PM', '%b %d %Y %I:%M%p')dateformat1 = &quot;%Y-%m-%d %H:%M:%S&quot; #&quot;2018-06-13 23:51:17&quot;dateformat2 = &quot;%Y/%m/%d %H:%M:%S&quot; #&quot;2018/06/13 23:51:52&quot;def __main__(): dt_start = None dt_end = None lines_new = [] with open(&quot;ImportLog.txt&quot;, &quot;r&quot;) as f: lines = f.readlines() for line in lines: if(&quot;BBB&quot; in line and &quot;Finished&quot; in line): dt_end = datetime.strptime(line[0:19], dateformat1) break if(&quot;BBB&quot; in line): dt_start = datetime.strptime(line[0:19], dateformat1) with open(&quot;Monitor.csv&quot;, &quot;r&quot;) as f: lines = f.readlines() dt_current = None for line in lines: try: dt_current = datetime.strptime(line[0:19], dateformat2) if(dt_current &lt; dt_start): continue elif(dt_current &gt; dt_end): break #之前竟然是continue。。。。 else: lines_new.append(line) except: print(&quot;No expect formated:{}&quot;.format(line)) with open(&quot;Monitor_BBB.csv&quot;, &quot;w&quot;) as f: f.writelines(lines_new) # dt_start = datetime.strptime(dt_start_str, dateformat1) # dt_end = datetime.strptime(dt_start_str, dateformat2) # a = datetime.strptime(&quot;2018-06-13 23:51:17&quot;, &quot;%Y-%m-%d %H:%M:%S&quot;) # b = datetime.strptime(&quot;2018/06/13 23:51:52&quot;, &quot;%Y/%m/%d %H:%M:%S&quot;) # print(a, b)__main__()","link":"/2018/06/14/2018/06/2018-06-14-CatchMonitorData/"},{"title":"Multi-parameters performance test","text":"Multi-variables performance test最近一直在忙，都没有时间总结和改进现在的测试流程，瓶颈已经显现出来。下周的一个主要的任务就是大批量的测试不同虚拟机下不同参数执行任务的性能,从而得到CPU和Memory消耗适合的一些参数，并做为benchmark。可以预见之后还有类似的测试的任务，那必要要提前做好改变。今天主要是把思路整理出来。 背景与需求之前已经有经过多次改良的脚本，可以执行在一个虚拟机上的完整测试任务，针对不同的机器，都有对应的配置文件。当测试流程开始时，会读取对应的配置，而且这些配置都是存放在Share Folder下对应的机器名文件夹下，那么每次测试之前，都需要手动去把对应的文件改成要想要的数据（主要就2个配置文件），然后执行任务，结束之后会有对应的Test Result和Monitor Data以供收集数据。 那么，接下来想要实现的就是一次性配置多个参数，不同的机器能够自动依次执行测试任务，最后将所有的测试结果合在一处，形成报表，而不再需要人工再去采集一下。 实现思路数据库目前DB需要人工去回滚，因为之前数据库资源紧张，经常需要去切换。如果写在代码中，容易失误而restore错数据库，所以用sql人工去执行回滚，当时为适合的方案。那么接下来要自动连续测试的话，首先第一步就是把自动回滚加入到每次测试用例执行前。 需要考虑的是使用什么方式： 可以直接用目前的python去写回滚脚本，并加入到整体流程代码中。 可以先用Powershell或Python写一个独立的回滚脚本，并可以加入系统Scheduler中，而主程序只是调用，这样可以将回滚任务分离。比如具体的DB信息，回滚基于的Backup都不需要在主程序中设置。我比较偏向这个选择，这样回滚数据库功能更容易共享，可以给别的测试任务使用。 配置文件原来的流程中是去基于机器名目录下去寻找固定名字的配置文件(e.g. config.ini)。那么现在必须增加配置选择，也有几种方案：首先必要的是新增变量组合的配置，这个可以放在主程序配置中，也可以在Share Folder下单独配置e.g. VM_Name/parameters.ini 类似于 1234567[TC1]configfile1.key1 = value1configfile2.key2 = value2[TC2]configfile1.key1 = value3configfile2.key2 = value4... 而在上面的前提下： 在VM名目录下按变量顺序新加文件夹，e.g. V1_V2/config.ini,引导主程序使用该目录下的配置。 建立一个config.ini模板，比如VM_Name/config.template.ini，然后将读取后的配置填充进去，每次都生成新的VM_Name/config.ini。这样的话，原来的主程序并不需要改变，只需要把新功能添加进去就好，而且之前写过类似的，可以拿来复用，我偏向这个方式。 多台虚拟机信息交互上面想通了，基本上一台机器的自动多次任务执行就没有什么问题了，主要是去实现。但现在涉及到不同机器之前先后执行顺序，一台执行完需要另一台机器接着做。 之前做过一个类似的，大体思路可以拿来用: 将不同机器中的主程序加入Scheduler中 建立一个入口批处理bat，在其中先后调用不同VM中的Scheduler去执行主程 每次触发一个Scheduler运行之后利用timeout等方式等待(所以Delay时间必须要大于前一个所有test都完成的时间，不然会有conflict) 前面3的改进版本： 死等固定时间肯定是很不明智的，尤其是在消耗时间不明确的情况下，这样效率非常低。那么这个时候，在每个主程序中加入一个操作，就是在每次轮到自己执行的时候，就会在一个share folder下生成一个信息文件表示自己开始执行号，这样在接下来的VM要执行的时候就会block住，等待上一个机器完成的信息，之后再继续自己执行 其实，现在的job benchmark就是用类似的思路。 最后，TestResult目前，每台机上跑的时候都会在本机生成以时间为名字的目录，并将该次的TestResult生成在该目录下。那么接下来的目标是： 生成结果统一生成到Share Folder，这样方便管理，而且大家都能看到。 生成结果统一放在一个目录日期目录下，之后再以每台机器自己的名字为目录，再在每台机子下用参数值做目录，最后的Result就放在这个目录下。 optional:将所以的结果统一个Excel文件中，放在root目录下，这样每轮跑完就可以直接查看结果。 结尾，晚安！","link":"/2018/06/14/2018/06/2018-06-16-MultiVarsPerformanceTest/"},{"title":"Create Excel with line chart by openpyxl","text":"Create Excel with line chart by openpyxl之前有用过openpyxl做了简单的介绍。 但是我今天在尝试画chart的时候，发现没有很好用的api可以参考，可能是我没找到吧，一些细节只能google，或者从源代码里找。 好在之前在做AMS报表的时候，有用ChartFX画过一些图表，大体上的api设计是一致的。 需求&amp;背景现在多虚拟机连续性能测试已经搭好了，当时自己给自己定了一个optional的目标，就是把最后的log也可视化，比如以总结的excel来展现。 这样就可以省去人工处理的时间，不然每次都要自己打开csv，add chart还是有点麻烦，如果直接打开excel就能看到图还是很爽的。 当然，今天只是完成了第一步，之后有需要和时间的话，再把其它test result整合进来。 实现任务很简单，就是把下面的csv转成excle并画图。 123456Time,CPU(%),Memory(%)2018/06/17 23:47:11,17.0,16.712018/06/17 23:47:13,1.5,17.012018/06/17 23:47:15,2.9,17.042018/06/17 23:47:17,23.6,17.222018/06/17 23:47:19,27.0,18.54 下面直接上代码，主要用了openpyxl. 像lib.cmutils_io这种是我自己写的类库，比如CSVUtils就是我用标准库csv封装了一些常用的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import openpyxlfrom datetime import datetimefrom lib.cmutils_io import CSVUtilsfrom openpyxl.chart import ( LineChart, Reference,)from openpyxl.chart.axis import DateAxisfrom openpyxl import WorkbookDATE_TIME_FORMAT = &quot;%Y/%m/%d %H:%M:%S&quot; #&quot;2018-06-13 23:51:17&quot; def test_monitor(): wb_tpl = Workbook() ws = wb_tpl.active # ws = wb_tpl.worksheets[0] ws.title = &quot;Monitor&quot; &quot;&quot;&quot; [ [2018/06/17 23:47:11,17.0,16.71], [2018/06/17 23:47:13,1.5,17.01], [2018/06/17 23:47:15,2.9,17.04], ... ] &quot;&quot;&quot; data_list = CSVUtils.readCSVRowsList(&quot;Monitor.csv&quot;) _max_row = len(data_list) _max_col = 3 for row_index, rows in enumerate(data_list): for col_index, value in enumerate(rows): if(row_index == 0): ws.cell(row=row_index+1, column=col_index+1).value = value else: if(col_index == 0): ws.cell(row=row_index+1, column=col_index+1).value = datetime.strptime(value, DATE_TIME_FORMAT) ws.cell(row=row_index+1, column=col_index+1).number_format = 'HH:mm:ss' else: ws.cell(row=row_index+1, column=col_index+1).value = float(value) chart1 = LineChart() chart1.title = &quot;CPU/Memory Mornitor&quot; chart1.style = 2 # default style when new a line chart chart1.height = 15 # default is 7.5 chart1.width = 30 # default is 15 chart1.legend.position = &quot;b&quot; chart1.y_axis.scaling.min = 0 chart1.y_axis.scaling.max = 100 # chart1.y_axis.title = 'Pecent' # chart1.x_axis.title = 'Time' # set y-axis data = Reference(ws, min_col=2, min_row=1, max_col=_max_col, max_row=_max_row) chart1.add_data(data, titles_from_data=True) # set time as category(x-axis) cats = Reference(ws, min_col=1, min_row=1, max_row=_max_row) chart1.set_categories(cats) # Style the lines # s1 = c1.series[0] # s1.marker.symbol = &quot;triangle&quot; # s1.marker.graphicalProperties.solidFill = &quot;FF0000&quot; # Marker filling # s1.marker.graphicalProperties.line.solidFill = &quot;FF0000&quot; # Marker outline # s1.graphicalProperties.line.noFill = True s2 = chart1.series[0] s2.smooth = True # Make the line smooth # s2.graphicalProperties.line.solidFill = &quot;00AAAA&quot; # s2.graphicalProperties.line.dashStyle = &quot;sysDot&quot; # s2.graphicalProperties.line.width = 100050 # width in EMUs s2 = chart1.series[1] s2.smooth = True # Make the line smooth # chart1.x_axis.tickLblPos = &quot;low&quot; # chart1.x_axis.tickLblSkip = 3 # whatever you like ws.add_chart(chart1, &quot;F10&quot;) dest_filename = 'test2.xlsx' wb_tpl.save(filename = dest_filename) test_monitor() 未完待续:)","link":"/2018/06/18/2018/06/2018-06-18-CreateLineChartByOpenpyxl/"},{"title":"Import Messages","text":"Import Messages工作中需要导Message的性能测试作为benchmark，最早的时候有同事使用运行批处理的方式，一些配置和环境都是固定的，也不够灵活。 所以我趁着之前准备数据的基础，我搭建了一个简单的相对灵活的自动化流程。到后期稳定之后，可以做到run at everywhere。当然，目前回滚数据库这一块还没有做到代码里，最后有需要的话可以加进来。 整个流程其实很简单： 读取配置文件 Check环境，将一些需要自定义的配置文件指向一个固定的公共folder 启动基于java的Message Service，来接收messages 按配置文件中的文件夹信息，根据其下的txt文件名顺序依次导入message，并check数据库和import log，输出到log文件中 更具体的信息参考：README 主要代码：Import Messages","link":"/2018/04/19/2018/04/2018-04-19-ImportMessages/"},{"title":"Blog First Day","text":"新建GitHub博客这是我第一次新建自己的GitHub博客，感谢@qiubaiying的分享利用 GitHub Pages 快速搭建个人博客 今天花了点时间，最后还是搭起来了，很开心。 在这个过程中遇到了一点小问题，比如按照攻略过程中，发现404NotFound。 现在想来应该是Github还在编译处理过程中，没有那么快，还是要有点耐心。 就像上次使create-react-app 一样，还以为使用没用，结果还是因为网络慢，在weitao提醒下吃个饭回来就好了，哈哈哈:)","link":"/2018/04/18/2018/04/2018-04-18-BlogFirstDay/"},{"title":"Analyse CSV log","text":"AnalyseCSVLogByMonth基于之前的Message Import测试工具，会生成原始的log文件TestResult.csv。 1234path,rows,timeused(s),timeused(m),avgtime,messageDateStep01,60000,270.18,4.50,0.0045,20161231Step02,60000,197.96,3.30,0.0033,20171231... 而现在又了新的需求，希望能统计一下基于月份的运行时间，所以我简单写了个python脚本，来解析csv文件，将同一个月份的数据group起来。 主要流程与算法运行流程1234resultList = readCSVRowsList(&quot;TestResult.csv&quot;)resultList = initAndSortByMessageMonth(resultList, 5)resultList = combineRowsByColumn(resultList, 5)writeToCSVFile(&quot;out.csv&quot;, resultList) 将csv文件加载到二维数组中 将MessageDate截取成yyyyMM的格式，并加入#，然后返回据此排序好的二维数组(List)。同时在代码里可以看到我还做了特殊处理，这个是具体的需求。通用的处理可以直接删除掉。 将相同月份的数据Combine到一起再返回新的数组回来 将最后的数据写入到输出文件TestResultCombined.csv 数据Combine算法代码中我已经做了注释，不再过多解释，就是逐行处理，结果放到新的list里。 1234567891011121314151617181920212223242526272829303132333435def combineRowsByColumn(csvRowsList, columnIndex = 5): resultList = [] for i, row in enumerate(csvRowsList): # keep csv header and do nothing if(i == 0): resultList.append(row) continue currentValue = row[columnIndex] # get the month of last row if(i - 1 &gt;= 0): lastValue = csvRowsList[i-1][columnIndex] else: lastValue = None # get the month of next row if(i + 1 &lt; len(csvRowsList)): nextValue = csvRowsList[i+1][columnIndex] else: nextValue = None # if last month is the same with current month, combine them if(lastValue != None and lastValue == currentValue): for j in range(len(row)): #ignore the month column if(j == columnIndex): pass else: # if is value, calculate them, if not ,just append them with '~' if(isFloat(row[j])): row[j] = &quot;{:.2f}&quot;.format(float(row[j]) + float(csvRowsList[i-1][j])) else: row[j] = csvRowsList[i-1][j] + &quot;~&quot; + row[j] # if next value is different, just add current row to the new list if(currentValue != nextValue): resultList.append(row) return resultList 更具体的信息参考：README主要代码：CSVLogAnalysis","link":"/2018/04/20/2018/04/2018-04-20-CSVLogAnalysis/"},{"title":"Manage txt files","text":"处理txt文件在最近的项目中，需要创建数据用来测试。有时生成的文件太大，需要分散到小的文件中，或者反之。 之前已经有用java写的生成数据的代码，但是改起来不太方便，也没有针对性，用python就方便很多。 代码其实很简单，以后工作中有用到的话，可以直接拿来用，不用再花时间造轮子了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#/usr/bin/python3&quot;&quot;&quot;author: xichecreate at: 04/21/2018description:There are two operations in this script 1. Aggregate several txt files to one file by the file name order which contains the same part name 2. Seperate one large txt file to several small files, you could control how many files you want to generateChange log:Date Author Version Description04/21/2018 xiche 1.0 Set up this script&quot;&quot;&quot;import osimport glob# Aggregate all the files which name contains the name as belowAGGREGATED_INPUT_FILE_PART_NAME = &quot;WKLMF&quot;AGGREGATED_OUTPUT_FILE_NAME = &quot;AggregatedFiles.txt&quot;# The original file which want to seperate to several filesSEPERATED_INPUT_FILE_NAME = &quot;AggregatedFiles.txt&quot;# Output files which like Seperated_1.txt/Seperated_2.txt/Seperated_3.txt/...SEPERATED_OUTPUT_FILE_NAME = &quot;Seperated.txt&quot;SEPERATED_OUTPUT_FILE_NUMS = 6def getFileNameWithoutSuffix(fullPath): &quot;&quot;&quot;Get filename without suffix: Filename.txt =&gt; FileName&quot;&quot;&quot; lastIndex = fullPath.rfind('.') return fullPath[:lastIndex]def getFileNameSuffix(fullPath): &quot;&quot;&quot;Get file name suffix like .txt&quot;&quot;&quot; lastIndex = fullPath.rfind('.') return fullPath[lastIndex:]def writeRowsListToFile(filePath, fileRows): with open(filePath, 'w') as f: for rows in fileRows: for row in rows: f.write(&quot;%s\\n&quot; % row) # def writeRowsToFile(filePath, rows): # with open(filePath, 'w') as f: # for row in rows: # f.write(&quot;%s\\n&quot; % row) def aggregateTxtFilesToOneFile(inputFilesPartName, outputFileName): &quot;&quot;&quot;Main function to aggregate txt files&quot;&quot;&quot; totalRows = [] list_of_files = glob.glob('./*{}*'.format(inputFilesPartName)) for filePath in list_of_files: with open(filePath) as f: lines = f.read().splitlines() totalRows.append(lines) writeRowsListToFile(outputFileName, totalRows)def seperateTxtFilesToSeveralFiles(inputFileName, outputFileName, outputFileNums): &quot;&quot;&quot;Main function to seperate txt files&quot;&quot;&quot; totalRows = [] totalRowNums = 0 rowsEachFile = 0 with open(inputFileName) as f: totalRows = f.read().splitlines() totalRowNums = len(totalRows) if(totalRowNums % outputFileNums == 0): rowsEachFile = int(totalRowNums/outputFileNums) else: rowsEachFile = int(totalRowNums/outputFileNums) + 1 for i in range(outputFileNums): with open(&quot;{}_{}{}&quot;.format(getFileNameWithoutSuffix(outputFileName), i+1, getFileNameSuffix(outputFileName)), 'w') as f: for j in range(rowsEachFile): f.write(&quot;%s\\n&quot; % totalRows[i *rowsEachFile + j]) def __main__(): aggregateTxtFilesToOneFile(AGGREGATED_INPUT_FILE_PART_NAME, AGGREGATED_OUTPUT_FILE_NAME) seperateTxtFilesToSeveralFiles(SEPERATED_INPUT_FILE_NAME, SEPERATED_OUTPUT_FILE_NAME, SEPERATED_OUTPUT_FILE_NUMS)__main__() 更具体的信息参考：README主要代码：ManageTxtFiles","link":"/2018/04/21/2018/04/2018-04-21-ManageTxtFiles/"},{"title":"Text Color Of Console","text":"Console输出彩色字体在平常工作中会使用到console输出日志或者字符，如果有不同的颜色的话就非常直观。比如Error用红色显示，Warn用黄色显示，Info用默认的白色，Success或者Pass的话就使用绿色。 终端的字符颜色是用转义序列控制的，是文本模式下的系统显示功能，和具体的语言无关,只要终端支持就好。转义序列是以ESC开头,用\\033来表示（ESC的ASCII码八进制:033, 十进制:27, 十六进制:1B）。 书写格式：1\\033[%showMode%;%fontcolor%;%bgcolor%m %content%\\033[0m 开头部分\\033[parm1;parm2;parm3 m：注意：开头部分的三个参数：显示方式，前景色，背景色是可选参数，他们对应的值都是不一样的，所以可以只写一个，顺序也没有要求，但是建议按照默认的格式规范书写。 结尾可以省略，但是为了书写规范，建议\\033[*;*;*m开头，\\033[0m结尾。 数值表示的参数含义： | 显示方式 | 前景色 | 背景色 | | ————- | ———- | ———- || 0（默认值） | 30（黑色） | 40（黑色） || 1（高亮） | 31（红色） | 41（红色） || 4（下划线） | 32（绿色） | 42（绿色） || 5（闪烁） | 33（黄色） | 43（黄色） || 7（反显） | 34（蓝色） | 44（蓝色） || 22（非粗体） | 35（洋 红） | 45（洋 红） || 24（非下划线） | 36（青色） | 46（青色） || 25（非闪烁） | 37（白色） | 47（白色） || 27（非反显） | - | - | 常见开头格式： 开头格式 效果 \\033[0m 默认字体正常显示，不高亮 \\033[32;0m 红色字体正常显示 \\033[1;32;40m 显示方式: 高亮 字体前景色：绿色 背景色：黑色 \\033[1;31;40m 显示方式: 高亮 字体前景色：红色 背景色：黑色 \\033[1;35;40m 显示方式: 高亮 字体前景色：洋红 背景色：黑色 \\033[1;33;40m 显示方式: 高亮 字体前景色：黄色 背景色：黑色 Python实例： 1print(&quot;\\033[1;35;40m 高亮洋红背景黑 \\033[0m&quot;) Java实例: 12345678910111213141516171819202122public static final String ANSI_RESET = &quot;\\u001B[0m&quot;;public static final String ANSI_BLACK = &quot;\\u001B[30m&quot;;public static final String ANSI_RED = &quot;\\u001B[31m&quot;;public static final String ANSI_GREEN = &quot;\\u001B[32m&quot;;public static final String ANSI_YELLOW = &quot;\\u001B[33m&quot;;public static final String ANSI_BLUE = &quot;\\u001B[34m&quot;;public static final String ANSI_PURPLE = &quot;\\u001B[35m&quot;;public static final String ANSI_CYAN = &quot;\\u001B[36m&quot;;public static final String ANSI_WHITE = &quot;\\u001B[37m&quot;;public static final String ANSI_BLACK_BACKGROUND = &quot;\\u001B[40m&quot;;public static final String ANSI_RED_BACKGROUND = &quot;\\u001B[41m&quot;;public static final String ANSI_GREEN_BACKGROUND = &quot;\\u001B[42m&quot;;public static final String ANSI_YELLOW_BACKGROUND = &quot;\\u001B[43m&quot;;public static final String ANSI_BLUE_BACKGROUND = &quot;\\u001B[44m&quot;;public static final String ANSI_PURPLE_BACKGROUND = &quot;\\u001B[45m&quot;;public static final String ANSI_CYAN_BACKGROUND = &quot;\\u001B[46m&quot;;public static final String ANSI_WHITE_BACKGROUND = &quot;\\u001B[47m&quot;;System.out.println(ANSI_GREEN_BACKGROUND + &quot;This text has a green background but default text!&quot; + ANSI_RESET);System.out.println(ANSI_RED + &quot;This text has red text but a default background!&quot; + ANSI_RESET);System.out.println(ANSI_GREEN_BACKGROUND + ANSI_RED + &quot;This text has a green background and red text!&quot; + ANSI_RESET);","link":"/2018/04/21/2018/04/2018-04-21-ColorfulConsole/"},{"title":"System Monitor","text":"System Monitor by python之前有用powershell获取CPU，内存的信息GetCPUMemory.ps1，利用的是win系统提供的计数器。 今天用python重新写了一个，用的是psutil库并且是用OO的思想组织代码，方便之后的重构与维护 主要代码其实代码也不多，专门做了一个模块sysinfo.py. 直接调用的main函数： 12345678def monitor_cpu_memory(): &quot;&quot;&quot;monitor system cpu and memory&quot;&quot;&quot; cpu_info = CPUInfo() mem_info = MemoryInfo() while(True): print(mem_info) print(cpu_info) time.sleep(1) 在这边我建立了一个基类SystemInfo定义了一些基本信息。这里有个细节，因为像cpu和内存的使用是一直变化的，所以在这边我写了一个虚函数（方法），需要子类去实现，并用一个线程每隔一秒就去刷新下需要更新的数据。 123456789101112@abstractmethoddef refresh(self): passdef refresh_loop(self): while(True): self.refresh() time.sleep(1)def __init__(self, units=Units.MB): self.__file_system = psutil.disk_partitions() self.refresh() t = Thread(target=self.refresh_loop) t.start() MemoryInfo这个类与基类的契合度最高，所以主要就是实现refresh方法。 123456def refresh(self): mem = psutil.virtual_memory() self.free = mem.free self.used = mem.used + mem.buffers + mem.cached self.total = mem.total self.units = Units.MB CPUInfocpu的数据比较特别，主要的信息都先存储下来，需要用到的时候可以使用。 123456789def refresh(self): cpu = psutil.cpu_times_percent(interval=1.00) self.user = round(cpu.user,1) self.nice = round(cpu.nice) self.system = round(cpu.system,1) self.idle = round(cpu.idle,1) self.iowait = round(cpu.iowait,1) self.irq = round(cpu.irq,1) self.softirq = round(cpu.softirq,1) __str__(self)每个类都有自己__str__(self)函数，带颜色的输出： 12def __str__(self): return '\\033[1;35;40m CPU: user:%s%% system:%s%% idle:%s%%\\033[0 m' % (self.user, self.system, self.idle) 更具体的信息参考：README主要代码：SystemMonitor","link":"/2018/04/22/2018/04/2018-04-22-SystemMonitor/"},{"title":"openpyxl","text":"openpyxl之前测试PamReport的报表，需要对比txt和excel报表，用java(org.apache.poi)写了对比的脚本。但感觉还是有些麻烦，java在文本io方面还是不够灵活方便。今天试了python的openpyxl这个库，感觉真的的特别好用，就试着用了一下基本操作。 Java我当时简单写了一个ExcelUtils，下面是取单元格数据的一个方法，感受一下。 123456789101112131415161718192021222324252627public static String getCellString(String cellStr){ String result = &quot;&quot;; CellReference cellReference = new CellReference(cellStr); Row row = curSheet.getRow(cellReference.getRow()); Cell cell = row.getCell(cellReference.getCol()); switch (cell.getCellTypeEnum()) { case NUMERIC: result = Double.toString(cell.getNumericCellValue()); break; case STRING: result = cell.getStringCellValue(); break; case FORMULA: result = cell.getCellFormula(); break; case BLANK: result = cell.getStringCellValue(); break; case BOOLEAN: result = Boolean.toString(cell.getBooleanCellValue()); break; default: result = cell.getStringCellValue(); break; } return result; } openpyxl新建文档 New Excel File创建一个新的excel，将默认的Sheet1改为salary,新建sheet Pi然后保存。 123456wb = Workbook()dest_filename = 'test.xlsx'ws1 = wb.activews1.title = &quot;salary&quot;ws2 = wb.create_sheet(title=&quot;Pi&quot;)wb.save(filename = dest_filename) 常用赋值 Common Usage字符串，数值 string, number123456ws1['A1'] = 'id'ws1['B1'] = 'name'ws1['C1'] = 'salary'ws1['A2'] = 1ws1['B2'] = 'xiche'ws1['C2'] = 9999 时间类型 datetime12ws1['A3'] = datetime.datetime(2010, 7, 21)print(ws1['A1'].number_format) 数值格式化 number format12345wb.guess_types = Truews1['B3'] = '3.14%'wb.guess_types = Falseprint(ws1['B3'])print(ws1['B3'].number_format) 公式 formula1ws1[&quot;A4&quot;] = &quot;=SUM(1, 1)&quot; 合并单元格 merge cells123ws1.merge_cells('A5:E5')ws1.merge_cells('A6:E7')ws1.unmerge_cells('A6:E7') 插入图像 image12image = Image('logo.png')ws1.add_image(img, 'C1') 最后生成的excel的效果： 读取文档 (Read Excel File)读取worksheet1234wb = openpyxl.load_workbook('test.xlsx')print(type(wb))print(wb.sheetnames)print(wb['salary']) 获取title (sheet name)12sheet_salary = wb['salary']print(&quot;title:%s&quot; % sheet_salary.title) 获取单元格的方式 get cells and its value1234567891011121314151617print(&quot;A1:%s B1:%s C1:%s&quot; % (sheet_salary['A1'].value, sheet_salary['B1'].value, sheet_salary['C1'].value))print(&quot;A2:%s B2:%s C2:%s&quot; % (sheet_salary['A2'].value, sheet_salary['B2'].value, sheet_salary['C2'].value))print(&quot;A3:%s B3:%s C3:%s&quot; % (sheet_salary['A3'].value, sheet_salary['B3'].value, sheet_salary['C3'].value))cell_A1 = sheet_salary.cell(column = 1, row = 1)cell_A2 = sheet_salary.cell(column = 1, row = 2)cell_B1 = sheet_salary.cell(column = 2, row = 1)cell_B2 = sheet_salary.cell(column = 2, row = 2)print(&quot;A1:%s B1:%s&quot; % (cell_A1.value, cell_B1.value))print(&quot;A2:%s B3:%s&quot; % (cell_A2.value, cell_B2.value))for i in range(1, 2): for j in range(1, 2, 3): columnChar = chr(ord('A')+j) rowNum = i print(&quot;%s%s:%s&quot; % (columnChar, rowNum, sheet_salary[columnChar+str(rowNum)].value)) 获取sheet已使用的最大范围 max row/column12print(&quot;max_row:%s&quot; % sheet_salary.max_row)print(&quot;max_column:%s&quot; % sheet_salary.max_column) 更具体的信息参考：openpyxl官网 我的demo：demo.py","link":"/2018/04/24/2018/04/2018-04-24-openpyxl/"},{"title":"Father","text":"今天是父亲走了的第5天。 根据老家的风俗和迷信，出殡的日子定在了下周一的早上。 他们喜欢用生辰八字和黄历来算合适的日子，讲究时辰。而对于我和母亲来说，父亲走了，早点操办完后事，可能不会难过那么久。 我现在倒是还好，心中已然接受了事实，就担心母亲。 等事情都处理完了，我还要回杭州去。希望母亲不要太过伤感，摔伤的手还没有完全好，还要休养好长一段时间。我舍不得离开了。 大概下一个整周还是要在家中，把家中打扫一下。 上上周末我从杭州回来。 之前母亲与我说父亲情况不好的时候，我还以为和去年一样，只是虚惊一场。但当我在医院看到父亲时，我抑制不住内心的情绪。看着虚弱憔悴的父亲，我明白，这一次我可能真的要失去我的父亲了。 在医院的那几天是最难熬的。 看着父亲一天一天虚弱下去，那种等待死亡的感觉，真的很难受。 明白父亲这次凶多吉少，已经没有治疗的价值，但我还是难以接受。 当兆和Bob给我出主意，再去上海的时候，我的内心再起波澜，仿佛那是唯一的生机，就算是万分之一的机率，难道就这样忍心放弃么。 可我内心也十分清楚，那只是心中的美好愿望，事实却是那样残忍。所以当我和母亲他们表达自己的不甘心时，一次次只是让自己认命不再挣扎而已。 父亲在最后几天，和我说了一些话。 那时候他的意识还算清楚，他说要学会坚强，他的病能医的话就医，不能医的话就算了。我忍着泪水，不住的点头，为什么命运这样残忍。 最后一晚，我们将父亲接回家中，等待最后的结局，死亡。 父亲意识开始不清楚。 听医生说，黄疸升的非常高，父亲就会陷入昏迷，那时他其实已经不会感觉到很大的痛苦了。 父亲最后还是说了些话，可已经听不太清楚他在说什么了，我能是嗯嗯的答应着他。 唯一听清的一句，父亲说着“改变”，“变得不一样”。他是在说他的人生么，希望我的人生不一样么，不再像他一样，或者说做那永远吃亏的人么？ 或许这是他人生最大的遗憾，我竟多少能体会一些。我们的性格真的太像了，但我可能好一些吧。 或者他是觉得自己太早向命运低头，没有善待自己，人生刚到享福的时间就这个戛然而止了，带着这么多的遗憾吧。 又或者他是觉得自己没有做好一个太好的父亲的表率，和母亲的关系没有处理的很好，希望我能改变而和他不一样吧。 无论如何，父亲已经离开了这熙熙攘攘的人世间，永世长眠，再也没有人纷争和痛苦，得到了永远的宁静。 我让JY最后发了些语音与父亲。我知道她不太可能来。但她还是太年轻，发的语音那样不和时宜。但已经够了，我心中已经十分的感谢她。 她已经做到了她能做的。 父亲听到JY的语音后，说了些话，可惜我已经听不清了，大概是说JY不错，要珍惜什么的吧，可能还问了她会不会来什么的。我也只能嗯嗯的回答。 我与父亲从小的交流就比较少。 很多事都是母亲陪着我去的，包括家长会，开学什么的。 但我还刻父亲和母亲一起陪我去之江的场景。他是一个好父亲，一个不会表达自己的父亲。 我不知道父亲的家庭情况，在这两天的接待客人的过程中，我才知道父亲8岁便没有了亲生父亲。而今年父亲才55岁。 父亲一直从事的都是机械相关的工作。 他只有小学毕业的水平，不知道什么原因，总之当年的他并没有好好学习。 辍学后便在厂里工作，勤勤恳恳，在最后的这个厂做着车间主任，一干就是15年多。 记得我还在小学的时候，父亲在小学旁边的一个厂子里干活。 那时每天放学，我都会去找父亲，等他下班了之后便会骑着自行车带我回家。 是他撑起了这个家，但当年的我那么幼稚，觉得自己的父亲没有别人厉害，而我呢，现在连父亲的百分之一都没有到。 5.1的时候，我带着JY回家，就是想爸妈开心一些，不要为我担心。但没想到父亲自己却已经开始不舒服，但他自己竟不多说。不曾想，这一别就成永别。 还住在老房子的时候父亲有一张书桌，那张桌子也是我小时候写过作业的地方。而在那张桌子下面，有几本父亲的书，是关于机械还有别的知识的。那算是我的启蒙教材。 我最喜欢的是书里的迷语那部分，还记得有一个是说花生的，黄房子，红帐子，里面住着个白胖子。 书中还有一些折纸的介绍，还有教你怎么织毛衣，等等等等。 小时候的夏天，家里没有电风扇，但那时也没有特别的热，玉环是一个小岛，海洋季风性气候，一家人坐在阳台上，便能吹到阵阵凉风，真的很舒服。 父亲母亲还会拿着扇子，给我扇凉，真的很温馨。 冬天的时候，我便会钻到爸妈的中间，从这头爬到那头，那时候笑的最开心了。 再小一些的时候，父母都在田间劳动，我还记得自己在田里和小昆虫小青蛙玩耍。老房子前原来还有面厂，母亲晚上的时候便会在其中劳作。晚上回来的时候，我们便能吃到香喷喷的新鲜出炉的米面。蘸着小菜或者就着菜汤，真的很好吃。 小时候物质生活一般，但还是过的很开心，因为有父母的关爱，虽然他们也会争吵，也会有伤心的时刻不过是因为家里穷的缘故罢了。后来生活水平提高，我也长大，但没有那样了。 高三那次，是让父母第一次为我操碎了心，而我内心也是十分的难受，明明知道自己这样不好，但完全已经不能控制自己了。 而爸妈也无奈，只能想尽一切办法，带我去看医生，去求神拜佛，带我去做法事，我难过自己为什么会这样，让父母担心，又“厌恶”父母的“无知”。 在那段休学在家的时间，我无比难受，2个月没有复习，完全不知道自己是什么样子，那也是我人生的第一个转折点。 一天我一个人逛到水库的山顶，父亲也跟着我。 父亲没有多说什么话，只是安慰了我一些话，我都不记得了，我只记得我自己喊了几嗓子，希望把自己心中的郁气都抒发出来，但我那时也明白，我需要的是时间，我要撑过去。 不出意外，高考考的很差，但是父母都没有多说什么，只是默默的做着他自己的事，给我提供高昂的学费，也从来没提复读什么的事。 我极度失落，也想着自己的人生也便这样了，不想再挣扎，有时常常想着自己就这样过吧，也不想打扰别人。 大学毕业后也曾想着回家，希望能在父母身边，幼稚的我做了第二次很愚蠢的决定，直接裸辞，那也是一个夏天，完全没有想过自己当时的情况。 是的，我又SB了。母亲极度伤心，但父亲却相信我会好起来的。 折腾了半年之后，我又回了HZ，工作了这几年，眼看光景越来越好，但没想到去年母亲一个电话，说父亲肝衰竭了。 从温州又到上海，有母亲的陪伴和医生的治疗，父亲恢复的不错。但当时母亲已经和我说，父亲这个没有办法根治，只能保持。眼看着父亲情况不错，心里也开心。但没有想到，这次的复发，父亲也没有在意，及早表明自己的情况，或许能及时治疗，再多几年的时光。 也许他就能等到我结婚的那一天，也许。。。 愿世间少些痛苦与遗憾 我要更加努力，像父亲说的一样，“改变” 安息","link":"/2018/07/04/2018/07/2018-07-04-MyDearFather/"},{"title":"colorlog","text":"colorlog这个module的核心就是提供了colored formaterr给logging模块，所以使用上还是遵循logging的方式。 今天就简单的介绍下用法，我感觉够用了，有更多的需求的话可以再去深入了解。 Log FormatActually, 只有Console是支持颜色的，之前我提到过终端中颜色的原理Text Color Of Console。这个module就是在输出到终端的时候，前后加了转义序列ESC(e.g.&quot;\\033[1;35;40m 高亮洋红背景黑 \\033[0m&quot;)我在demo中顺便加入了输出log到文件的简单用法来对比。 12LOG_FORMAT_CONSOLE = &quot;%(log_color)s%(asctime)s [%(levelname)-5.5s] %(message)s&quot;LOG_FORMAT_FILE = &quot;%(asctime)s [%(levelname)-5.5s] %(message)s&quot; 从上面的代码可以看出来，为了使用自定义颜色，需要在format string中加入%(log_color)s。 ColoredFormatter这个是关键的一个类，从colorlog源码片段中可以看到，它是继承自logging.Formatter，封装了自己的实现。 12345678class ColoredFormatter(logging.Formatter): # 略... def format(self, record): # 略... # Add a reset code to the end of the message # (if it wasn't explicitly added in format str) if self.reset and not message.endswith(escape_codes['reset']): message += escape_codes['reset'] 可以直接使用默认的设置formatter = ColoredFormatter(LOG_FORMAT_CONSOLE)得到default的配色。 但我觉得默认的不是很好看，而它也支持自定义。 从下面的源码片段里可以看到，colorlog支持8种颜色，可以挑个自己喜欢的，觉得不好的当然也可以改源码，加入自己的颜色。 1234567891011# The color namesCOLORS = [ 'black', 'red', 'green', 'yellow', 'blue', 'purple', 'cyan', 'white'] 此外，它也支持改变背景色，只要加上前缀bg_，而如果想要高亮的话，可以加前缀bold_。尽情试一下效果吧:) 12345678910PREFIXES = [ # Foreground without prefix ('3', ''), ('01;3', 'bold_'), ('02;3', 'thin_'), # Foreground with fg_ prefix ('3', 'fg_'), ('01;3', 'fg_bold_'), ('02;3', 'fg_thin_'), # Background with bg_ prefix - bold/light works differently ('4', 'bg_'), ('10', 'bg_bold_'),] 这个是我目前使用的配色，参考下： 1234567891011121314formatter_console = ColoredFormatter( LOG_FORMAT_CONSOLE, datefmt=None, reset=True, log_colors={ 'DEBUG': 'cyan', 'INFO': 'green', 'WARNING': 'bold_yellow', 'ERROR': 'bold_red', 'CRITICAL': 'bold_red,bg_white', }, secondary_log_colors={}, style='%') 其他其实上边讲完，剩下的就是使用logging的常规方式，可以在demo里看到： 123456789101112131415161718192021formatter_file = logging.Formatter(LOG_FORMAT_FILE)handler_stream = logging.StreamHandler()handler_stream.setLevel(LOG_LEVEL)handler_stream.setFormatter(formatter_console)handler_file = logging.FileHandler(&quot;colorlog.log&quot;)handler_file.setLevel(LOG_LEVEL)handler_file.setFormatter(formatter_file)log = logging.getLogger(__name__)log.setLevel(LOG_LEVEL)# set file and console hander to loglog.addHandler(handler_stream)log.addHandler(handler_file)log.debug(&quot;A quirky message only developers care about&quot;)log.info(&quot;Curious users might want to know this&quot;)log.warn(&quot;Something is wrong and any user should be informed&quot;)log.error(&quot;Serious stuff, this is red for a reason&quot;)log.critical(&quot;OH NO everything is on fire&quot;) 更具体的信息参考：colorlog 我的demo：colorlog/demo.py","link":"/2018/04/25/2018/04/2018-04-25-colorlog/"},{"title":"chart.js","text":"chart.jsChart.js 是一个开源的的 js 图表库，它支持大多数常用的图表。之前就用过它来展示内存和 CPU 的监测结果，现在来回顾总结一下。 data之前的版本我每行都有 KeyString 其实是有些冗余的，但在数据量不是特别大的时候，还是可以接受的。当然，最好还是用 csv 的格式，比较标准，这边就先这样。 123456Time=2017-12-19T01:25:09;CPURate=0.00;MemRate=70.48Time=2017-12-19T01:25:11;CPURate=27.97;MemRate=70.43Time=2017-12-19T01:25:13;CPURate=37.07;MemRate=70.00...Time=2017-12-19T01:27:08;CPURate=41.62;MemRate=69.48Time=2017-12-19T01:27:10;CPURate=38.41;MemRate=69.51 js libs1234&lt;script src=&quot;js/jquery-3.2.1.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;js/Chart.bundle.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;js/utils.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;js/moment.js&quot;&gt;&lt;/script&gt; 这边我引入了 jquery 来操作 DOM，Chart.bundle.js 则是 chart.js 的核心库，并且其中已经打包了一些第三方库。可以在它的 github 上看到有不同的版本供选择。 chart.js release moment.js，它是与日期处理有关，有些 bundle 是包含了它的，不过需要自己 build 出来。 utils.js 则是封装了一些实用的工具，比如颜色，具体可以看一下这个文件，里面代码不多。 123456789window.chartColors = { red: &quot;rgb(255, 99, 132)&quot;, orange: &quot;rgb(255, 159, 64)&quot;, yellow: &quot;rgb(255, 205, 86)&quot;, green: &quot;rgb(75, 192, 192)&quot;, blue: &quot;rgb(54, 162, 235)&quot;, purple: &quot;rgb(153, 102, 255)&quot;, grey: &quot;rgb(201, 203, 207)&quot;}; 注：我使用的库基本上都是没有压缩过的，方便查看源码，一般在真正产品上都是使用像.min.js 这样压缩过的库。 Read file and create data object使用 jquery 读取文件(像 Chrome 之类的浏览器不支持非 web 应用下的直接文件读取，需要特殊配置)之后，逐行进行解析并加入来数据 list 中，以供后面使用。 注：之前生成的 data 文件是 win 下的，所以用\\r\\n来 split 获取行，如果是 linux 下的话，就是\\n了,为了兼容，其实还是有必要添加额外的 check。 123456789101112131415161718success :function(data){ records = data.split(&quot;\\r\\n&quot;); records.forEach(function(rec){ if (rec.trim()){ dataRecord = { time:moment(rec.split(&quot;;&quot;)[0].split(&quot;=&quot;)[1], timeFormat).toDate(), cpuRate:rec.split(&quot;;&quot;)[1].split(&quot;=&quot;)[1], memRate:rec.split(&quot;;&quot;)[2].split(&quot;=&quot;)[1] }; dataRecords.push(dataRecord); dataLabels.push(dataRecord.time); cpuData.push(parseFloat(dataRecord.cpuRate)); memData.push(parseFloat(dataRecord.memRate)); } }); //略} 获取 h5 canvas下面是标准的 html5 画布上下文，以供 chartjs 使用。 1var ctx = document.getElementById(&quot;canvas&quot;).getContext(&quot;2d&quot;); config and draw the chart下面刚是核心的配置代码，不同的 chart 大同小异，真是提供的 data 不同。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061var config = { //配置chart类型 type: &quot;line&quot;, data: { //time labels: dataLabels, //定义cpu和内存的数据集，包括颜色和实际数据等，这边便用到了之前说过的util.js的颜色。 datasets: [ { label: &quot;CPU&quot;, backgroundColor: color(window.chartColors.red) .alpha(0.5) .rgbString(), borderColor: window.chartColors.red, fill: false, data: cpuData }, { label: &quot;Memory&quot;, backgroundColor: color(window.chartColors.blue) .alpha(0.5) .rgbString(), borderColor: window.chartColors.blue, fill: false, data: memData } ] }, options: { //这边定义chart相关的其它信息 title: { text: &quot;CPU Memory Monitor&quot; }, scales: { //定义x,y轴的类型 xAxes: [ { type: &quot;time&quot;, time: { parser: timeFormat, //var timeFormat = 'yyyy-MM-dd HH:mm:ss';上文中有定义，用来解析时间string tooltipFormat: &quot;HH:mm:ss&quot; //这边是控制显示在报表中的时间格式 }, scaleLabel: { display: true, labelString: &quot;Date&quot; } } ], yAxes: [ { scaleLabel: { display: true, labelString: &quot;value&quot; } } ] } }};//将chart画出来var chart = new Chart(ctx, config); 显示效果Sample 更具体的信息参考：chart.js 我的 demo：Monitor","link":"/2018/04/27/2018/04/2018-04-27-chartjs/"},{"title":"Process Info","text":"Process Info之前的测试中，使用psutil可以得到系统的CPU/Memory的使用百分比，但是没试过读取个别进程的信息。现在想要实现这样的功能，方便更好的分析java.exe的内存使用变化。 有一种思路是使用系统本身提供的查看进程的shell cmd和subprocess.check_output组合得到信息。 tasklist在windows系统中，提供了tasklist来查看process的信息，大致的结果如下： 12345678910C:\\Users\\mayn&gt;tasklist映像名称 PID 会话名 会话# 内存使用========================= ======== ================ =========== ============System Idle Process 0 Services 0 8 KSystem 4 Services 0 108 KRegistry 108 Services 0 22,924 Ksmss.exe 376 Services 0 1,128 Kcsrss.exe 580 Services 0 5,124 Kwininit.exe 672 Services 0 6,676 K 查看tasklist的使用帮助，我们发现可以利用本身提供的参数取得想要的数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172C:\\Users\\mayn&gt;tasklist /?TASKLIST [/S system [/U username [/P [password]]]] [/M [module] | /SVC | /V] [/FI filter] [/FO format] [/NH]描述: 该工具显示在本地或远程机器上当前运行的进程列表。参数列表: /S system 指定连接到的远程系统。 /U [domain\\]user 指定应该在哪个用户上下文执行这个命令。 /P [password] 为提供的用户上下文指定密码。如果省略，则 提示输入。 /M [module] 列出当前使用所给 exe/dll 名称的所有任务。 如果没有指定模块名称，显示所有加载的模块。 /SVC 显示每个进程中主持的服务。 /APPS 显示 Microsoft Store 应用及其关联的进程。 /V 显示详细任务信息。 /FI filter 显示一系列符合筛选器 指定条件的任务。 /FO format 指定输出格式。 有效值: &quot;TABLE&quot;、&quot;LIST&quot;、&quot;CSV&quot;。 /NH 指定列标题不应该 在输出中显示。 只对 &quot;TABLE&quot; 和 &quot;CSV&quot; 格式有效。 /? 显示此帮助消息。筛选器: 筛选器名称 有效运算符 有效值 ----------- --------------- -------------------------- STATUS eq, ne RUNNING | SUSPENDED NOT RESPONDING | UNKNOWN IMAGENAME eq, ne 映像名称 PID eq, ne, gt, lt, ge, le PID 值 SESSION eq, ne, gt, lt, ge, le 会话编号 SESSIONNAME eq, ne 会话名称 CPUTIME eq, ne, gt, lt, ge, le CPU 时间，格式为 hh:mm:ss。 hh - 小时， mm - 分钟，ss - 秒 MEMUSAGE eq, ne, gt, lt, ge, le 内存使用(以 KB 为单位) USERNAME eq, ne 用户名，格式为 [域\\]用户 SERVICES eq, ne 服务名称 WINDOWTITLE eq, ne 窗口标题 模块 eq, ne DLL 名称注意: 当查询远程计算机时，不支持 &quot;WINDOWTITLE&quot; 和 &quot;STATUS&quot; 筛选器。Examples: TASKLIST TASKLIST /M TASKLIST /V /FO CSV TASKLIST /SVC /FO LIST TASKLIST /APPS /FI &quot;STATUS eq RUNNING&quot; TASKLIST /M wbem* TASKLIST /S system /FO LIST TASKLIST /S system /U 域\\用户名 /FO CSV /NH TASKLIST /S system /U username /P password /FO TABLE /NH TASKLIST /FI &quot;USERNAME ne NT AUTHORITY\\SYSTEM&quot; /FI &quot;STATUS eq running&quot; 以vscode的进程code.exe为例，可以筛选出对应的进程： 1234567891011C:\\Users\\mayn&gt;tasklist /FI &quot;imagename eq code.exe&quot;映像名称 PID 会话名 会话# 内存使用========================= ======== ================ =========== ============Code.exe 1492 Console 1 146,248 KCode.exe 9176 Console 1 69,080 KCode.exe 9120 Console 1 11,412 KCode.exe 1700 Console 1 81,984 KCode.exe 5784 Console 1 221,540 KCode.exe 9464 Console 1 106,296 KCode.exe 8392 Console 1 98,076 K 筛选器是可以组合的，比如：&quot;tasklist /FI &quot;imagename eq java.exe&quot; /FI \\&quot;username eq xiche\\&quot;&quot;这样就可以得到自己想要的进程信息，并取得一些值。 subprocess.check_outsubprocess是python中调用进程与进程交互的一个库，可以用它来调用运行程序。 这边我们使用check_out来得到tasklist的输出结果，然后利用正则表达式取得对应的值，组装成字典列表来供之后使用。 12345678910111213141516171819202122import reimport subprocessdef get_processes_running(): tasks = subprocess.check_output( &quot;tasklist /FI \\&quot;imagename eq code.exe\\&quot; &quot;).decode(&quot;gbk&quot;).split(&quot;\\r\\n&quot;) p = [] for task in tasks: m = re.match(&quot;(.+?) +(\\d+) (.+?) +(\\d+) +(\\d+.* K).*&quot;, task) if m is not None: p.append({&quot;image&quot;: m.group(1), &quot;pid&quot;: m.group(2), &quot;session_name&quot;: m.group(3), &quot;session_num&quot;: m.group(4), &quot;mem_usage&quot;: m.group(5) }) return pprocesses = get_processes_running()for process in processes: print(process[&quot;pid&quot;], process[&quot;mem_usage&quot;]) 运行的结果如下: 123456789PS C:\\Users\\mayn\\Desktop&gt; python test.py1492 157,088 K9176 75,932 K9120 11,416 K1700 119,968 K2868 237,328 K2616 121,216 K5580 33,724 K10688 103,604 K psutil可能大家发现了，这样使用tasklist取得的值是固定了，而我们想得到更多的值(e.g cpu)需要使用更多的命令来得到信息。 但其实还有一种方便的方法，直接使用psutil，用他来获取系统信息真的很强很方便。 123456789101112def processinfo(processName): pids = psutil.pids() pids_return = [] for pid in pids: p = psutil.Process(pid) if processName.lower() in p.name().lower() : pids_return.append(pid) return pids_returnfor pid in processinfo(&quot;code.exe&quot;): ps = psutil.Process(pid) print(ps.cpu_percent(interval=1.0), ps.memory_percent()) 更多信息请参考：https://github.com/bearfly1990/PowerScript/wiki/Python3","link":"/2018/08/07/2018/08/2018-08-07-ProcessInfo/"},{"title":"System Monitor by Powershell","text":"Process Monitor先上代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354$CurrentDir = Split-Path $MyInvocation.MyCommand.Path$file = &quot;$CurrentDir/Monitor.csv&quot;$ps_name = &quot;java&quot;$show_sys_cpu_rate = $false$show_sys_mem_rate = $false$memory_sys_total = (Get-WmiObject -Class Win32_PhysicalMemory |measure capacity -sum).Sum #(gwmi win32_computersystem).TotalPhysicalMemory$cpu_cores = (Get-WmiObject Win32_ComputerSystem).NumberOfLogicalProcessors# $memory_sys_total = (Get-Counter &quot;\\Memory\\System Driver Total Bytes&quot;).CounterSamples | Sort-Object Path# $memory_sys_total = $memory_sys_total[0].CookedValue&quot;time,CPU(%),Memory(%),CPU_$ps_name(%), Memory_$ps_name(%)&quot; &gt;&gt; $filewhile ($True) { $cpu_rate_pss = (Get-Counter &quot;\\process($ps_name*)\\% Processor Time&quot;).CounterSamples | Sort-Object Path $cpu_rate_sys = (Get-Counter &quot;\\processor(_total)\\% processor time&quot;).CounterSamples | Sort-Object Path $cpu_rate_sys = $cpu_rate_sys[0].CookedValue if($show_sys_cpu_rate -eq $false){ $cpu_rate_sys = 0 } $memory_pss = (Get-Counter &quot;\\Process($ps_name*)\\Working Set - Private&quot;).CounterSamples | Sort-Object Path if($show_sys_mem_rate){ $memory_sys_available = (Get-Counter &quot;\\Memory\\Available Bytes&quot;).CounterSamples | Sort-Object Path $memory_sys_available = $memory_sys_available[0].CookedValue $memory_sys = $memory_sys_total - $memory_sys_available }else{ $memory_sys = 0 } $cpu_rate_pss_total = 0 $memory_pss_total = 0 $cpu_rate_pss.Count try { For ($i = 0; $i -lt $cpu_rate_pss.Count; $i++) { $cpu_rate_pss_total = $cpu_rate_pss_total + $cpu_rate_pss[$i].CookedValue $memory_pss_total = $memory_pss_total + $memory_pss[$i].CookedValue } }catch{ Write-Output &quot;&quot; } $cpu_rate_sys = &quot;{0:F2}&quot; -f $cpu_rate_sys $cpu_rate_pss_total = &quot;{0:F2}&quot; -f $cpu_rate_pss_total / $cpu_cores $memory_rate_sys = &quot;{0:F2}&quot; -f ( $memory_sys / $memory_sys_total * 100) $memory_rate_pss_total = &quot;{0:F2}&quot; -f ( $memory_pss_total / $memory_sys_total * 100) $time = Get-Date -format &quot;MM/dd/yyyy HH:mm:ss&quot; Write-Output &quot;Time=$time;CPU=$cpu_rate_sys;Mem=$memory_rate_sys;CPU($ps_name)=$cpu_rate_pss_total;Mem($ps_name)=$memory_rate_pss_total&quot; &quot;$time,$cpu_rate_sys,$memory_rate_sys,$cpu_rate_pss_total,$memory_rate_pss_total&quot; &gt;&gt; $file sleep 1} 之前也有写过监测系统CPU和内存的Powershell脚本和Python脚本，不过当时都只有监测系统的CPU和内存的占比变化，没有针对进程。 最近在做测试的时候，已经在脚本中加入了Python对进程的操作，不过因为和具体项目结合在一起，还没有时间把代码抽象出来。今天本来想着在之前Powershell脚本的基础上改一下，不过没想到踩了一些坑，主要还是不熟悉。尤其如果对C#比较了解的话，写起来会方便很多。Powershell可以直接调用.Net的许多对象和方法。 Main Knowledge和之前ProcessInfo中类似，Powershell也有许多方法去取得进程信息。可以利用系统本身的命令得到，而今天使用的，类似系统自身的Performance Monitor，通过Get-Counter方法来得到对应的计数器信息。 Get-Counter在Powershell运行窗体中，我们输入以下信息，可以得到系统的总的CPU使用率 123456PS C:\\Users\\mayn&gt; Get-Counter &quot;\\processor(_total)\\% processor time&quot;Timestamp CounterSamples--------- --------------2018/8/21 22:41:37 \\\\pc-cx\\processor(_total)\\% processor time : 2.49161836524493 那如果我们想得到某个进程呢？ 123456PS C:\\Users\\mayn&gt; Get-Counter &quot;\\process(Code)\\% processor time&quot;Timestamp CounterSamples--------- --------------2018/8/21 22:54:45 \\\\pc-cx\\process(code)\\% processor time : 0 那多个同名进程呢？就如这VSCode的进程code.exe有好多个,就可以用通配符： 12345678910111213141516171819202122232425262728PS C:\\Users\\mayn&gt; Get-Counter &quot;\\process(Code*)\\% processor time&quot;Timestamp CounterSamples--------- --------------2018/8/21 22:55:57 \\\\pc-cx\\process(code#7)\\% processor time : 0 \\\\pc-cx\\process(code#6)\\% processor time : 0 \\\\pc-cx\\process(code#5)\\% processor time : 0 \\\\pc-cx\\process(code#4)\\% processor time : 0 \\\\pc-cx\\process(code#3)\\% processor time : 0 \\\\pc-cx\\process(code#2)\\% processor time : 0 \\\\pc-cx\\process(code#1)\\% processor time : 0 \\\\pc-cx\\process(code)\\% processor time : 0 那么回过头来再看代码里，就思路很清晰了，就是利用Get-Counter来取得我们想要的信息，再处理，就可以得到我们想要的结果。 那怎么看我可以得到哪些信息呢？一个就是上网查，另一个是可以如下去得到相关的信息,你可以把memory替换成process等其它值来看有什么对应的信息可以获取。 12345678910111213141516171819202122232425262728293031323334353637PS C:\\Users\\mayn&gt; (Get-Counter -ListSet memory).paths\\Memory\\Page Faults/sec\\Memory\\Available Bytes\\Memory\\Committed Bytes\\Memory\\Commit Limit\\Memory\\Write Copies/sec\\Memory\\Transition Faults/sec\\Memory\\Cache Faults/sec\\Memory\\Demand Zero Faults/sec\\Memory\\Pages/sec\\Memory\\Pages Input/sec\\Memory\\Page Reads/sec\\Memory\\Pages Output/sec\\Memory\\Pool Paged Bytes\\Memory\\Pool Nonpaged Bytes\\Memory\\Page Writes/sec\\Memory\\Pool Paged Allocs\\Memory\\Pool Nonpaged Allocs\\Memory\\Free System Page Table Entries\\Memory\\Cache Bytes\\Memory\\Cache Bytes Peak\\Memory\\Pool Paged Resident Bytes\\Memory\\System Code Total Bytes\\Memory\\System Code Resident Bytes\\Memory\\System Driver Total Bytes\\Memory\\System Driver Resident Bytes\\Memory\\System Cache Resident Bytes\\Memory\\% Committed Bytes In Use\\Memory\\Available KBytes\\Memory\\Available MBytes\\Memory\\Transition Pages RePurposed/sec\\Memory\\Free &amp; Zero Page List Bytes\\Memory\\Modified Page List Bytes\\Memory\\Standby Cache Reserve Bytes\\Memory\\Standby Cache Normal Priority Bytes\\Memory\\Standby Cache Core Bytes\\Memory\\Long-Term Average Standby Cache Lifetime (s) 请参考Get-Counter CPU Cores and % Processor Time细心的话，大家可能会发现，我在计算最后进程的cpu使用率的时候，除以了CPU的核心数。 这是因为在我们使用&quot;\\process($ps_name*)\\% Processor Time&quot;取得进程CPU使用率的时候，得到的是所有同名进程在不同Processors/Cores上使用率的总合。也就是说，比如我的机子是6核的，那么如果该进程用满了CPU，那么通过这个CounterSet得到的值将会是**600%**。 请参考：Understanding Processor (% Processor Time) and Process (%Processor Time)","link":"/2018/08/21/2018/08/2018-08-21-ProcessMonitor/"},{"title":"Send email by python","text":"Send email by pythonpython有两个原生库(smtplib,email)可以用来发送邮件，只要你有smtp server就行。之前在公司已经写过简单的类来实现，回来又重新写了一个使用163邮箱来发送的工具类。 目前已经能够完成基本发送功能，包括html格式，添加附件等，只是弄了好久，还是没有办法在正文中添加图片，一直554错误，被网易认为垃圾信息(错误代码一览)而发送不了，之后有时间再看。 授权码想利用163的邮箱发送邮件，就需要先开通授权码： 主要代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#/usr/bin/python3&quot;&quot;&quot;author: xichecreate at: 08/15/2018description: Utils for send emailChange log:Date Author Version Description08/15/2018 xiche 1.0.1 Setup&quot;&quot;&quot;import smtplibimport osfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartfrom email.mime.image import MIMEImagefrom email.header import Headerclass EmailUtils: __sender = &quot;bearfly1990@163.com&quot; __recipients = [&quot;xiongchen1990@163.com&quot;, &quot;xchen1230@163.com&quot;] __subject = 'python email 尝试' __content = '&lt;p&gt;This is the test by &lt;b&gt;python&lt;/b&gt;, please have a try.&lt;/p&gt;' #&lt;p&gt;截图如下：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;cid:image1&quot;&gt;&lt;/p&gt; __mail_permission = 'xxxxxx'#授权码，非邮箱密码 __mail_host = 'smtp.163.com' __mail_port = 25 #SSL 465 __attachments_path = [r'C:\\Users\\mayn\\Desktop\\test.txt'] __content_images_path = [{'cid':'image1', 'path': r'C:\\Users\\mayn\\Desktop\\ymj.jpg'}] def __init__(self): pass def send_email(self): msg = MIMEMultipart() # msg = MIMEText(self.content, 'plain', 'utf-8') msg['Subject'] = Header(self.subject, 'utf-8') msg['From'] = self.sender msg['To'] = ','.join(self.recipients) content = MIMEText(self.__content,'html','utf-8') msg.attach(content) for content_image in self.__content_images_path: msgImage = MIMEImage(open(content_image['path'], 'rb').read()) msgImage.add_header('Content-ID', '&lt;{}&gt;'.format(content_image['cid'])) msg.attach(msgImage) for attachments_path in self.__attachments_path: attachment = MIMEText(open(attachments_path, 'rb').read(), 'base64', 'utf-8') attachment[&quot;Content-Type&quot;] = 'application/octet-stream' attachment[&quot;Content-Disposition&quot;] = 'attachment; filename=&quot;{}&quot;'.format(os.path.basename(attachments_path)) msg.attach(attachment) # smtp = smtplib.SMTP(self.__mail_host, port=self.__mail_port) try: smtp = smtplib.SMTP_SSL() # 注意：如果遇到发送失败的情况（提示远程主机拒接连接），这里要使用SMTP_SSL方法 smtp.connect(self.__mail_host) smtp.login(self.sender, self.__mail_permission) # print(msg.as_string()) smtp.sendmail(self.sender, self.recipients, msg.as_string()) print('email send successfully.') except smtplib.SMTPException as e: print(str(e)) finally: smtp.quit() # 发送完毕后退出smtp '''sender''' @property def sender(self): return self.__sender @sender.setter def sender(self, sender): self.__sender = sender '''subject''' @property def subject(self): return self.__subject @subject.setter def subject(self, subject): self.__subject = subject '''recipients''' @property def recipients(self): return self.__recipients @recipients.setter def recipients(self, recipients): if(isinstance(recipients, str)): self.__recipients = recipients.split(';') elif(isinstance(recipients, list)): self.__recipients = recipients '''sender''' @property def content(self): return self.__content @content.setter def content(self, content): self.__content = contentif __name__ == '__main__': emailUtils = EmailUtils() emailUtils.send_email() # print(os.path.basename(r'C:\\Users\\mayn\\Desktop\\ymj.jpg')) 更多信息请参考：https://github.com/bearfly1990/PowerScript/blob/master/Python3/mylib/cmutils_email.py","link":"/2018/08/16/2018/08/2018-08-16-SendEmailByPython/"},{"title":"Performance Test Training (1)","text":"Performance Test Training (1)今天是性能测试培训的第一天，主要内容是性能测试涉及到的概念，以及讲师自己的一些实践，个人感觉还是有些收获的。 下面就简单总结下一点知识。 Performance Idicator针对目前大多数的系统，都可以发起请求，来得到结果。尤其是REST API这种形式的，比如之前针对Publisher Rest API，就有对应的SoupUI的cases，也有基于Jmeter对api的反应时间(Response Time)进行的测试。 总的来说，有下面三个主要的指标: TPS (Transaction Per Second) Response Time Error Rate TPSTransaction per second指是一秒钟时间内能完成的事务数，是一个最基本最重要的指标。一般来说，在制定性能测试计划的时候，会设定目标TPS的目标值，从具体的需求中抽离出来。 TPS的计算方式为1/RT*users，比如一个用户实例时，假设RT为0.2,则TPS = 1/0.2=20 在理想情况下，假设RT不变，则TPS如下所示： Users(Threads) RT TPS 1 0.2 1/0.2*1=5 2 0.2 1/0.2*2=10 3 0.2 1/0.2*3=15 … … … Response Time(RT)系统反应时间一般不会一成不变的，当系统请求越来越多的时候，反应时间会逐渐延长。所以结合上请求数，TPS一般为一个曲线，并且有峰值。 当超过之后，便会开始达到系统处理的瓶颈，使反应时间增加，反而使TPS减小。所以一般我们会从小变大线程数来得到TPS的结果，示意图如下，实际没有这么平滑。 Error Rate当线程增加的时候，便会增加应用的负荷，便可能会有error产生，这个时候便有一个指标便是Error Rate，即一批线程中有错误的比率，比如一个正常的请求返回繁忙的错误。 尤其是在达到最大的TPS的时候，如果错误率很高的话，那也是没有意义的。 Others一般来说，性能指标能否完成，主要看以下： 速度相关(TPS / RT) 容量相关(吞吐, Hit, PV) 资源相关(CPU, Memory, IO) 我们很多时候会先定义性能指标，而最后的测试是为了估算应用最后能否达到预期的数值，给予项目人员及时的反馈，并帮助分析性能的瓶颈，甚至找到最根本的原因。 Performance Test Roles上面讲的是性能测试中的表象和指标，其实只是性能测试一个最基本的一个方面，理论上来说，性能测试工作可以划分为以下几个角色： 性能脚本工程师 性能场景设计工程师 性能分析工程师 Performance Scripts Engineer这个最好理解，同时也是我最近的角色，负责Messaage Import的性能测试环境的搭建，性能相关数据的采集。 相对来说，大部分人都能做到这个程度，毕竟技术门槛不高，而且目标比较明确。 Performance Scenario Design Engineer这一块的话，更像是目前David和陶总担任的角色。因为从用户需求角度和项目理解熟悉程度，我还没有那么高，同时我更多的精力是在完成测试脚本和执行测试计划。 当然，在一些情况下，我也有自己的一些测试方向和想法，但因为资源有限的情况下，还是要优先以客户需求为导向的测试。 希望之后我也能更多承担起这块的角色，但这个前提是我对业务还有系统的功能有更深入的了解。 因为测试数据的建立对我来说就有一定的难度，需要大家的support. Performance Analysis Engineer这个角色是性能测试最核心也是最难的一个角色。需要对应用，业务都了解的情况下，掌握非常好的分析方法和经验，会使用各种分析工具和系统级命令。 从最直接的性能结果开始，最终达到代码级别的诊断与分析。 一般来说都是一步一步精细化下去： 现象 -&gt; OS -&gt; CPU -&gt; Processor -&gt; Thread -&gt;Java/C/…-&gt; dump -&gt; code Performance Test Knowledge性能测试要做的好，需要掌握了解许多的知识，也是一个很庞大的知识体系： 压力工具(Load Runner/Jmeter/…) 监控分析工具 中间件(MQ/Tuxedo/…) 网络(Network)，网络性能分析 OS(Windows/Linux/…) 应用服务器(Apache/Nginx/…) DB 存储 Code … Last今天还介绍了许多实用的知识，比如Linux系统的架构，常用的一些网络协议，系统自己的性能查看工具等。 之后有时候，再自己多实践与总结。 晚安。","link":"/2018/08/25/2018/08/2018-08-25-PerfTestTraining/"},{"title":"Spider for Douban (01)","text":"Top n Movies of Spider for Douban网上有许多爬虫的入门脚本，都是去爬豆瓣前 250 的电影信息，我也参考并写了一个，做为入门。 因为豆瓣的网页结构相对稳定，变化不多，不然要花太多时候去维护。 在代码中可以修改 page 的值来取得更多的页数，我发现其实是可以超过 250 个的，排名 250 后的资源，也可以通过手动修改得到。 Result运行后的结果大致如下： 1234567891011121314151617肖申克的救赎(9.6)霸王别姬(9.5)这个杀手不太冷(9.4)阿甘正传(9.4)美丽人生(9.5)泰坦尼克号(9.3)千与千寻(9.3)辛德勒的名单(9.4)盗梦空间(9.3)机器人总动员(9.3)忠犬八公的故事(9.3)三傻大闹宝莱坞(9.2)海上钢琴师(9.2)放牛班的春天(9.2)大话西游之大圣娶亲(9.2)楚门的世界(9.1)... 下载的海报如下： Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798'''author: xichecreate at: 08/24/2018description: Spider for DoubanChange log:Date Author Version Description08/24/2018 xiche 1.0 Top n Movies of Spider for Douban08/28/2018 xiche 1.1 refactor the code'''import requestsimport osimport codecsimport refrom contextlib import closingfrom bs4 import BeautifulSoupfrom abc import abstractmethod, ABCclass Spider(ABC): url_request = '' html = '' HEADER_BROWER = { 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)' 'Chrome/63.0.3239.132 Safari/537.36'} def download_page(self): data = requests.get(self.url_request, headers=self.HEADER_BROWER).text self.html = data return data @abstractmethod def parse_html(self, html): pass @abstractmethod def start_spider(self): pass def getPic(self, data): pic_list = re.findall(r'src=&quot;http.+?.jpg&quot;', data) return pic_list def download_pic(self, url, name, folder = 'imgs/'): rootPath = folder if not os.path.exists(rootPath): os.makedirs(rootPath) response = requests.get(url, stream=True) pic_type = '.' + url.split('.')[-1] with closing(requests.get(url, stream=True)) as responses: with open(rootPath + name + pic_type, 'wb') as file: for data in response.iter_content(128): file.write(data)class SpiderDouban250(Spider): start = 0 page = 2 movies_list_total = [] def __init__(self, page=2): self.page = page def parse_html(self, html=None): html = html if html else self.html soup = BeautifulSoup(html, &quot;html.parser&quot;) movie_list_soup = soup.find('ol', attrs={'class': 'grid_view'}) movie_name_list = [] for movie_li in movie_list_soup.find_all('li'): detail = movie_li.find('div', attrs={'class': 'hd'}) movie_name = detail.find('span', attrs={'class': 'title'}).getText() score = movie_li.find('div', attrs={'class': 'bd'}) movie_score = score.find('span', attrs={'class': 'rating_num'}).getText() movie_detail = &quot;{}({})&quot;.format(movie_name , movie_score) movie_name_list.append(movie_detail) next_page = soup.find('span', attrs={'class': 'next'}).find('a') if next_page: return movie_name_list, self.url_request + next_page['href'] return movie_name_list, None def start_spider(self): start = self.start movies_list_total = [] while(start &lt; 25 * self.page): self.url_request = 'https://movie.douban.com/top250?self.start={}'.format(start) self.download_page() picdata = self.getPic(self.html) movies, url = self.parse_html() movies_list_total = movies_list_total + movies index_movies = 0 for picinfo in picdata: self.download_pic(picinfo[5:-1], 'Top' + str(index_movies+1) + '-' + movies[index_movies]) print(movies[index_movies] + ' download finished') index_movies += 1 start += 1 with codecs.open('movies.txt', 'w', encoding='utf-8') as fp: fp.write(u'\\r\\n'.join(movies_list_total))if __name__ == '__main__': spider_douban = SpiderDouban250(2) spider_douban.start_spider()","link":"/2018/08/25/2018/08/2018-08-25-SpiderDouban/"},{"title":"Next to do","text":"Next to do自从在了TE，每天的工作都挺饱和的，当然这个和自己想要做更多的事情有关系。还是要理一理目前的状况和接下去主要的目标。 Import Message Test现在的脚本已经足够应对大部分的情况，而且一些细节也有优化过了。尤其这两天，我还是花了时间把自动生成report的信息给好好整了下。 这个脚本肯定还是会继续更新的，不过主要精力还是要转移到benchmark那边去。 之前在做的时候，还是留下了尾巴，没有处理干净，欠的一些债要还，很多文件没有整理好。这个还是要做的。 Learn new technology最近下了好多资料，初略看了下还是挺不错的，但是还是一个问题，自己要花时间下去。 也加了一些培训老师的微信，但观察下来，还是要先自己动自己，不然也只是枉然，只会更加难受。 Python基础要夯实一下，毕竟没有从无到有学，所以很多细节和基础没有概念，只是会用而已。 Go的专栏订阅了，现在出了几期了，可以慢慢开始了解，看看使用的场景。 Java还是最重要的，只是自己有个心结，不知道能不能从头再静下心来。没想这么多年了，自己却基本还是吃老本，羞愧，汗颜。 加油吧 English英语还是要继续学习，不过一直都没有坚持下来，还是断断续续。 其实之前在publisher的时候，用英文交流的机会更多，David反而一般说中文，哈哈哈。 还是要坚持，之前每天一篇VOA，感觉还是不错的，还是要继续。 Sport因为忙和懒，运动的机会反而少了，一周才一次，感觉远远不够。 之前买的跳绳什么的，也就用了几次。 一周还是要至少运动两次，精力好的话就早上去，跳绳比较适合。 如果晚上的话，还是去运河边上去快走和跑步。 想当然心情不好的时候，都是骑着自己的小破车乱走，杭州都走遍了，哈哈，不能让他荒废了。 Life最近注册了滴滴，也开了几单，感觉就想做的时候做一点还是可以的，但是如果做为赚外快的方式，还是性价比低了一些。 看吧，没事想开开车的时候就可以做一做，也挺好的，就是要注意不要违章了，那样挺麻烦的。 感情生活也就这样吧，一个人好好呆着，未来未来，不用太操心，还是看之后的路吧。 人的一生，我才过了三分之一左右，还是要好好生活。 坚持本心，不忘初衷，相信日子会一天天好起来了。加油。","link":"/2018/08/19/2018/08/2018-08-19-%5BPrivate%5DNext_To_Do/"},{"title":"Data Structure In Python","text":"Data Structure In PythonToday I met a problem that I would like to count the duplicate items in a list. Generally, I could iterate the list and count it by myself. But I don’t think that’s a good idea, and python perhaps have the easiest way to do this. So I searched for the solution and find it. there is a build-in function to do this. 123456789&gt;&gt;&gt; list_test = ['test','test','xiche','xiche','xiche',1,1,1,1,1,['a']]&gt;&gt;&gt; print(list_test.count('test'))2&gt;&gt;&gt; print(list_test.count('xiche'))3&gt;&gt;&gt; print(list_test.count(1))5&gt;&gt;&gt; print(list_test.count(['a']))1 So it’s important to be familar with the usage of data structure in python and it will help you to code more efficient. And let me summarize some basic operations about them and I will continue to update this article when I find some new interesting tips. Learn to look into offical document is very import. ListList is a basic data structure, and in python, it’s very flexible and could use it as below: 12345678910111213141516171819202122232425262728293031323334353637383940414243def list_test(): list_01 = [] list_01.append('a') list_01.append('b') list_01.remove('b') list_02 = [1] list_03 = list_01 + list_02 print(list_03) #['a', 1] for item in list_03: print(item) #a #1 print(len(list_03)) #2 list_04 = list_03.copy() # list_04 = list_03[:] list_03.clear() print(list_03) #[] print(list_04) #['a', 1] list_04.reverse() print(list_04) #[1, 'a'] #list_04.sort() #print(list_04) #Traceback (most recent call last): # File &quot;.\\python_structure.py&quot;, line 25, in &lt;module&gt; # list_04.sort() #TypeError: '&lt;' not supported between instances of 'str' and 'int' list_04 = [2,3,4,9] list_04.sort() print(list_04) #[2, 3, 4, 9] list_04.sort(reverse=True) print(list_04) #[9, 4, 3, 2] print(list_04.index(3)) #2 StackAs we know, Stack is always based on List in most languages and the features is: LIFO: Last In First Out we could use method in List to simulate easily by only use append() and pop() 123456789101112131415161718def stack_test(): stack = [] stack.append(1) stack.append(2) stack.append(3) print(stack) #[1, 2, 3] value = stack.pop() print(value) #3 print(stack) #[1, 2] stack.append(4) value = stack.pop() print(value) #4 print(stack) #[1, 2] QueueQueue is another basic data structure, there is a module queue to handle this: FIFO: First In First Out 1234567891011121314import queuedef queue_test(): queue_01 = queue.Queue() queue_01.put(1) queue_01.put(2) queue_01.put(3) value = queue_01.get() print(value) #1 queue_01.put(4) value = queue_01.get() print(value) print(queue_01.empty()) #False Dictionaries(Map)Dictionaries is another import data structure in python. 1234567891011121314151617181920212223242526def dictionary_test(): fruits = {'banana': 1, 'apple': 2} print(fruits) #{'banana': 1, 'apple': 2} print(fruits['apple']) #1 print(list(fruits)) #['banana', 'apple'] print(sorted(fruits)) #['apple', 'banana'] del fruits['apple'] print(fruits) #{'banana': 1} fruits = dict([('apple', 10), ('banana', 20), ('watermelon', 30)]) print(fruits) #{'apple': 10, 'banana': 20, 'watermelon': 30} fruits = dict(apple=11, banana=21, watermelon=31) print(fruits) #{'apple': 11, 'banana': 21, 'watermelon': 31} for fruit_name in fruits: print(fruits[fruit_name]) #11 #21 #31 SetsSets is usually used to remove the duplicate values 123456789101112131415161718192021222324def sets_test(): fruits = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'} print(fruits) #{'banana', 'orange', 'pear', 'apple'} print('apple' in fruits) #True fruits = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana'] print(fruits) #['apple', 'orange', 'apple', 'pear', 'orange', 'banana'] fruits = set(fruits) print(fruits) #{'apple', 'orange', 'banana', 'pear'} sets_01 = {'a','b','c'} sets_02 = {'c','d','e'} print(sets_01 &amp; sets_02) #'c' print(sets_01 | sets_02) #{'e', 'a', 'c', 'b', 'd'} print(sets_01 - sets_02) #{'b', 'a'} Tuples and SequencesTuples and sequences is also flexible. 1234567891011121314def test_tuples_sequences(): tuples_01 = 100,200,'test' print(tuples_01) #(100, 200, 'test') print(tuples_01[2]) #test tuples_02 = 300,400 tuples_01 = tuples_01, tuples_02 print(tuples_01) #((100, 200, 'test'), (300, 400)) var_01,var_02,var_03 = 100,200,'test' print(var_01, var_02, var_03) #100 200 test To Be Continued…please refer to https://docs.python.org/3/tutorial/datastructures.html for more information.","link":"/2018/08/28/2018/08/2018-08-28-PyDataStructure/"},{"title":"Files Operation In Python","text":"12# update log:# 08/30/2018: update the method name style Files Operation In PythonWe usually need to handle files like read/write files with different type, so today I’ll show some codes to do this. Read/Write txt filesFor a txt file, we often want to read the file to a string list, sometimes we don’t like to include the \\r\\n, then we could use fin.read().splitlines(). So I create a txt Utils as below to handle several situations 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class TxtUtils: @classmethod def write_list_to_file_with_newline(cls, filePath, rowsList, mode='w'): PathUtils.check_make_dir_exist(filePath) with open(filePath, mode) as f: for row in rowsList: f.write(&quot;%s\\n&quot; % row) @classmethod def write_list_to_txt_file(cls, filePath, rowsList, mode='w'): PathUtils.check_make_dir_exist(filePath) with open(filePath, mode, newline='') as f: for row in rowsList: f.write(&quot;%s&quot; % row) @classmethod def remove_first_line(cls, filePath): rowList = cls.read_txt_rows_list_with_newline(filePath) rowList = list(rowList) cls.write_list_to_txt_file(filePath, rowList[1:]) @classmethod def read_first_line(cls, filePath): rowList = [] with open(filePath, newline='') as f: rowList = f.read().splitlines() rowList = (x for x in rowList if x.strip()) rowList = list(rowList) return rowList[0] if len(rowList) &gt; 0 else &quot;&quot; @classmethod def read_string_from_txt(cls, filePath): with open(filePath, newline='') as f: return f.read() @classmethod def read_txt_rows_list(cls, filePath): rowList = [] with open(filePath, newline='') as f: rowList = f.read().splitlines() rowList = (x for x in rowList if x.strip()) return list(rowList) @classmethod def read_txt_rows_list_with_newline(cls, filePath): rowList = [] with open(filePath, newline='') as f: rowList = f.readlines() rowList = (x for x in rowList if x.strip()) return list(rowList) @classmethod def replace_variables(cls, file_template, file_destination, dict): new_lines = [] for line in cls.read_txt_rows_list_with_newline(file_template): for key,val in dict.items(): line = line.replace(key, val) new_lines.append(line) cls.write_list_to_txt_file(file_destination, new_lines) CSV FilesCSV format is another common type for a file and I also create a simple utils for this. 1234column1,column2,column31,11,1112,22,2223,33,333 1234567891011121314151617class CSVUtils: @staticmethod def write_to_csv_file(filePath, rowsList, delimiterX=',',quotecharX=' ', quotingX=csv.QUOTE_MINIMAL): with open(filePath, 'w', newline='') as csvfile: spamwriter = csv.writer(csvfile, delimiter=delimiterX, quotechar=quotecharX, quoting=quotingX) for rows in rowsList: spamwriter.writerow(rows) @staticmethod def read_csv_row_list(filePath): csvRowsList = [] with open(filePath, newline='') as csvfile: csvReader = csv.reader(csvfile, delimiter=',', quotechar='|') for row in csvReader: csvRowsList.append(row) return csvRowsList Config fileThere are different type of config file, e.g. properties, ini, xml, json,… ini fileBelow it’s a simple utils only a function set, to set key/value to a ini file: 123456789101112131415161718192021222324252627282930class ConfigUtils: @staticmethod def set(filePath, section, key, value): lines = [] new_lines = [] sec_expected = False key_expected = False set_ready = False with open(filePath, 'r', newline='') as f: lines = f.readlines() for index, line in enumerate(lines): is_section = &quot;[&quot; in line.strip().lower() if(is_section and section.lower() in line.strip().lower()): sec_expected = True new_lines.append(line) continue if(sec_expected and key.lower() in line.lower()): key_expected = True line = &quot;{0}={1}\\r\\n&quot;.format(key, value) new_lines.append(line) continue if(is_section and sec_expected and not key_expected): new_lines.append(&quot;{0}={1}\\r\\n&quot;.format(key, value)) sec_expected = False key_expected = True new_lines.append(line) with open(filePath, 'w', newline='') as f: f.writelines(new_lines) Path UtilsUsually, we have to handle some path e.g. join path, get the file name from the path, convert the path string to other format. There are a lot of build-in method to do such thing, like os.path.basename(full_path), so that we could get the simple file name from the full path. Below it’s a simple utils currently. 123456789101112131415161718192021222324252627282930class PathUtils: @staticmethod def check_make_dir_exist(filePath): fileDir = getDirFromFullPath(filePath) if not os.path.exists(fileDir): os.makedirs(fileDir) @staticmethod def get_dir_name_from_full_path(filePath): fileDir = filePath index1 = filePath.rfind(&quot;\\\\&quot;) index2 = filePath.rfind(&quot;/&quot;) index = index1 if index1 &gt; index2 else index2 if(index &gt; -1): fileDir = filePath[0:index] return fileDir @staticmethod def get_filename_from_full_path(fullPath): file_name = os.path.basename(fullPath) # lastIndex = fullPath.rfind('\\\\') # if(lastIndex == -1): # lastIndex = fullPath.rfind('/') # if(lastIndex == -1): # lastIndex = 0 # return fullPath[lastIndex:] return file_name I will continue to update the utils. Please refer to https://github.com/bearfly1990/PowerScript/tree/master/Python3/mylib for more information.","link":"/2018/08/29/2018/08/2018-08-29-PyFileOperation/"},{"title":"Class In Python","text":"Class In Python今天简单总结下在Python中定义类的方法，包括属性，方法，继承等方面的内容。 主要还是根据自己的实践和踩过的坑来描述，后续如果有新的内容，会继续更新。 A Simple Class12345678910111213#simple_class.pyclass SimpleClass(): property_01 = 'property_01' def __init__(self): self.property_02 = 'property_02' print('init a class', self.__class__.__name__, self.property_01) def __del__(self): print ('destory class', self.__class__.__name__, self.property_02) def test(self): print('test')simple_class = SimpleClass()simple_class.test() 运行的结果如下： 1234PS C:\\Users\\mayn\\Desktop\\python_test\\class_in_python&gt; python .\\simple_class.pyinit a class SimpleClass property_01testdestory class SimpleClass property_02 长话短说，上面的例子中，定义了一个非常简单类，有两个属性property_01 property_02，两个内置方法__init__ __del__，代表了构造函数与销毁方法 同时，大家也能看到一个很重要的关键字self selfself在这边有点类似于Java中的this,代表了实例对象，代表当前对象的地址,对实例的许多操作都要靠它。 而神奇的是self并不是 python 的关键字(keyword)，只是按照惯例而叫这个名字，也就是说可以用别的名字代替。 比如，我可以用this,instance等等其实名字，只要是方法的第一个参数就可以: 12345678class SimpleClass(): def __init__(self): self.value = 1 def test(this, param): print(this.value + param)simple_class = SimpleClass()simple_class.test(1)# output: 2 上面的例子能正常运行，this, self都代表了该类的实例对象。但很多的 IDE 都只认self,所以大家最好用惯例，除非有特别的需要。 Class Property and Instance Property熟悉Java的大家都知道，在Java的类里，分为类变量和实例变量，同样的也有类方法和实例方法，通过static关键字来区分。 类变量和类方法都是所以类的实例共享的，并且可以通过Class.static Instance.static来访问。 有注意到第一个例子里，有property_01， property_02两个属性。他们区别就是一个是在方法外定义的，别一个是在方法体中直接使用self来定义使用。 那哪一个是类变量，另一个是实例变量呢？下面例子看完就知道了： 12345678910111213141516171819202122232425262728293031class SimpleClass(): property_01 = 'property_01' list_01 = [1] def __init__(self): self.property_02 = 'property_02' def __str__(self): return ','.join([self.property_01, self.property_02])SimpleClass.property_01 = 'property_01_changed'SimpleClass.list_01.append(2)simple_class_01 = SimpleClass()simple_class_02 = SimpleClass()simple_class_02.property_02 = 'property_02_changed'print(simple_class_01)# property_01_changed,property_02print(simple_class_02)# property_01_changed,property_02_changedprint('simple_class_01:',simple_class_01.list_01, 'simple_class_02:', simple_class_02.list_01)# simple_class_01: [1, 2] simple_class_02: [1, 2]simple_class_02.property_01 = 'property_01_changed_again'simple_class_02.list_01.append(3)print(SimpleClass.property_01)# property_01_changedprint(simple_class_01)# property_01_changed,property_02print(simple_class_02)# property_01_changed_again,property_02_changedprint('simple_class_01:',simple_class_01.list_01, 'simple_class_02:', simple_class_02.list_01)# simple_class_01: [1, 2, 3] simple_class_02: [1, 2, 3] 是的，如果你仔细看就发现他没有那么严格的类变量与实例变量的区分。 如果是在方法体外的，可以直接通过Class.property来访问并且改变值，但是如果通过Instance.property改变了值之后。该实例的变量就会改变，而Class指向的变量还是没变，其它实例变量也是同一个值。 Class Method and Static Method我们再来看一个例子： 1234567891011121314151617181920class SimpleClass(): @staticmethod def static_method(): print('this is static method, no cls need') @classmethod def class_method_01(cls): print('this is class method 01') @classmethod def class_method_02(cls): cls.class_method_01() print('this is class method 02')SimpleClass.class_method_02()# this is class method 01# this is class method 02SimpleClass.static_method()# this is static method, no cls need 可以看到，可以定义static method和class method来通过类名直接访问使用。 那他们的区别是什么呢？ static method静态方法其实就是一个单纯的方法，相当于把类外的一个普通方法挪到这个类中，通过类名访问。但是他其实是和这个类及其实例没有任何的关联，所以他不能调用得到这个类的其实信息。 class method类方法刚可以通过cls得到类的其他信息，比如调用其它的类方法。 Inherit我们再来看一个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041class Teacher(object): def __init__(self, name): self.name = name self.__private_data = 'Teacher private data' def start_teach(self): print('start teach!') print(self.__private_data) self.__private_method() def say_hi(self): print(&quot;hi!I'm {}&quot;.format(self.name)) def __private_method(self): print('private method') def _protect_method(self): print('protect method')class MathTeacher(Teacher): def __init__(self, name, salary): super(MathTeacher, self).__init__(name) self.salary = salary def start_teach(self): print('MathTeacher start to teach! with salary={}'.format(self.salary)) self._protect_method()teacher = Teacher('Teacher_01')teacher.say_hi()# hi!I'm Teacher_01teacher.start_teach()# start teach!# Teacher private data# private methodmathTeacher = MathTeacher('Math_01', 100)mathTeacher.say_hi()# hi!I'm Math_01mathTeacher.start_teach()# MathTeacher start to teach! with salary=100# protect method private/protect data在python中，默认双下划线__是私有的，外部不能访问，在类中代表私有属性和方法，子类不能继承使用。 单下划线_也有特殊意义，有点类似于java中的protect简单总结如下： 123_xxx ：保护变量，意思是只有类对象和子类对象能够访问到这些变量(尽量避免在类外部直接修改)__xxx__ ：系统定义名字__xxx ：类中的私有变量名 multi-inheritancepython 是支持多重继承的，我还没有工作中用过，暂略，到时研究下。 Abstract Class下面是一个简单的虚类的例子，Teacher 类是不能实例化的，而MathTeacher一定要实现start_teach方法。 1234567891011121314151617181920212223from abc import ABC,ABCMeta,abstractmethod,abstractpropertyclass Teacher(ABC): __metaclass__ = ABCMeta def __init__(self, name): self.name = name @abstractmethod def start_teach(self): pass def say_hi(self): print('hi!')class MathTeacher(Teacher): def __init__(self, name): super(MathTeacher, self).__init__(name) def start_teach(self): print('MathTeacher start to teach!')math_teacher = MathTeacher('Math01')math_teacher.start_teach()","link":"/2018/09/05/2018/09/2018-09-05-PyClass/"},{"title":"Python RESTful API","text":"RESTful APIREST 全称是 Representational State Transfer，中文意思是表述性状态转移。 它首次出现在 2000 年 Roy Fielding 的博士论文中，Roy Fielding 是 HTTP 规范的主要编写者之一。 他在论文中提到：”我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。REST 指的是一组架构约束条件和原则。” 如果一个架构符合 REST 的约束条件和原则，我们就称它为 RESTful 架构。 REST 本身并没有创造新的技术、组件或服务，而隐藏在 RESTful 背后的理念就是使用 Web 的现有特征和能力， 更好地使用现有 Web 标准中的一些准则和约束。虽然 REST 本身受 Web 技术的影响很深， 但是理论上 REST 架构风格并不是绑定在 HTTP 上，只不过目前 HTTP 是唯一与 REST 相关的实例。 所以我们这里描述的 REST 也是通过 HTTP 实现的 REST。 Python flask-restfulflask是 python 的一个轻量级网站开发框架，直接利用他就能开发简单的web api。 但是使用flask-restful的话，就更加的简单和适合。 flask下面是直接使用 flask 的例子: 12345678910from flask import Flaskapp = Flask(__name__)@app.route('/helloworld')def hello_world(): return &quot;Hello World!&quot;if __name__ == &quot;__main__&quot;: app.run(debug=True) 上面是一个非常简单的使用 flask 开发 api 的例子，在运行后直接就能访问使用 1234567891011PS C:\\Users\\mayn\\Desktop&gt; python ./hello.py * Serving Flask app &quot;hello&quot; (lazy loading) * Environment: production WARNING: Do not use the development server in a production environment. Use a production WSGI server instead. * Debug mode: on * Restarting with stat * Debugger is active! * Debugger PIN: 215-586-846 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) flask-restful如果使用flask-restful的话，那就会结构更清晰一些： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from flask import Flaskfrom flask_restful import reqparse, abort, Api, Resourceapp = Flask(__name__)api = Api(app)TODOS = { 'todo1': {'task': 'Build an API'}, 'todo2': {'task': 'Test it'}, 'todo3': {'task': 'Write Blog'},}def abort_if_todo_doesnt_exist(todo_id): if todo_id not in TODOS: abort(404, message=&quot;Todo {} doesn't exist.&quot;.format(todo_id))parser = reqparse.RequestParser()parser.add_argument('task')# 3个方法，对应get/delete/putclass Todo(Resource): def get(self, todo_id): abort_if_todo_doesnt_exist(todo_id) return TODOS[todo_id] def delete(self, todo_id): abort_if_todo_doesnt_exist(todo_id) del TODOS[todo_id] return '', 204 def put(self, todo_id): args = parser.parse_args() task = {'task': args['task']} return task, 201# 2个方法，对应get/postclass TodoList(Resource): def get(self): return TODOS def post(self): args = parser.parse_args() todo_id = int(max(TODOS.keys()).lstrip('todo')) + 1 todo_id = 'todo%i' % todo_id TODOS[todo_id] = {'task': args['task']} return TODOS[todo_id], 201# 绑定api对应的类api.add_resource(TodoList, '/todos')api.add_resource(Todo, '/todos/&lt;todo_id&gt;')if __name__ == &quot;__main__&quot;: app.run(debug=True) 总结总体来说，使用flask-restful框架真的方便很多, 一些处理已经封装好。比如json就是默认的输入输出，状态状态码也直接放在 return 中和返回的内容一起就好。 更多信息请参考： todo.py RESTful 架构详解 python 实现 RESTful 服务（基于 flask)","link":"/2018/11/09/2018/11/2018-11-09-PyRestfulAPI/"},{"title":"Progress Bar in Python","text":"Progress Bar in Python很多时候，我们在测试的过程中，希望能够直观的看到测试的进度，而不是去看 log 或者查询数据库得到信息。 这个时候，如果有数据完成的进度的话就非常的方便。 今天就简单总结下这两天查询的资料，并且已经在实际中用到。 How如果要在 Console 中实现进度条，那就是要所有所有字符全部在同一行，而且可以修改。 如果直接使用 print 语句，python 会结尾加上换行符(\\n)，这就导致在 Console 下一旦被 print 之后就无法再修改了。所以我们不能直接使用 print 通过 sys.stdout.write() 我们就可以在 Console 输出字符，并且不会加上任何结尾。 通过 sys.stdout.flush() 可以把输出暂时打印在 Console 中。 而通过转义字符\\r，就可以回到首行，然后如果继续输出的话，就可以连续在同一行改变字符，实现进度的效果。 下面就简单介绍一些进度条的实现，网上一些作者的实例。 Demo 01下面是一个最简单的实现： 123456import sys, timefor i in range(50): sys.stdout.write('{0}\\r'.format( '*' * (i + 1))) sys.stdout.flush() time.sleep(0.2) Demo 02这个例子是上一个例子的加强版，有数值进度和向后推进的箭头。 1234567import sys,timej = '#'for i in range(1,61): j += '#' sys.stdout.write(str(int((i/60)*100))+'% '+j+'-&gt;'+ &quot;\\r&quot;) sys.stdout.flush() time.sleep(0.2) Demo 03这个例子更简单一些，只有进度的数值。 12345678910import sysfrom time import sleepdef viewBar(i): output = sys.stdout for count in range(0, i + 1): second = 0.1 sleep(second) output.write('\\rcomplete percent -----&gt;:%.0f%%' % count) output.flush()viewBar(100) Demo 04这个例子用了一个比较常用的第三方库tqdm,有兴趣大家可以去搜一下官方文档，还是比较灵活的。 1234from time import sleepfrom tqdm import tqdmfor i in tqdm(range(1, 500), ascii=True): sleep(0.01) Demo 05这个例子其实和Demo 02差不多的 12345678910111213import sysimport timedef view_bar(num,total): rate = num / total rate_num = int(rate * 100) #r = '\\r %d%%' %(rate_num) r = '\\r%s&gt;%d%%' % ('=' * rate_num, rate_num,) sys.stdout.write(r) sys.stdout.flushif __name__ == '__main__': for i in range(0, 101): time.sleep(0.1) view_bar(i, 100) Demo 06这个例子是我觉得相对比较好的，自己改良了一下放在自己项目中用了，下面是原始的 sample，我加了线程的处理，这样就可以异步写进度，而不会影响原来的进程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import sys, timeimport randomimport _threadclass ShowProcess(): &quot;&quot;&quot; 显示处理进度的类 调用该类相关函数即可实现处理进度的显示 &quot;&quot;&quot; i = 0 # 当前的处理进度 max_steps = 0 # 总共需要处理的次数 max_arrow = 50 #进度条的长度 infoDone = 'done' # 初始化函数，需要知道总共的处理次数 def __init__(self, max_steps, infoDone = 'Done'): self.max_steps = max_steps self.i = 0 self.infoDone = infoDone # 显示函数，根据当前的处理进度i显示进度 # 效果为[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;]100.00% def show_process(self, i=None): if i is not None: self.i = i else: self.i += 1 num_arrow = int(self.i * self.max_arrow / self.max_steps) #计算显示多少个'&gt;' num_line = self.max_arrow - num_arrow #计算显示多少个'-' percent = self.i * 100.0 / self.max_steps #计算完成进度，格式为xx.xx% process_bar = '[{}{}] {:02}%\\r'.format('#' * num_arrow, '-' * num_line,percent) #带输出的字符串，'\\r'表示不换行回到最左边#带输出的字符串，'\\r'表示不换行回到最左边 sys.stdout.write(process_bar) #这两句打印字符到终端 sys.stdout.flush() if self.i &gt;= self.max_steps: self.close() def close(self): print('') print(self.infoDone) self.i = 0VALUE = 0def start_import(): global VALUE while(True): VALUE += 1 time.sleep(0.2 * random.random()) if VALUE == 100: time.sleep(2) breakdef monitor_process( threadName, delay): max_steps = 100 process_bar = ShowProcess(max_steps, 'OK') # 1.在循环前定义类的实体， max_steps是总的步数， infoDone是在完成时需要显示的字符串 # for i in range(max_steps): while(True): process_bar.show_process(VALUE) # 2.显示当前进度 time.sleep(1) if(VALUE == 100): process_bar.show_process(100) break_thread.start_new_thread( monitor_process, (&quot;Thread-1&quot;, 2, ) )start_import() 更多信息可以参考： https://www.cnblogs.com/jsben/p/5792952.html https://pypi.org/project/tqdm/","link":"/2018/09/12/2018/09/2018-09-12-PyProgressBar/"},{"title":"XPath in Python","text":"XPath In Python在对XML文件进行处理的时候，可以使用标准DOM的 api，但是相对来说不是很方便。 使用XPath的话就可以快速的定位节点，选取得到想要的值。 下面就对在Python中使用XPath处理XML文档做简单的介绍。 XML File下面是一个简单的XML的例子，可以看到基本的Element, Node, Attribute, Text等都是有的。 以&lt;bookstore&gt;为根结点，有多个&lt;book&gt;，每个&lt;book&gt;都有不同的种类，再下一层是书的&lt;title&gt; &lt;author等具体信息。 层次与内容还是比较清晰的。 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;bookstore&gt;&lt;book category=&quot;COOKING&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Everyday Italian&lt;/title&gt; &lt;author&gt;Giada De Laurentiis&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;30.00&lt;/price&gt;&lt;/book&gt;&lt;book category=&quot;CHILDREN&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;book category=&quot;WEB&quot;&gt; &lt;title lang=&quot;en&quot;&gt;XQuery Kick Start&lt;/title&gt; &lt;author&gt;James McGovern&lt;/author&gt; &lt;author&gt;Per Bothner&lt;/author&gt; &lt;author&gt;Kurt Cagle&lt;/author&gt; &lt;author&gt;James Linn&lt;/author&gt; &lt;author&gt;Vaidyanathan Nagarajan&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;49.99&lt;/price&gt;&lt;/book&gt;&lt;book category=&quot;WEB&quot;&gt; &lt;title lang=&quot;en&quot;&gt;Learning XML&lt;/title&gt; &lt;author&gt;Erik T. Ray&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;39.95&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; XPathXPath 于 1999 年 11 月 16 日 成为 W3C 标准。 XPath 被设计为供 XSLT、XPointer 以及其他 XML 解析软件使用 下面主要简单介绍主要的语法： Select NodeXPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。 下面列出了最有用的路径表达式： 表达式 描述 nodename 选取此节点的所有子节点。 / 从根节点选取。 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 . 选取当前节点。 .. 选取当前节点的父节点。 @ 选取属性。 Predicates谓语用来查找某个特定的节点或者包含某个指定的值的节点。 谓语被嵌在方括号中。 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 路径表达式 结果 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素。 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素。 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素。 /bookstore/book[position()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素。 //title[@lang=’eng’] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。 /bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 /bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 Unknow NodeXPath 通配符可用来选取未知的 XML 元素。 通配符 描述 * 匹配任何元素节点。 @* 匹配任何属性节点。 node() 匹配任何类型的节点。 Select Several Paths通过在路径表达式中使用|运算符，可以选取若干个路径。 路径表达式 结果 //book/title | //book/price 选取 book 元素的所有 title 和 price 元素。 //title | //price 选取文档中的所有 title 和 price 元素。 /bookstore/book/title | //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 Practise in python下面主要简单介绍使用xml.etree.ElementTree在python中使用XPath Get node by text value在上面的例子中，我们想要得到书名为Learning XML的书的信息，所以XPath路径应该为//book[title='Learning XML'] 然后再把信息打印出来 12345678910111213import xml.etree.ElementTree as ETxml_file_path = 'sample.xml'xml_et = ET.parse(xml_file_path)node_book = xml_et.find(&quot;.//book[title='Learning XML']&quot;)print(node_book)print(node_book.tag)print(str(node_book.attrib))for child in node_book: print(child.tag, child.attrib, child.text) output: 12345678PS C:\\Users\\mayn\\Desktop\\python_test\\xpath_in_python&gt; python .\\xpath_python.py&lt;Element 'book' at 0x03642CC0&gt;book{'category': 'WEB'}title {'lang': 'en'} Learning XMLauthor {} Erik T. Rayyear {} 2003price {} 39.95 Get node by attribute value如果我们想找到种类为WEB的所以的书，那我们可以用.//book[@category='WEB']。 但是如果我们还是用find的话，只能得到第一个符合条件的，所以我们需要用findall。 1234567891011121314import xml.etree.ElementTree as ETxml_file_path = 'sample.xml'xml_et = ET.parse(xml_file_path)node_books = xml_et.findall(&quot;.//book[@category='WEB']&quot;)print(node_books)for node_book in node_books: print(node_book.tag) print(str(node_book.attrib)) for child in node_book: print(child.tag, child.attrib, child.text) 123456789101112131415161718PS C:\\Users\\mayn\\Desktop\\python_test\\xpath_in_python&gt; python .\\xpath_python.py[&lt;Element 'book' at 0x02F12AB0&gt;, &lt;Element 'book' at 0x02F12CC0&gt;]book{'category': 'WEB'}title {'lang': 'en'} XQuery Kick Startauthor {} James McGovernauthor {} Per Bothnerauthor {} Kurt Cagleauthor {} James Linnauthor {} Vaidyanathan Nagarajanyear {} 2003price {} 49.99book{'category': 'WEB'}title {'lang': 'en'} Learning XMLauthor {} Erik T. Rayyear {} 2003price {} 39.95 Summary总之，因为XML的结构化特性，使用XPath能非常方便的去query我们想要的信息。 在python中，结合ElementTree的API，能很方便的操作XML文档。 更多信息可以参考： http://www.w3school.com.cn/xpath/index.asp https://docs.python.org/3/library/xml.etree.elementtree.html","link":"/2018/09/08/2018/09/2018-09-08-PyXPath/"},{"title":"RESTful API Doc","text":"RESTful API Doc针对RESTful API,有许多工具可以用来编写文档比如Swagger2，之前发现一个挺好用的库就是apidocjs 这个库支持大多数流行程序语言，把接口相关的信息放在注释中，而这个js库解析注释中的有效信息生成html文档，需要安装node.js。 Sample下面直接根据昨天的API例子来添加注释，生成文档。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125from flask import Flaskfrom flask_restful import reqparse, abort, Api, Resourceapp = Flask(__name__)api = Api(app)TODOS = { 'todo1': {'task': 'Build an API'}, 'todo2': {'task': 'Test it'}, 'todo3': {'task': 'Write Blog'},}def abort_if_todo_doesnt_exist(todo_id): if todo_id not in TODOS: abort(404, message=&quot;Todo {} doesn't exist.&quot;.format(todo_id))parser = reqparse.RequestParser()parser.add_argument('task')class Todo(Resource): &quot;&quot;&quot; @api {get} /todo/task/&lt;task_id&gt; get todo by id @apiVersion 0.1.0 @apiName GetTodoItem @apiGroup Todo @apiPermission all @apiParam {Number} task_id The todo id. @apiExample Example usage: https://localhost:5000/v1/todo/task/todo1 @apiSuccessExample {json} Success-Response: HTTP/1.1 200 OK {'task': 'Build an API'} &quot;&quot;&quot; &quot;&quot;&quot; @api {get} /todo/&lt;todo_id&gt; get todo by id @apiVersion 0.2.0 @apiName GetTodoItem @apiGroup Todo @apiPermission all @apiParam {Number} todo_id The todo id. @apiExample Example usage: https://localhost:5000/v1/todo/todo1 @apiSuccessExample {json} Success-Response: HTTP/1.1 200 OK {'task': 'Build an API'} &quot;&quot;&quot; def get(self, todo_id): abort_if_todo_doesnt_exist(todo_id) return TODOS[todo_id] &quot;&quot;&quot; @api {delete} /todo/&lt;todo_id&gt; delete todo task by id @apiVersion 0.1.0 @apiName DeleteTodo @apiGroup Todo @apiPermission admin @apiParam {Number} todo_id The todo id. @apiExample Example usage: https://localhost:5000/v1/todo/todo1 @apiSuccessExample {json} Success-Response: HTTP/1.1 204 No Content @apiErrorExample Response (example): HTTP/1.1 401 Not Authenticated {&quot;error&quot;: &quot;NoAccessRight&quot;} &quot;&quot;&quot; def delete(self, todo_id): abort_if_todo_doesnt_exist(todo_id) del TODOS[todo_id] return '', 204 &quot;&quot;&quot; @api {put} /todo/&lt;todo_id&gt; put a new task @apiVersion 0.1.0 @apiName PutTask @apiGroup Todo @apiPermission admin @apiParam {Number} todo_id The todo id. @apiExample Example usage: https://localhost:5000/v1/todo/todo1 {&quot;task&quot;: &quot;New Task&quot;} @apiSuccessExample {json} Success-Response: HTTP/1.1 201 Created {&quot;task&quot;: &quot;New Task&quot;} &quot;&quot;&quot; def put(self, todo_id): args = parser.parse_args() task = {'task': args['task']} return task, 201class TodoList(Resource): &quot;&quot;&quot; @api {get} /todos get todolist @apiVersion 0.1.0 @apiName GetTodoList @apiGroup TodoList @apiPermission all @apiExample Example usage: https://localhost:5000/v1/todos &quot;&quot;&quot; def get(self): return TODOS &quot;&quot;&quot; @api {post} /todos post todo list @apiVersion 0.1.0 @apiName PostTodoList @apiGroup TodoList @apiPermission admin @apiExample Example usage: https://localhost:5000/v1/todo/todo1 {&quot;task&quot;:&quot;New Task&quot;} @apiSuccessExample {json} Success-Response: HTTP/1.1 201 Created {'task': 'New Task'} &quot;&quot;&quot; def post(self): args = parser.parse_args() todo_id = int(max(TODOS.keys()).lstrip('todo')) + 1 todo_id = 'todo%i' % todo_id TODOS[todo_id] = {'task': args['task']} return TODOS[todo_id], 201api.add_resource(TodoList, '/todos')api.add_resource(Todo, '/todos/&lt;todo_id&gt;')if __name__ == &quot;__main__&quot;: app.run(debug=True) 运行下面的命令，就可以在pytrestapi_doc目录下生成api文档 12PS C:\\Users\\mayn\\Desktop\\workspace&gt; apidoc -i .\\pyrestapi\\ -o .\\pytrestapi_docinfo: Done. 下面是报告的结果,可以感受下：https://bearfly1990.github.io/apidoc/pyrestapi-doc/ 下面是简单的环境配置步骤： 安装node.js 安装 apidoc npm install apidoc -g 装备好已经编写好对应注释的代码项目 apidoc -i myapp/ -o apidoc/ 就能生成在apidoc文件夹中 更多信息请参考官网： apidocjs","link":"/2018/11/10/2018/11/2018-11-10-PyRestfulAPIDoc/"},{"title":"Software Test Categories","text":"软件测试的分类在刚入测试坑的时候，测试的基本概念还停留在早期学习的相关知识。对于黑盒/白盒，单元测试，集成测试，冒烟测试等多多少少有点了解，但其实分的不是很清楚。 入坑之后，补充过基础知识，参加过一些培训，再加上真正深入测试工作，才有了更多的体会，恍然大悟的感觉。 今天主要总结下软件测试的分类。 按软件测试的阶段单元测试(Unit Testing)单元测试是对软件组成单元进行测试。其目的是检验软件基本组成单位的正确性。测试的对象是软件设计的最小单位：模块。所以也可能被称为模块测试 测试阶段：编码后 测试对象：最小模块 测试人员：白盒测试工程师或开发工程师 测试依据：代码和注释+详细设计文档 测试方法：白盒测试 测试内容：模块接口测试、局部数据结构测试、路径测试、错误处理测试、边界测试 集成测试(Integration Testing)集成测试也称联合测试、组装测试，将程序模块采用适当的集成策略组装起来，对系统的接口及集成后的功能进行正确性检测的测试工作。主要目的是检查软件单位之间的接口是否正确。 测试阶段：一般单元测试之后进行 测试对象：模块间的接口 测试人员：白盒测试工程师或开发工程师 测试依据：单元测试的模块+概要设计文档 测试方法：黑盒测试与白盒测试相结合 测试内容：模块之间数据传输、模块之间功能冲突、模块组装功能正确性、全局数据结构、单模块缺陷对系统的影响 系统测试(System Testing)将软件系统看成是一个系统的测试。包括对功能、性能以及软件所运行的软硬件环境进行测试。一般测试的时间也是最多花在系统测试阶段，耗费最多的人时，并且自动化的 Case 都要覆盖。 测试阶段：集成测试通过之后 测试对象：整个系统（软、硬件） 测试人员：黑盒测试工程师 测试依据：需求规格说明文档 测试方法：黑盒测试 测试内容：功能、界面、可靠性、易用性、性能、兼容性、安全性等 验收测试(Acceptance Testing)验收测试是部署软件之前的最后一个测试操作。它是技术测试的最后一个阶段，也称为交付测试。验收测试的目的是确保软件准备就绪，按照项目合同、任务书、双方约定的验收依据文档，向软件购买都展示该软件系统满足原始需求。 测试阶段：系统测试通过之后 测试对象：整个系统（包括软硬件）。 测试人员：主要是最终用户或者需求方。 测试依据：用户需求、验收标准 测试方法：黑盒测试 测试内容：同系统测试(功能…各类文档等) 按是否查看代码划分黑盒测试(Black-box Testing)黑盒测试也称功能测试，测试中把被测的软件当成一个黑盒子，不关心盒子的内部结构是什么，只关心软件的输入数据与输出数据。 白盒测试(White-box Testing)白盒测试又称结构测试、透明盒测试、逻辑驱动测试或基于代码的测试。白盒指的打开盒子，去研究里面的源代码和程序结果。 灰盒测试(Gray-Box Testing)灰盒测试，是介于白盒测试与黑盒测试之间的一种测试，灰盒测试多用于集成测试阶段，不仅关注输出、输入的正确性，同时也关注程序内部的情况。 按是否执行程序划分静态测试(Static testing)静态测试是指不运行被测程序本身，仅通过分析或检查源程序的语法、结构、过程、接口等来检查程序的正确性。对需求规格说明书、软件设计说明书、源程序做结构分析、流程图分析、符号执行来找错。 检查项： 代码风格和规则审核； 程序设计和结构的审核； 业务逻辑的审核； 走查、审查与技术复审手册。 静态质量：度量所依据的标准是 ISO9126。在该标准中，软件的质量用以下几个方面来衡量，即: 功能性(Functionality) 可靠性(Reliability) 可用性(Usability) 有效性(Efficiency) 可维护性（Maintainability） 可移植性(Portability)。 动态测试(Dynamic testing)动态测试方法是指通过运行被测程序，检查运行结果与预期结果的差异，并分析运行效率、正确性和健壮性等性能。这种方法由三部分组成：构造测试用例、执行程序、分析程序的输出结果。 按是否手工执行测试手工测试(Manual testing)手工测试就是由人去一个一个的输入用例，然后观察结果，和机器测试相对应，属于比较原始但是必须的一个步骤。 优点： 自动化无法替代探索性测试、发散思维类无既定结果的测试。 缺点： 执行效率慢，量大易错。 自动化测试(Automation Testing)就是在预设条件下运行系统或应用程序，评估运行结果，预先条件应包括正常条件和异常条件。简单说自动化测试是把以人为驱动的测试行为转化为机器执行的一种过程。 自动化测试又可分为： 功能测试自动化 性能测试自动化 安全测试自动化。 通常所说的自动化是指功能测试自动化。AI 技术的兴起发展，类似可预见工作都有可能会被伪 AI 代替。 按软件质量特性分类安全测试(Security Testing)安全测试是在 IT 软件产品的生命周期中，特别是产品开发基本完成到发布阶段，对产品进行检验以验证产品符合安全需求定义和产品质量标准的过程。 现在对安全知识的普及，大家意识都提上来了。比如现在越来越多的不支持 HTTP 协议，转用 HTTPS 等。 性能测试(Performance Testing)性能测试是通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。 可靠性测试和压力测试都属于性能测试，两者可以结合进行。 通过负载测试，确定在各种工作负载下系统的性能，目标是测试当负载逐渐增加时，系统各项性能指标的变化情况。压力测试是通过确定一个系统的瓶颈或者不能接受的性能点，来获得系统能提供的最大服务级别的测试。 可靠性测试(Reliability Testing)软件可靠性 (software reliability )是软件产品在规定的条件下和规定的时间区间完成规定功能的能力。 规定的条件是指直接与软件运行相关的使用该软件的计算机系统的状态和软件的输入条件，或统称为软件运行时的外部输入条件;规定的时间区间是指软件的实际运行时间区间;规定功能是指为提供给定的服务，软件产品所必须具备的功能。 软件可靠性不但与软件存在的缺陷和(或)差错有关，而且与系统输入和系统使用有关。软件可靠性的概率度量称软件可靠度。 安装测试(Installing Testing)安装测试即对于需要安装的软件的测试，保证软件的安装包能正常安装部署，需要考虑到系统版本，环境依赖等因素。 兼容性测试(Compatibility Testing)软件兼容性测试是指检查软件之间能否正确地进行交互和共享信息。随着用户对来自各种类型软件之间共享数据能力和充分利用空间同时执行多个程序能力的要求，测试软件之间能否协作变得越来越重要。软件兼容性测试工作的目标是保证软件按照用户期望的方式进行交互。 其它测试分类冒烟测试(Smoke Testing)冒烟测试目的是确认软件基本功能正常，冒烟测试的执行者是版本编译人员。 现在基本执行对象为测试人员，在正规测试一个新版本之前，投入较少的人力和时间验证基本功能，通过则测试准入。 我忘记有一个很形象的比喻，据说最早在硬件测试中，对测试样品通电之后，如果直接冒烟了，那么说明这个样品直接不合格，不需要后续的测试了，打回重做。 随机测试(Ad-hoc Testing)随机测试主要是根据测试者的经验对软件进行功能和性能抽查。 根据测试说明书执行用例测试的重要补充手段，是保证测试覆盖完整性的有效方式和过程。 随机测试主要是对被测软件的一些重要功能进行复测，也包括测试那些当前的测试用例(TestCase)没有覆盖到的部分。 探索性测试(Exploratory testing)探索性测试可以说是一种测试思维技术。它没有很多实际的测试方法、技术和工具，但是却是所有测试人员都应该掌握的一种测试思维方式。探索性强调测试人员的主观能动性，抛弃繁杂的测试计划和测试用例设计过程，强调在碰到问题时及时改变测试策略。 探索性测试自动化暂时无法代替。好的测试人员也无法被代替。 回归测试(Regression Testing)回归测试是指修改了旧代码后，重新进行测试以确认修改没有引入新的错误或导致其他代码产生错误。自动回归测试将大幅降低系统测试、维护升级等阶段的成本。 在整个软件测试过程中占有很大的工作量比重，软件开发的各个阶段都会进行多次回归测试。通过选择正确的回归测试策略来改进回归测试的效率和有效性是很有意义的。 α 测试(Alpha Testing)α 测试是由一个用户在开发环境下进行的测试，也可以是公司内部的用户在模拟实际操作环境下进行的测试。α 测试的目的是评价软件产品的 FLURPS(即功能、局域化、可使用性、可靠性、性能和支持)。 大型通用软件，在正式发布前，通常需要执行 Alpha 和 Beta 测试。α 测试不能由程序员或测试员完成。 β 测试(Beta Testing)Beta 测试是一种验收测试。Beta 测试由软件的最终用户们在一个或多个客户场所进行。 α 测试与 Beta 测试的区别：Findyou 测试的场所不同：Alpha 测试是指把用户请到开发方的场所来测试,beta 测试是指在一个或多个用户的场所进行的测试。 Alpha 测试的环境是受开发方控制的,用户的数量相对比较少,时间比较集中。beta 测试的环境是不受开发方控制的,用户数量相对比较多,时间不集中。 alpha 测试先于 beta 测试执行。通用的软件产品需要较大规模的 beta 测试,测试周期比较长。 测试分类的思维导图下面就用一张图来稍微总结一下，不一定完全准确，就当参考一下吧。 最后软件测试的分类可以从许多角度切入，主要的分类了解了就差不多了，具体的可以针对性的学习。 参考： 软件测试概念及分类整理汇总 软件测试的定义与分类 软件测试分类及测试中三个主要概念","link":"/2018/11/13/2018/11/2018-11-13-TestCategories/"},{"title":"YAML","text":"YAML正如 YAML 所表示的 YAML Ain’t Markup Language，YAML 是一种简洁的非标记语言。YAML 以数据为中心，使用空白，缩进，分行组织数据，从而使得表示更加简洁易读。 编程免不了要写配置文件，YAML 是专门用来写配置文件的语言，非常简洁和强大，远比 JSON 格式方便。 我记得自己第一次真正接触是在建立 github 博客的时候，因为用来配置站点信息的便是使用 Yaml 来配置的。 基本规则YAML 有以下基本规则： 大小写敏感 使用缩进表示层级关系 禁止使用 tab 缩进，只能使用空格键 缩进长度没有限制，只要元素对齐就表示这些元素属于一个层级。 使用#表示注释，从这个字符一直到行尾，都会被解析器忽略。 字符串可以不用引号标注 YAML 支持的数据结构有三种。 对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary） 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 纯量（scalars）：单个的、不可再分的值 三种数据结构Map使用冒号（:）表示键值对，同一缩进的所有键值对属于一个 map，如： 123456# YAML 表示age : 12name : huang# 对应的 Json 表示{'age':12,'name':'huang'} 也可以将一个 map 写在一行： 12345# YAML 表示{age:12,name:huang}# 对应的 Json 表示{'age':12,'name':'huang'} List使用连字符（-）表示： 123456# YAML 表示- a- b- 12# 对应 Json 表示['a','b',12] 也可以写在一行： 1234# YAML 表示[a,b,c]# 对应 Json 表示[ 'a', 'b', 'c' ] map 和 list 的元素可以是另一个 map 或者 list 或者是纯量。 123456789languages: - Ruby - Perl - Pythonwebsites: YAML: yaml.org Ruby: ruby-lang.org Python: python.org Perl: use.perl.org scalar(纯量)数据最小的单位，不可以再分割。 字符串 布尔值 整数 浮点数 Null 时间 日期 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849number: 12.30isSet: true #falseparent: ~ # 对应json中的nulliso8601: 2001-12-14t21:59:43.10-05:00date: 1976-07-31# YAML 允许使用两个感叹号，强制转换数据类型。e: !!str 123f: !!str true# 如果字符串之中包含空格或特殊字符，需要放在引号之中。str: '内容： 字符串'# 单引号和双引号都可以使用，双引号不会对特殊字符转义。s1: '内容\\n字符串's2: &quot;内容\\n字符串&quot;# 单引号之中如果还有单引号，必须连续使用两个单引号转义。str: 'labor''s day'# 字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格。str: 这是一段 多行 字符串# 多行字符串可以使用|保留换行符，也可以使用&gt;折叠换行。this: | Foo Barthat: &gt; Foo Bar# +表示保留文字块末尾的换行，-表示删除字符串末尾的换行。s1: | Foos2: |+ Foos3: |- Foo# 字符串之中可以插入 HTML 标记。message: | &lt;p style=&quot;color: red&quot;&gt; 段落 &lt;/p&gt; 锚点&amp;和别名*，可以用来引用。1234567891011121314151617181920212223242526defaults: &amp;defaults adapter: postgres host: localhostdevelopment: database: myapp_development &lt;&lt;: *defaultstest: database: myapp_test &lt;&lt;: *defaults# 等同于下面的代码。defaults: adapter: postgres host: localhostdevelopment: database: myapp_development adapter: postgres host: localhosttest: database: myapp_test adapter: postgres host: localhost 函数和正则表达式的转换这是 JS-YAML 库特有的功能，可以把函数和正则表达式转为字符串。 123# example.ymlfn: function () { return 1 }reg: /test/ 解析上面的 yml 文件的代码如下。 123456789var yaml = require(&quot;js-yaml&quot;);var fs = require(&quot;fs&quot;);try { var doc = yaml.load(fs.readFileSync(&quot;./example.yml&quot;, &quot;utf8&quot;)); console.log(doc);} catch (e) { console.log(e);} 从 JavaScript 对象还原到 yaml 文件的代码如下。 123456789101112131415var yaml = require(&quot;js-yaml&quot;);var fs = require(&quot;fs&quot;);var obj = { fn: function() { return 1; }, reg: /test/};try { fs.writeFileSync(&quot;./example.yml&quot;, yaml.dump(obj), &quot;utf8&quot;);} catch (e) { console.log(e);} 参考：YAML语言YAML 最最基础语法","link":"/2018/11/15/2018/11/2018-11-15-YAML/"},{"title":"Dokuwiki","text":"Dokuwikidokuwiki 是一个开源 WIKI 引擎程序，运行于 PHP 环境下，程序小巧而功能强大、灵活，适合中小团队和个人网站知识库的管理。 之前在windows下装过，我今天在Linux下装了一下，记录一下大概的过程，给自己参考。 PHPDokuwili是基于 php 做后端处理的，所以我们需要安装并且版本要在 5.6 以上。 如果直接执行下面的命令，CentOS 就会自动帮你安装好，并可以查看安装的位置。 但是因为在线库还是5.5的，所以需要升级。 123456789yum install phprpm -ql php/etc/httpd/conf.d/php.conf/etc/httpd/conf.modules.d/10-php.conf/usr/lib64/httpd/modules/libphp5.so/usr/share/httpd/icons/php.gif/var/lib/php/session 我们首先移除掉老的库链接，换成新的,再执行升级操作，并把相关的库都安装了。 12345678# 查看版本php -v# 升级软件仓库rpm -Uvh https://mirror.webtatic.com/yum/el7/epel-release.rpmrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpmyum remove php-common# 会有一个提示 y/n: 输入 yesyum install -y php56w php56w-opcache php56w-xml php56w-mcrypt php56w-gd php56w-devel php56w-mysql php56w-intl php56w-mbstring Apache Httpd安装这个就比较简单了，而且云机上一般都已经安装好了。 1yum install Httpd 配置 Httpd 与 PHP找到httpd的配置文件，一般在如下的路径： 1/etc/httpd/conf/httpd.conf 修改httpd.conf： 12345AddType application/x-compress .ZAddType application/x-gzip .gz .tgz# 在下面添加如下内容AddType application/x-httpd-php-source .phpsAddType application/x-httpd-php .php Httpd 的其它基本配置这次暂略。 为了安装好 dokuwiki 之后能上传安装一些插件，而不受到上传文件大小限制，就要找到 php.ini然后，改掉。默认才 2M。 12/etc/php.iniupload_max_filesize = 10M 安装 dokuwiki在官网下好包之后，直接放到httpd的目录中，一般默认如下： 1/var/www/html 然后我们解压缩之后，放到 html 目录下，然后给这个目录权限。 1chmod -R 777 /var/www/html/dokuwiki 重启httpd 1httpd -k restart 访问 localhost/dokuwiki/install.php,按照说明安装就好了。 插件与主题dokuwiki 有许多第三方插件与主题，需要自己探索，这里暂略。 最后我今天搭建好的wiki地址：http://47.98.42.142/dokuwiki/ 暂时没有什么东西，就装了一些简单的插件，后续再慢慢丰富。","link":"/2018/11/18/2018/11/2018-11-18-Dokuwiki/"},{"title":"Work Diary (01)","text":"背景昨天有一个重要的测试任务，感觉可以做的更好，有一些点可以稍微总结分析一下，作为参考。 主要任务由于客户遇到了相对严重的问题（在极端情况下，客户及本地无法重现），这次需要修复（根据分析 Log 与代码，得到修复方案），并根据客户需求增加了 log 的输出。 可测版本在昨天中午准备就绪，大约 1 点钟左右开始正式测试。 主要测试点为： 冒烟及回归测试，保证新的版本没有影响原来的功能。（包括将一些参数设长，如 timeout 的时间，保证没有意外错误） 通过特殊代码，主动触发 bug，检测能否 catch 到，保证客户方面再出现这方面的情况，不会再出来重大错误。 通过设置特殊参数，使系统容易触发异常，并测试在参数正常情况下，重置服务后能否修复并继续执行任务。 etc… 测试新增的 Log 功能 测试新的改动尤其是 Log 功能对性能的影响 测试过程回顾与分析首先第一点，自己没做好的是没有制定详细的测试计划。 虽然在小本本上有简单罗列测试要点，但没有完整的 Test Cases 列表，导致后续测试工作“无法可依”，尤其是在遇到问题的情况下，容易思路混淆。 至少提前一天已经知道大概的测试内容，但还是小看了相关测试的复杂度与可能遇到的问题。 至下午 5 点左右，除 Log 相关功能外，其它测试点基本完成。 出不来的 Log当时把 Log 功能放到最后，是因为与开发人员沟通过，觉得功能实现简单，相对容易测试，所以认为优先级最低（结果证明出问题的就是这边，尴尬）。 开始测试 Log 功能之后，便一直发现不能得到预期的结果，开始怀疑功能有问题，开始与开发人员一同查看。 这边就涉及到一个问题，就是开发人员一直认为是配置与运行使用的问题，所以一直在尝试重启，配置，运行的过程，所以耗费了不少时间。 主要是我本身对这个功能模块不是很熟悉，所以只能不断根据开发人员的建议配置与重试，但仍然无法成功，此时已将近 7 点左右。 这次的增加的 Log 功能是基于原先已有的模块，增加了输出的内容，原计划是在 Debug 级别配置。 最后，向原先熟悉相关模块的开发人员寻求帮忙与确认后，找到了问题的症结，Debug 级别只有在 Debug 版本能会输出想到的内容，在 Release 版本是不会输出的，这属于 C++的坑。 消失的进程在明确问题后，开发人员改过代码，special build 了一个版本给我测试，能够看到预期的 Log 内容, 本来还是很开心的。 但是当这个问题解决后，我们的目光又聚集到另外一个问题: xxxService 的进程数目不对，并一直在创建与销毁。 在我们在系统中，需要配置 xxxService 的数量，当服务整个起来之后，可以在 task manager 那边看到相应数目的进程。这些进程会提供一定的服务，并且上文提到的 Log 便是它们来输出的。 当时的现象是： Log 能够输出，但是文件数量一直在增加(正常情况是文件数量应与进程数一致，并在短期内不会有新建与销毁) 在任务管理器可以看出，设置了 15 个数，但实际上只有 3 个，并且偶尔能看到新增几个新的，但立马消失。 不管 Log 打开还是关闭，都有同样的问题。 在其它相关联的服务 log 中，有大量的与 xxxService Time Out 的 error（侧面证明了一直在销毁与新建） 不影响正常的业务处理 etc… 其实在下午的测试过程中，就有感觉一些不对劲，但当时关注点不在这上面，而且一直是在做破坏性测试，所以一直有一些 error 都没太在意。尤其是这个问题不影响正常的业务处理。 当所有功能没有问题之后，目光就集中在这上边，开始意识到问题的严重性，带着这样的问题，产品肯定是不能发布的。 所以一方面，开发人员在查找原因，而我开始对比测试。 原因到底是什么开发人员的反馈是最新加的功能不会有大的问题，逻辑很简单，而最近两天基本上只有这次的改动。很可能是别的问题，所以一直在查找。 而我这边，首先确认了下上一个 release 版本是没有这样的问题的，然后也尝试把 log disable 掉，没有输出，但在新版本还是有这样的问题。潜意识里觉得新加的 Log 功能应该是没有问题了，这时，又没有了头绪，而时间已经将近 10 点。 最后，开发人员有了另一种假设，可能是编译的问题，导致 exe 自己出错死掉（但其实不能解释为什么还坚挺地活着几个）。并且发现当前版本前一天的 build 是可以的，便更加相信是其它因素。 我第一次听说可能会有这样的问题（build 不对），基本上那时便没有更深入的去定位出问题的范围。 等到近事出反常必有妖，新的 full build 版本出来后，我尝试了下，还是不行，便陷入的僵局。 柳暗花明由于时间太晚，便只能暂时做罢（哈哈哈哈哈，要是某些公司，肯定等不及第二天再来看的） 但事出反常必有妖，我一直觉得肯定是代码哪里有问题，但我心里也把 Log 这边排除了（毕竟逻辑相对简单，只是多增加输出，更有两位开发人员的肯定）。更觉得是其它的问题。 早上来了之后，便与 David 一起 review，我把现象展示出来，在 David 的提示下，我们将 error log 打开（不确定昨晚是否一样打开过）。重现问题之后，猛然在几个进程的 log 里看到了一段 exception,大意是 map 的 key 不对，找不到。 这回我看到了代码，确实在新的更改那边有取 map 值的操作，但理论上来说，从 map 中取值最多为空 null 之类，不应该有重大 error. 在简单分析之后，还是重新修改编译了一个版本，我尝试之后，问题果然解决了。 根源是在为了取一个值(user)的时候，用的是已有的一个公共方法（方式)，而在当前使用情况下，会诱发问题，更详细具体的根源我没有深究。 反思总之，如果做的更好一点，就能更早的定位问题，让代价更小一些。 要制定相对详细的测试计划，辅助整个流程的进行。 优先测试目标明确的case，比如这次的log，如果一开始就发现出不来，就可以给回开发人员研究，自己做其它的测试，更高效一些。 一定要按自己的思路去定位问题，在有限的条件下，缩小问题的范围，确定好的与有问题的版本的界线。 不要相信开发人员，哈哈哈哈哈，revert 大法好，在实现不能确定问题根源情况下，回退代码测试是最有效的。尤其是像这次代码 commit 很少的情况下，代价相对很低。 晚安","link":"/2018/11/21/2018/11/2018-11-21-WorkDiary-01/"},{"title":"Design Patterns(Proxy)","text":"前言早些年看《大话设计模式》的时候，用Java实现了书中的例子，可惜当年没有保存好，好多东西都丢了。 现在想着用python重新实现一下 ，重新学习。 虽然 python 没有接口的概念，但是可以用抽象类代替。 代理模式代理模式是给某一个对象提供一个代理对象，并由代理对象控制对原对象的引用。通俗的来讲代理模式就是我们生活中常见的中介。 为什么要用代理模式？ 中介隔离作用： 在某些情况下，一个客户类不想或者不能直接引用一个委托对象，而代理类对象可以在客户类和委托对象之间起到中介的作用，其特征是代理类和委托类实现相同的接口。 开闭原则，增加功能： 代理类除了是客户类和委托类的中介之外，我们还可以通过给代理类增加额外的功能来扩展委托类的功能，这样做我们只需要修改代理类而不需要再修改委托类，符合代码设计的开闭原则。 代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后对返回结果的处理等。代理类本身并不真正实现服务，而是同过调用委托类的相关方法，来提供特定的服务。 真正的业务功能还是由委托类来实现，但是可以在业务功能执行的前后加入一些公共的服务。例如我们想给项目加入缓存、日志这些功能，我们就可以使用代理类来完成，而没必要打开已经封装好的委托类。 有哪几种代理模式？ 我们有多种不同的方式来实现代理。如果按照代理创建的时期来进行分类的话， 可以分为两种：静态代理、动态代理。 静态代理静态代理是由程序员创建或特定工具自动生成源代码，在对其编译。 下面这个是以购买房子为例子： 1234567891011121314151617181920212223242526272829303132333435from abc import ABCMeta,abstractmethodclass IBuyHouseMan(metaclass=ABCMeta): @abstractmethod def pay_money(self, money): ''' :param money: :return: ''' raise NotImplementedErrorclass BuyHouseManImpl(IBuyHouseMan): def pay_money(self, money): print(&quot;pay money({}) for house!&quot;.format(money))class BuyHouseManProxy(IBuyHouseMan): def __init__(self, buy_house_man): self.buy_house_man = buy_house_man def pay_money(self, money): print(&quot;prepare contract&quot;) self.buy_house_man.pay_money(money) print(&quot;get commission &quot;)def test(): buy_house_man = BuyHouseManImpl() buy_house_man.pay_money(1000) buy_house_proxy = BuyHouseManProxy(buy_house_man) buy_house_proxy.pay_money(1000)if __name__ == &quot;__main__&quot;: test() 动态代理动态代理是在程序运行时通过反射机制动态创建的，耦合性大大降低，更加灵活。 我对 python 的反射还不太熟悉，下面是网上的例子，感觉他写的有点麻烦和绕, 到时我重新写下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from abc import ABCMeta,abstractmethodimport typesclass InvocationHandler: def __init__(self, obj, func): self.obj = obj self.func = func def __call__(self, *args, **kwargs): print('handler:', self.func, args, kwargs) print('before call the func.') self.func(*args, **kwargs) print('after call the func.')class HandlerException(Exception): def __init__(self, cls): super(HandlerException, self).__init__(cls, 'is not a hanlder class')class Proxy: def __init__(self, cls, hcls): self.cls = cls self.hcls = hcls self.handlers = dict() def __call__(self, *args, **kwargs): self.obj = self.cls(*args, **kwargs) return self def __getattr__(self, attr): print('get attr:', attr) isExist = hasattr(self.obj, attr) res = None if isExist: res = getattr(self.obj, attr) if isinstance(res, types.MethodType): if self.handlers.get(res) is None: self.handlers[res] = self.hcls(self.obj, res) return self.handlers[res] else: return res return resclass ProxyFactory: def __init__(self, hcls): if issubclass(hcls, InvocationHandler) or hcls is InvocationHandler: self.hcls = hcls else: raise HandlerException(hcls) def __call__(self, cls): return Proxy(cls, self.hcls)@ProxyFactory(InvocationHandler)class BuyHouseMan: def __init__(self, money): self.money = money def pay_money(self): print(&quot;pay money({}) for house!&quot;.format(self.money))def test(): buy_house_man = BuyHouseMan(1000) print(type(buy_house_man)) buy_house_man.pay_money()if __name__ == &quot;__main__&quot;: test() 如果使用 Java 的话，那么就很简单了，reflect类库中就有现成的InvocationHandler，很方便： 123456789101112131415161718192021package main.java.proxy.impl;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;public class DynamicProxyHandler implements InvocationHandler { private Object object; public DynamicProxyHandler(final Object object) { this.object = object; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;before invoke&quot;); Object result = method.invoke(object, args); System.out.println(&quot;after invoke&quot;); return result; }} 测试类： 12345678910111213141516package main.java.proxy.test;import main.java.proxy.BuyHouse;import main.java.proxy.impl.BuyHouseImpl;import main.java.proxy.impl.DynamicProxyHandler;import java.lang.reflect.Proxy;public class DynamicProxyTest { public static void main(String[] args) { BuyHouse buyHouse = new BuyHouseImpl(); BuyHouse proxyBuyHouse = (BuyHouse) Proxy.newProxyInstance(BuyHouse.class.getClassLoader(), new Class[]{BuyHouse.class}, new DynamicProxyHandler(buyHouse)); proxyBuyHouse.buyHosue(); }} CGLIB代理待填坑。 参考: 设计模式—代理模式 Python：动态代理机制","link":"/2018/11/19/2018/11/2018-11-19-DesignPattern-Proxy/"},{"title":"Swagger","text":"背景项目组最近新开发一套 RESTful API，需要对应的 API 文档，其实之前我有用 apidoc 作为方案给大家看过。 感觉还好，也是可以直接写成注释放在代码里最后生成。但能感受到它属于轻量级的，内容相对自由，灵活度很大，但也意味着更容易犯错。 不过最后上面还是决定使用 swagger 这一套东西，所以简单的看了下，功能果然成强大，显得更 professionnal 一些。 下面就罗列下我目前掌握的一些信息，其实学习 swagger 的过程就是学习 RESTful API 的过程。 SwaggerSwagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新 。 接口的方法，参数和模型紧密集成到服务器端的代码，允许 API 来始终保持同步。 Swagger 让部署管理和使用功能强大的 API 从未如此简单。 Swagger UI我们先来看一看最后文档的样子，下面是官方的 Live Demo， 大家可以感受一下。 Swagger UI 主要用来通过读取对应的 YAML/JSON 文件，来解析并展示 “漂亮”的 api 文档，效果还是不错的，相当于前端展示功能。 它的 github 地址https://github.com/swagger-api/swagger-ui 可以下载之后，在本地放在 http server 下直接使用，当然也可以二次开发，具体内容请看官方的 ReadMe。 Swagger Editor那么另外一个项目https://github.com/swagger-api/swagger-editor 便是用来编写文档的。 它采用的是 YAML 语法，所以可以丰富的表达许多内容。下面是官方的Live Demo，其实很多细节内容在这边就可以学习到，它的样例就已经很丰富。 Editor 也可以下载来到本地使用，但是需要 Node.js的支持，具体请参考官方 ReadMe。 代码中的使用通过一定的配置（需要继续研究），可以通过注解的方式(主要以 java 为例)，可以生成 api 文档，甚至可以直接调试。 作用范围 API 使用位置 对象属性 @ApiModelProperty 用在出入参数对象的字段上 协议集描述 @Api 用于 controller 类上 协议描述 @ApiOperation 用在 controller 的方法上 Response 集 @ApiResponses 用在 controller 的方法上 Response @ApiResponse 用在 @ApiResponses 里边 非对象参数集 @ApiImplicitParams 用在 controller 的方法上 非对象参数描述 @ApiImplicitParam 用在@ApiImplicitParams 的方法里边 描述返回对象的意义 @ApiModel 用在返回对象类上 Samples1234567@ApiOperation(&quot;信息软删除&quot;)@ApiResponses({ @ApiResponse(code = CommonStatus.OK, message = &quot;操作成功&quot;), @ApiResponse(code = CommonStatus.EXCEPTION, message = &quot;服务器内部异常&quot;), @ApiResponse(code = CommonStatus.FORBIDDEN, message = &quot;权限不足&quot;) })@ApiImplicitParams({ @ApiImplicitParam(paramType = &quot;query&quot;, dataType = &quot;Long&quot;, name = &quot;id&quot;, value = &quot;信息id&quot;, required = true) })@RequestMapping(value = &quot;/remove.json&quot;, method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_UTF8_VALUE)public RestfulProtocol remove(Long id) { 12@ApiModelProperty(value = &quot;标题&quot;) private String title; @ApiImplicitParam 属性 取值 作用 paramType 查询参数类型 path 以地址的形式提交数据 query 直接跟参数完成自动映射赋值 body 以流的形式提交 仅支持 POST header 参数在 request headers 里边提交 form 以 form 表单的形式提交 仅支持 POST dataType 参数的数据类型 只作为标志说明，并没有实际验证 Long String name 接收参数名 value 接收参数的意义描述 required 参数是否必填 true 必填 false 非必填 defaultValue 默认值 paramType 示例path12@RequestMapping(value = &quot;/findById1/{id}&quot;, method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_UTF8_VALUE)@PathVariable(name = &quot;id&quot;) Long id body123@ApiImplicitParams({ @ApiImplicitParam(paramType = &quot;body&quot;, dataType = &quot;MessageParam&quot;, name = &quot;param&quot;, value = &quot;信息参数&quot;, required = true) })@RequestMapping(value = &quot;/findById3&quot;, method = RequestMethod.POST, produces = MediaType.APPLICATION_JSON_UTF8_VALUE, consumes = MediaType.APPLICATION_JSON_VALUE)@RequestBody MessageParam param 提交的参数是这个对象的一个 json，然后会自动解析到对应的字段上去，也可以通过流的形式接收当前的请求数据，但是这个和上面的接收方式仅能使用一个（用@RequestBody 之后流就会关闭了） header123456@ApiImplicitParams({ @ApiImplicitParam(paramType = &quot;header&quot;, dataType = &quot;Long&quot;, name = &quot;id&quot;, value = &quot;信息id&quot;, required = true) })String idstr = request.getHeader(&quot;id&quot;); if (StringUtils.isNumeric(idstr)) { id = Long.parseLong(idstr); } Form12@ApiImplicitParams({ @ApiImplicitParam(paramType = &quot;form&quot;, dataType = &quot;Long&quot;, name = &quot;id&quot;, value = &quot;信息id&quot;, required = true) })@RequestMapping(value = &quot;/findById5&quot;, method = RequestMethod.POST, produces = MediaType.APPLICATION_JSON_UTF8_VAL SpringMVC 集成 Swagger 插件以及 Swagger 注解的简单使用我发现一篇不错的文章，有时间试一下。 SpringMVC 集成 Swagger 插件以及 Swagger 注解的简单使用 未完待续，保持更新。 参考： swagger注释API详细说明 https://swagger.io/","link":"/2018/11/25/2018/11/2018-11-22-Swagger/"},{"title":"Python gRPC","text":"RPCRPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC 协议假定某些传输协议的存在，如 TCP 或 UDP，为通信程序之间携带信息数据。在 OSI 网络通信模型中，RPC 跨越了传输层和应用层。RPC 使得开发包括网络分布式多程序在内的应用程序更加容易。 RPC 架构先说说 RPC 服务的基本架构。一个完整的 RPC 架构里面包含了四个核心的组件，分别是 Client ,Server,Client Stub 以及 Server Stub，这个 Stub 可以理解为存根。分别说说这几个组件： 客户端（Client），服务的调用方。 服务端（Server），真正的服务提供者。 客户端存根，存放服务端的地址消息，再将客户端的请求参数打包成网络消息，然后通过网络远程发送给服务方。 服务端存根，接收客户端发送过来的消息，将消息解包，并调用本地的方法。 RPC 主要是用在大型企业里面，因为大型企业里面系统繁多，业务线复杂，而且效率优势非常重要的一块，这个时候 RPC 的优势就比较明显了。 项目一般使用 maven 来管理。比如一个处理订单的系统服务，先声明它的所有的接口（i.e. Java 中的 interface），然后将整个项目打包为一个 jar 包，服务端这边引入这个二方库，然后实现相应的功能，客户端这边也只需要引入这个二方库即可调用了。 为什么这么做？主要是为了减少客户端这边的 jar 包大小，因为每一次打包发布的时候，jar 包太多总是会影响效率。另外也是将客户端和服务端解耦，提高代码的可移植性。 HTTP and RPC理论上来说 RPC 能实现的功能， 用 HTTP 也能实现，那它们的区别是什么？什么情况下使用 RPC？ 他们最本质的区别是 RPC 主要是基于 TCP/IP 协议的，而 HTTP 服务主要是基于 HTTP 协议的，我们都知道 HTTP 协议是在传输层协议 TCP 之上的，所以效率上来看的话，RPC 是更优的。 OSI 网络七层模型回顾一下 OSI 的七层网络结构模型（实际应用中基本上都是五层），它可以分为以下几层： （从上到下） 第一层：应用层。定义了用于在网络中进行通信和传输数据的接口； 第二层：表示层。定义不同的系统中数据的传输格式，编码和解码规范等； 第三层：会话层。管理用户的会话，控制用户间逻辑连接的建立和中断； 第四层：传输层。管理着网络中的端到端的数据传输； 第五层：网络层。定义网络设备间如何传输数据； 第六层：链路层。将上面的网络层的数据包封装成数据帧，便于物理层传输； 第七层：物理层。这一层主要就是传输这些二进制数据。实际应用过程中，五层协议结构里面是没有表示层和会话层的。应该说它们和应用层合并了。我们应该将重点放在应用层和传输层这两个层面。因为 HTTP 是应用层协议，而 TCP 是传输层协议。知道了网络的分层模型以后我们可以更好地理解为什么 RPC 服务相比 HTTP 服务要好一些！ RPC 流行框架Google 开源了 gRPC，Facebook 开源了 Thrift，Twitter 开源了 Finagle，百度开源了 bRPC，腾讯开源了 Tars，阿里开源了 Dubbo 和 HSF，新浪开源了 Motan 等，这些互联网公司在解决分布式高并发业务问题的同时，也向外界展示自己的技术实力。 gRPC 是 Google 公布的开源软件，基于最新的 HTTP2.0 协议，并支持常见的众多编程语言。 我们知道 HTTP2.0 是基于二进制的 HTTP 协议升级版本，目前各大浏览器都在快马加鞭的加以支持。 这个 RPC 框架是基于 HTTP 协议实现的，底层使用到了 Netty 框架的支持。 Thrift 是 Facebook 的一个开源项目，主要是一个跨语言的服务开发框架。它有一个代码生成器来对它所定义的 IDL 定义文件自动生成服务代码框架。用户只要在其之前进行二次开发就行，对于底层的 RPC 通讯等都是透明的。不过这个对于用户来说的话需要学习特定领域语言这个特性，还是有一定成本的。 HSF 全称为 High-Speed Service Framework，旨在为淘系的应用提供一个分布式的服务框架，HSF 从分布式应用层面以及统一的发布/调用方式层面为大家提供支持，从而可以很容易的开发分布式的应用以及提供或使用公用功能模块，而不用考虑分布式领域中的各种细节技术，例如远程通讯、性能损耗、调用的透明化、同步/异步调用方式的实现等等问题。 gRPC Demo on PythonInstall gRPC python lib可以直接通过 pip 来安装相关的第三方库： 123pip install grpciopip install protobufpip install grpcio-tools Define gRPC interface编写接口文件 data.proto： 12345678syntax = &quot;proto3&quot;;package demo;service AddNumber { rpc do_add(data) returns (data){}}message data { string text = 2;} 如上所示，设计了 service AddNumber，调用的方法为 do_add，参数为 data，内容为字符串格式。 Build protobuf:编译 protobuf， 然后会生成两个文件 data_pb2.py 和 data_pb2_grpc.py： 1python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. ./data.proto Server主要为注册服务，实现业务逻辑和返回的数据，这里就调用了上一步编译出来的两个 modules 1234567891011121314151617181920212223242526272829import grpcimport timeimport data_pb2, data_pb2_grpcfrom concurrent import futuresONE_DAY_IN_SECONDS = 3600 * 24HOST = 'localhost'PORT = '8888'class AddNumber(data_pb2_grpc.AddNumberServicer): def do_add(self, request, context): txt = request.text txt = str(int(txt) + 1) return data_pb2.data(text = txt)def start_server(): grpcServer = grpc.server(futures.ThreadPoolExecutor(max_workers=4)) # 这里将AddNumber这个类的实例注册进了服务 data_pb2_grpc.add_AddNumberServicer_to_server(AddNumber(), grpcServer) grpcServer.add_insecure_port(HOST + ':' + PORT) grpcServer.start() try: while True: time.sleep(ONE_DAY_IN_SECONDS) except KeyboardInterrupt: grpcServer.stop(0)if __name__ == '__main__': start_server() Client这里对应编写了主机和端口，然后使用 AddNumberStub 服务，调用 do_add 方法。 在服务端那么便会执行对应的操作。 1234567891011121314import grpcimport data_pb2, data_pb2_grpcHOST = 'localhost'PORT = '8888'def run(): conn = grpc.insecure_channel(HOST + ':' + PORT) client = data_pb2_grpc.AddNumberStub(channel=conn) response = client.do_add(data_pb2.data(text='3')) print(&quot;received: &quot; + response.text)if __name__ == '__main__': run() 下面是运行 client 的结果： 传过去的是 3，通过+1 最后得到的是 4 12PS C:\\Users\\mayn\\Desktop\\gRPC_Demo&gt; python client.pyreceived: 4 最后RPC 实现的细节还有很多，包括序列化与反序列化，动态代理，网络通信NIO等，有机会再深入研究下。 参考： https://github.com/bearfly1990/PowerScript/tree/master/RPC/py_gRPC_Demo 深入理解 RPC : 基于 Python 自建分布式高并发 RPC 服务 Python RPC 之 gRPC - 谢烟客 RPC 服务和 HTTP 服务对比 - wangyunpeng0319 HSF 的原理分析 - zxcodestudy","link":"/2018/11/26/2018/11/2018-11-26-PygRPC/"},{"title":"Swagger in SpringBoot","text":"背景前两天提到项目组打算用 swagger 来做为 RESTful API 的解决方案，现在已经确定下来了。 之前看到有文章介绍 SpringBoot 中结合 Swagger 直接生成文档的方法，今天尝试了一下，入门还是比较简单的。 建立 Maven 工程我们使用 Maven 管理 jar 包依赖，网上有许多的教程，就不赘述。今天在配置环境的时候，看到有一个阿里云的镜像，可以记录一下： 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt; 引入相关依赖完整的配置请看pom.xml SpringBoot 相关123456789101112131415161718192021222324252627282930313233&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Swagger 相关依赖1234567891011&lt;!-- swagger2 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt; 热启动通过下面的片段来开启了热启动，代码更新后自动重启，方便很多。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 1234567&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;/configuration&gt;&lt;/plugin&gt; 创建 Swagger2 配置类在 SpringBoot 主类同级目录下新建 Swagger 的配置类如下： 12345678910111213141516171819202122232425@Configuration@EnableSwagger2public class Swagger2 { @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(&quot;fun.bearfly&quot;)) .paths(PathSelectors.any()) .build(); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(&quot;Generate RESTful APIs with Spring Boot and Swagger2 &quot;) .description(&quot;More info please see:https://bearfly1990.github.io/&quot;) .termsOfServiceUrl(&quot;https://bearfly1990.github.io/&quot;) .contact(&quot;bearfly1990&quot;) .version(&quot;1.0&quot;) .build(); }} Controller 中的注解接口文档的信息通过注解来写入，所以就放在 Controller 这一层。 下面的代码主要模拟了对单词的 CRUD 操作，每个接口都有对应的接口注释，通过@ApiOperation， @ApiImplicitParam 等注解来实现，这个我之前的文章有提到：Swagger#代码中的使用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@RestController@RequestMapping(value=&quot;/ewords&quot;)public class EWordController { static Map&lt;Long, EWord&gt; ewordsMap = Collections.synchronizedMap(new HashMap&lt;Long, EWord&gt;()); @ApiOperation(value=&quot;HelloWorld&quot;, notes=&quot;&quot;) @RequestMapping(value=&quot;helloworld&quot;, method=RequestMethod.GET) public String sayHello() { return &quot;hello, world!&quot;; } @ApiOperation(value=&quot;GetEword&quot;, notes=&quot;&quot;) @RequestMapping(value={&quot;&quot;}, method=RequestMethod.GET) public List&lt;EWord&gt; getEWordList() { List&lt;EWord&gt; ewords = new ArrayList&lt;EWord&gt;(ewordsMap.values()); return ewords; } @ApiOperation(value=&quot;CreateEword&quot;, notes=&quot;Create Eword by Eword&quot;) @ApiImplicitParam(name = &quot;eword&quot;, value = &quot;user domain&quot;, required = true, dataType = &quot;EWord&quot;) @RequestMapping(value=&quot;&quot;, method=RequestMethod.POST) public String postEWord(@RequestBody EWord eword) { ewordsMap.put(eword.getId(), eword); return &quot;success&quot;; } @ApiOperation(value=&quot;GetEwordDetails&quot;, notes=&quot;Get eword by id&quot;) @ApiImplicitParam(name = &quot;id&quot;, value = &quot;ewordID&quot;, required = true, dataType = &quot;Long&quot;) @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.GET) public EWord getEWord(@PathVariable Long id) { return ewordsMap.get(id); } @ApiOperation(value=&quot;UpdateEwordInfo&quot;, notes=&quot;Update eword by id&quot;) @ApiImplicitParams({ @ApiImplicitParam(name = &quot;id&quot;, value = &quot;ewordID&quot;, required = true, dataType = &quot;Long&quot;), @ApiImplicitParam(name = &quot;user&quot;, value = &quot;ewordDomain&quot;, required = true, dataType = &quot;EWord&quot;) }) @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.PUT) public String putEWord(@PathVariable Long id, @RequestBody EWord newUser) { EWord eword = ewordsMap.get(id); eword.setCnWord(newUser.getCnWord()); eword.setEnWord(newUser.getEnWord()); ewordsMap.put(id, eword); return &quot;success&quot;; } @ApiOperation(value=&quot;DeleteEword&quot;, notes=&quot;Delete eword by id&quot;) @ApiImplicitParam(name = &quot;id&quot;, value = &quot;ewordID&quot;, required = true, dataType = &quot;Long&quot;) @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.DELETE) public String deleteEWord(@PathVariable Long id) { ewordsMap.remove(id); return &quot;success&quot;; }} 生成文档上面的配置与代码完成后，直接启动 SpringBoot，访问：http://localhost:8080/swagger-ui.html 就能查看文档了。 搞定~ 工程目录：https://github.com/bearfly1990/PowerScript/tree/master/Java/workspace/swagger 参考： 深入理解 RPC : 基于 Python 自建分布式高并发 RPC 服务 - 程序猿 DD 构建微服务：Spring boot 入门篇 - 微信公众号：纯洁的微笑","link":"/2018/11/27/2018/11/2018-11-27-SwaggerSpringBoot/"},{"title":"String Pattern","text":"背景今天遇到的一个算法问题，感觉又回到了当年做 ACM 的时候，不过好久没动脑，不好使了，哈哈哈。 问题描述需求简单来说是给你两个字符串，一个是需要被匹配的，另一个是用来匹配的模式，我用接口定义了一下： 12345678910111213141516171819202122/*** @Description: Algorithms samples interface.* @author bearfly1990* @date Nov 30, 2018 9:24:26 PM*/package org.bearfly.fun.javalearn.algorithms;public interface IAlgorithms { /** * &lt;p&gt;This is the definition to check input string could match pattern or not&lt;p&gt; * &lt;p&gt;example:&lt;/p&gt; * &lt;p&gt;matched: input = &quot;java hello world&quot;, pattern = &quot;ABC&quot;&lt;/p&gt; * &lt;p&gt;matched: input = &quot;java hello java hello&quot;, pattern = &quot;ABAB&quot;&lt;/p&gt; * &lt;p&gt;unmatch: input = &quot;java hello java&quot;, pattern = &quot;ABC&quot;&lt;/p&gt; * &lt;p&gt;unmatch: input = &quot;java hello&quot;, pattern = &quot;ABC&quot;&lt;/p&gt; * &lt;p&gt;...&lt;/p&gt; * @param input * @param pattern * @return boolean inputStr could match patter or not */ public boolean matchPattern(String input, String pattern);} 具体实现定义辅助方法预计输入参数大概是(&quot;hello world java&quot;, &quot;ABC&quot;)，针对这个，我们需要把他们都先转换到数组或者 List 中，所以我写一个方法去把”ABC”拆成[“A”,”B”, “C”]。 12345678910111213141516/*** @Description: StringUtils* @author bearfly1990* @date Nov 30, 2018 10:16:58 PM*/package org.bearfly.fun.javalearn.utils;public class StringUtils { public static String[] arrayCharToStr(char[] charArray) { String[] newStrArray = new String[charArray.length]; for(int i = 0; i &lt; charArray.length; i++) { newStrArray[i] = String.valueOf(charArray[i]); } return newStrArray; }} 主要逻辑代码我已经在注释中写的比较详细了，就不再分析，直接看代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/*** @Description: Algorithms Implements* @author bearfly1990* @date Nov 30, 2018 9:42:29 PM*/package org.bearfly.fun.javalearn.algorithms.impl;import java.util.HashMap;import java.util.Map;import org.bearfly.fun.javalearn.algorithms.IAlgorithms;import org.bearfly.fun.javalearn.utils.StringUtils;public class AlgorithmsImpl implements IAlgorithms { @Override public boolean matchPattern(String input, String pattern) { // [&quot;hello&quot;, &quot;world&quot;, &quot;java&quot;] String[] wordArray = input.split(&quot; &quot;); // [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;] String[] patternArray = StringUtils.arrayCharToStr(pattern.toCharArray()); // if pattern and string num is not matched, no compare any more. // e.g. hello world java ！= AB if (wordArray.length != patternArray.length) { return false; } Map&lt;String, String&gt; matchedMap = new HashMap&lt;&gt;(); for (int i = 0; i &lt; patternArray.length; i++) { String patternEntry = patternArray[i]; String wordEntry = wordArray[i]; if (matchedMap.containsKey(patternEntry)) { if (!matchedMap.get(patternEntry).equals(wordEntry)) { // it means the patternEntry have maped a word but current word is not expected. return false; } } else if (matchedMap.containsValue(wordEntry)) { // it means the current patternEntry match a word belong to other patternEntry. // so that's not ok. return false; } else { // the first time to map it. matchedMap.put(patternEntry, wordEntry); } } return true; }} 其实除了上面的方法，还有另一个办法，但本质上还是一样的，测试也通过了： 12345678910111213141516171819202122232425262728293031323334public boolean matchPattern2(String input, String pattern) { // [&quot;hello&quot;, &quot;world&quot;, &quot;java&quot;] String[] wordArray = input.split(&quot; &quot;); // [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;] String[] patternArray = StringUtils.arrayCharToStr(pattern.toCharArray()); // if pattern and string num is not matched, no compare any more. // e.g. hello world java ！= AB if (wordArray.length != patternArray.length) { return false; } for (int i = 0; i &lt; patternArray.length; i++) { String patternEntry = patternArray[i]; String wordEntry = wordArray[i]; if (wordEntry.startsWith(&quot;**&quot;)) { if (!wordEntry.equals(patternEntry)) { return false; } } else if (patternEntry.startsWith(&quot;**&quot;)) { return false; } for (int j = i; j &lt; patternArray.length; j++) { if (wordArray[j].equals(wordEntry)) { wordArray[j] = &quot;**&quot; + patternEntry; } if (patternArray[j].equals(patternEntry)) { patternArray[j] = &quot;**&quot; + patternEntry; } } } return true;} 测试代码写了简单的单元测试，主要的 case 应该都包含了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/*** @Description: Algorithms Test Class* @author bearfly1990* @date Nov 30, 2018 9:42:29 PM*/package org.bearfly.fun.javalearn.algorithms;import static org.junit.Assert.assertEquals;import org.bearfly.fun.javalearn.algorithms.impl.AlgorithmsImpl;import org.junit.Test;public class AlgorithmsTest { @Test public void testStringMatch() { IAlgorithms as = new AlgorithmsImpl(); String input = &quot;java hello world&quot;; String pattern = &quot;ABC&quot;; assertEquals(true, as.matchPattern(input, pattern)); input = &quot;hello hello hello&quot;; pattern = &quot;AAA&quot;; assertEquals(true, as.matchPattern(input, pattern)); input = &quot;hello java hello java good&quot;; pattern = &quot;ABABC&quot;; assertEquals(true, as.matchPattern(input, pattern)); input = &quot;hello java hello java good&quot;; pattern = &quot;ABABD&quot;; assertEquals(true, as.matchPattern(input, pattern)); input = &quot;hello java hello java good&quot;; pattern = &quot;ABABA&quot;; assertEquals(false, as.matchPattern(input, pattern)); input = &quot;java hello hello&quot;; pattern = &quot;ABC&quot;; assertEquals(false, as.matchPattern(input, pattern)); input = &quot;java hello hello&quot;; pattern = &quot;AB&quot;; assertEquals(false, as.matchPattern(input, pattern)); }} 最后思路理顺了之后实现还是很简单的，但是没有一个实际操作编码的过程，只在脑子里构思还是不容易的，主要还是一时没有反应过来，或者想的太复杂的时候，哈哈哈。 有空可以再去撸几道 ZJU 的 Online Judge 了（捂脸） 代码我上传到了 github：https://github.com/bearfly1990/PowerScript/tree/master/Java/workspace/javalearn","link":"/2018/11/30/2018/11/2018-11-30-StringPattern/"},{"title":"Python Windows Service","text":"Background最近在优化LoadTest的流程，在执行测试的时候，我们需要在10台虚拟机上运行HostProxy, 这意味着需要登陆这10台机子并且一个个运行代理程序，这简直要人命。 所以我第一个想到的就是把这个过程做成一个windows service，让他们一直监控一个文件命令，当下达运行指令的时候，10台机子就会自动运行，同理也能停止。 虽然最后发现通过service启的进程不能被Host Manager访问到（有时间再仔细研究，应该有解决方案），用Scheduler替代了，但其它操作还是没有问题的。 pywin32如果想要编写windows service，就要用到pywin32这个库，可以从网上下到，我记得好像不能直接使用pip安装。 Code下面便是一个简单的例子，在服务启动之后，便会每5秒在文件中写数据。 主要我们自己的业务逻辑便是写在SvcDoRun这个函数中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import win32serviceutilimport win32serviceimport win32eventimport servicemanagerimport socketimport os, timeclass PySvc(win32serviceutil.ServiceFramework): # you can NET START/STOP the service by the following name _svc_name_ = &quot;PythonTestService&quot; # this text shows up as the service name in the Service # Control Manager (SCM) _svc_display_name_ = &quot;Python Test Service&quot; # this text shows up as the description in the SCM _svc_description_ = &quot;This service writes stuff to a file&quot; def __init__(self, args): win32serviceutil.ServiceFramework.__init__(self,args) # create an event to listen for stop requests on self.hWaitStop = win32event.CreateEvent(None, 0, 0, None) # core logic of the service def SvcDoRun(self): # import servicemanager f = open('c:\\\\test.txt', 'w+') rc = None # if the stop event hasn't been fired keep looping while rc != win32event.WAIT_OBJECT_0: f.write('TEST DATA\\n') f.flush() # block for 5 seconds and listen for a stop event rc = win32event.WaitForSingleObject(self.hWaitStop, 5000) f.write('Service is stopped.\\n') f.close() # called when we're being shut down def SvcStop(self): # tell the SCM we're shutting down self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING) # fire the stop event win32event.SetEvent(self.hWaitStop)if __name__ == '__main__': win32serviceutil.HandleCommandLine(PySvc) Usage在编写好脚本之后，比如上面的代码假设文件名为PythonService.py,那么就可以用以下的方式来使用。 当然，在第一部安装之后，可以直接在service面板操作，不一定需要用脚本。 12345678910111213#1.安装服务python PythonService.py install#2.让服务自动启动python PythonService.py --startup auto install #3.启动服务python PythonService.py start#4.重启服务python PythonService.py restart#5.停止服务python PythonService.py stop#6.删除/卸载服务python PythonService.py remove 更多信息可以参考： https://www.cnblogs.com/zoro-robin/p/6110188.html","link":"/2018/10/18/2018/10/2018-10-18-PyWinService/"},{"title":"Python Screen Gif","text":"2018-10-22Background最近用了一些 gif 生成工具，感觉挺好用的，就想着 python 是不是也可以实现，自己做一个。 浏览了一些文章，发现有一些现成的库可以用。 最终的想法还是做成一个 GUI，今天是第一步，思路利用PIL的ImageGrab抓取屏幕，然后使用opencsv写入视频流，再用moviepy截取视频的画面生成 gif。 Lib Installed下面是主要用的库，pillow就是PIL， opencv-python就是cv2。 在使用moviepy的时候，会需要下载第三方 exe ffmpeg-win32-v3.2.4.exe 123pip install pillowpip install opencv-pythonpip install moviepy Code12345678910111213141516171819202122232425262728293031323334import numpy as npimport cv2import moviepy.editor as mpyfrom PIL import ImageGrabOUTPUT_NAME = 'test'screen_grabed = ImageGrab.grab() # 获得当前屏幕k = np.zeros((200, 200), np.uint8)width, height = screen_grabed.size # 获得当前屏幕的大小fourcc = cv2.VideoWriter_fourcc(*'XVID') # 编码格式video = cv2.VideoWriter('{}.avi'.format( OUTPUT_NAME), fourcc, 16, (width, height)) # 输出文件命名为test.avi,帧率为16，可以自己设置while True: im = ImageGrab.grab() imm = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR) # 转为opencv的BGR格式 video.write(imm) cv2.imshow('imm', k) if cv2.waitKey(1) &amp; 0xFF == ord('q'): breakvideo.release()cv2.destroyAllWindows()#import imageio# imageio.plugins.ffmpeg.download()# 视频文件的本地路径content = mpy.VideoFileClip('{}.avi'.format(OUTPUT_NAME))# 剪辑0分1秒到0分6秒的片段。注意：不使用resize则不会修改清晰度c1 = content.subclip((0, 1), (0, 6)).resize((960, 640))# 将片段保存为gif图到python的默认路径c1.write_gif('{}.gif'.format(OUTPUT_NAME)) Next接下来，需要研究一下几个点： 怎么使用 GUI 来录制固定区域的屏幕 是否可以截取图片流，再直接把图片打包成 gif，省去中间生成 avi 的过程。 … 2018-10-24Background上一篇把截图做成视频，再把视频转回 gif 似乎有点画蛇添足，今天就考虑直接把截图转成 gif Code1234567891011121314151617import os, timeimport imageiofrom PIL import ImageGrabimages = []start_time = time.time()while True: im = ImageGrab.grab() im.save('temp.png') #save image to the list. images.append(imageio.imread('temp.png')) time.sleep(0.001) end_time = time.time() if(end_time - start_time &gt; 5): breakimageio.mimsave('test.gif', images, duration=0.3)c1.write_gif('{}.gif'.format(OUTPUT_NAME)) Next 怎么使用 GUI 来录制固定区域的屏幕 是否可以提高性能，每次截图放到 list 中性能消耗比较大 … 2018-10-27Background今天更新了下代码，提高了可用性，增加了UI设定截屏的区域，有时间再更新这篇，已经基本可用了。 Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import tkinterimport os, time, tempfileimport imageiofrom PIL import ImageGrabfrom threading import Threadimport msvcrtoutput_gif_name = 'test.gif'width_grab = 600height_grab = 400x_grab = 0y_grab = 0flag_stop_record = [False]flag_is_recording = [False]root = tkinter.Tk()root.overrideredirect(True)#root.attributes(&quot;-alpha&quot;, 0.3)窗口透明度70 %root.attributes(&quot;-alpha&quot;, 0.3)#窗口透明度60 %root.geometry(&quot;{}x{}+{}+{}&quot;.format(width_grab, height_grab, x_grab,y_grab))#&quot;300x200+10+10&quot;canvas = tkinter.Canvas(root)canvas.configure(width = width_grab)canvas.configure(height = height_grab)canvas.configure(bg = &quot;blue&quot;)canvas.grid(column=0,row=0)# canvas.configure(highlightthickness = 0)# canvas.pack()# canvas.pack(side=&quot;bottom&quot;,fill=&quot;both&quot;,expand=True)# x, y = 0, 0def move(event): global x_grab,y_grab, width_grab, height_grab new_x = (event.x-x)+root.winfo_x() new_y = (event.y-y)+root.winfo_y() x_grab,y_grab = new_x,new_y s = &quot;{}x{}+{}+{}&quot;.format(width_grab, height_grab, new_x, new_y) # s = &quot;{}x{}+&quot; + str(new_x)+&quot;+&quot; + str(new_y) root.geometry(s) # canvas.create_rectangle(new_x, new_y, 300, 200, fill=&quot;blue&quot;) print(&quot;s = &quot;,s) print(root.winfo_x(),root.winfo_y()) print(event.x,event.y) print()def button_1(event): global x,y x,y = event.x,event.y print(&quot;x, y = &quot;, x, y) print(&quot;event.x, event.y = &quot;,event.x,event.y)def start_record_gif(): global output_gif_name, x_grab, y_grab, flag_stop_record images_temp_list = [] # start_time = time.time() isEnded = False while(not isEnded): if(flag_stop_record[0] == True): isEnded = True # try: # flag_stop_record.get(False) # isEnded = True # except: # isEnded = False im = ImageGrab.grab((x_grab, y_grab , x_grab+width_grab, y_grab+height_grab)) im.save('temp.png') images_temp_list.append(imageio.imread('temp.png')) time.sleep(0.001) imageio.mimsave(output_gif_name, images_temp_list, duration=0.3) # end_time = time.time() # if(end_time - start_time &gt; 5): # breakdef record_gif(event): global flag_is_recording if(not flag_is_recording[0]): root.geometry(&quot;0x0+0+0&quot;) flag_is_recording[0] = True t = Thread(target=start_record_gif, args=()) t.start() else: passdef exit(event): # flag_stop_record.put(True) flag_stop_record[0] = True root.destroy()canvas.bind(&quot;&lt;B1-Motion&gt;&quot;, move)canvas.bind(&quot;&lt;Button-1&gt;&quot;, button_1)canvas.bind(&quot;&lt;Button-3&gt;&quot;, exit)canvas.bind(&quot;&lt;Double-Button-1&gt;&quot;,record_gif)root.mainloop() Next 优化使用方式 使用temp目录 … 更多信息可以参考： python小应用之moviepy的视频剪辑制作gif图 利用Python来完成屏幕录制","link":"/2018/10/27/2018/10/2018-10-27-PyScreenGif/"},{"title":"DB For Test Result","text":"背景之前做的自动化测试，最后的测试结果都是通过邮件发送给相关人员，没有做持久化，所以有时候想找一个版本的结果要去翻邮件列表，而且很有可能找不到。 接下来我希望能把最后的测试结果存放在数据库中，这样就可以方便对历史结果进行查询，充分利用数据库的特性。 需求分析目前一些测试用例已经固定，变化不会太大，像 Benchmark 和 Message Import 都有一批固定的数据会重复跑。不过目标还是能创建相对通用的数据结构， 让大多数的结果都能存放。 希望最后达到的效果： 历次测试结果都能保存下来 能区别出测试的有效性，有时候测试因为环境或者别的原因失败了，那么是没有参考价值的 可以方便查询某一版本，某一参数或者某一天的测试结果（最好能直接通过 sql 查询到） 现状分析测试最直接的结果是生成在 testResult.csv 文件中，主要结构类似如下： TestCase Msg Nums CostTime(s) Failed to Load/Post TC01_MsgType1 30000 1200 0/0 TC02_MsgType2 45000 1652 0/0 TC03_MsgType3 45000 2320 0/0 测试完成后，会根据 TestResult.xlsx.template 生成一份 excel 报告，额外包含一些比如 CPU/Memory 及使用的参数等信息。最后通过 SMTP 服务器发送到全组邮件列表。 设计方案方案一：将测试结果文件直接保存在数据库中这个方案是简单的，做的工作也最少，只要在数据库中建立一张表，建立几个必要的字段(e.g. date, version)再加上文件 IO 流就好了，比如： TestVersion APPVersion TotalTime FileResult 20181202_121232 P01 2450 xxx 20181202_134630 P02 3600 xxx 20181202_151256 Po3 1660 xxx 当然，直接把文档存在数据库也不太好，可以直接存在一个目录下，而在数据库里只存文件路径： 123456789create table testresult( id int primary key auto_increment, datetimes datetime, testVersion char(15), APPVersion varchar(20), TotalTime int, FilePath varchar(30) -- File MediumBlob); 但明显的，这个方案并不理想，比如我想查询一个 case 在一个时间段的平均花费时间，我没有方法用这个方式得到，打开几个对应的文档自己查看，那是不现实的。 方案二：将测试结果构造成 JSON 等数据结构这个方案其实只是方案一的改进版，将测试结果数据抽象之后，可以做为 JSON 或者别的数据库支持的格式存储，可以使用 sql 直接查询其中的数据。 TestVersion APPVersion TotalTime JsonResult 20181202_121232 P01 2450 {xxx} 20181202_134630 P02 3600 {xxx} 20181202_151256 Po3 1660 {xxx} 所以如果数据库支持的话，那也是相对方便的。 比如 JSON 可以类似如下： 12345678910111213141516[ { &quot;TestCase&quot;: &quot;TC01_MsgType1&quot;, &quot;MsgNums&quot;: 30000, &quot;CostTime&quot;: 1200, &quot;FailedLoad&quot;: 0, &quot;FailedPost&quot;: 0 }, { &quot;TestCase&quot;: &quot;TC01_MsgType1&quot;, &quot;MsgNums&quot;: 30000, &quot;CostTime&quot;: 1200, &quot;FailedLoad&quot;: 0, &quot;FailedPost&quot;: 0 }] 123456789create table testresult( id int primary key auto_increment, datetimes datetime, testVersion char(15), APPVersion varchar(20), TotalTime int, JsonResult json DEFAULT NULL -- File MediumBlob); 比如通过如下的语句就能找到一些想要的数据： 1select JsonResult-&gt;'$[*].CostTime' from testresult; 更多的 mysql 的操作请看最后参考中的一些文档。 方案三：使用传统数据格式来存储因为公司里不方便用 mysql 之类的数据库，现在的有 sqlserver，那么就直接利用吧。 这样的话就要设计更多的字段了，我画了一个简单的 E-R 图： 像这样如果想要查询某一段时间一个 case 的平均时间就很方便了： 12345select AVG(TD.CostTime) from TestResult TR, TestDetails TD, TestCases TCwhere TD.TestResultID = TA.ID and TD.TestCaseID = TC.IDand TC.TestCaseName = 'ImportMsg01'and TR.TestTime &gt; '2018-12-01'group by TD.CostTime 最后主要思路这样差不多了，真的在做的时候应该会有许多细节，到时完善。这个是接下来这周的一个小目标，加油。 参考： mysql 对 json 数据的使用 对 MySQL 中 JSON 数据类型的操作和分析 MySQL5.7 新特性之 JSON 类型 mysql5.7Json 数组解析","link":"/2018/12/02/2018/12/2018-12-02-TestResultDBDesign/"},{"title":"Java Annotation","text":"背景开始回顾 Java 的一些基础知识，今天看了下注解。 像在 Spring 中，可以使用 xml 来配置类之间的关系与实例化，虽然结构清晰，但不是很方便。使用注解来让框架自己去识别你的意图，把配置放在了类定义中。 以下面上周写的简单 REST API 为例，通过使用 Spring MVC 相关注解就能配置方法与 HTTP Request 的对应。而使用 swagger 相关的注解，便很方便的生成对应的 API 文档。 而这些都是框架、第三方库通过反射得到注解的信息之后再得到的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package fun.bearfly.swagger.controller;import java.util.ArrayList;import java.util.Collections;import java.util.HashMap;import java.util.List;import java.util.Map;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RestController;import fun.bearfly.swagger.model.EWord;import io.swagger.annotations.ApiImplicitParam;import io.swagger.annotations.ApiImplicitParams;import io.swagger.annotations.ApiOperation;@RestController@RequestMapping(value=&quot;/ewords&quot;)public class EWordController { static Map&lt;Long, EWord&gt; ewordsMap = Collections.synchronizedMap(new HashMap&lt;Long, EWord&gt;()); @ApiOperation(value=&quot;HelloWorld&quot;, notes=&quot;&quot;) @RequestMapping(value=&quot;helloworld&quot;, method=RequestMethod.GET) public String sayHello() { return &quot;hello, world!&quot;; } @ApiOperation(value=&quot;GetEword&quot;, notes=&quot;&quot;) @RequestMapping(value={&quot;&quot;}, method=RequestMethod.GET) public List&lt;EWord&gt; getEWordList() { List&lt;EWord&gt; ewords = new ArrayList&lt;EWord&gt;(ewordsMap.values()); return ewords; } @ApiOperation(value=&quot;CreateEword&quot;, notes=&quot;Create Eword by Eword&quot;) @ApiImplicitParam(name = &quot;eword&quot;, value = &quot;user domain&quot;, required = true, dataType = &quot;EWord&quot;) @RequestMapping(value=&quot;&quot;, method=RequestMethod.POST) public String postEWord(@RequestBody EWord eword) { ewordsMap.put(eword.getId(), eword); return &quot;success&quot;; } @ApiOperation(value=&quot;GetEwordDetails&quot;, notes=&quot;Get eword by id&quot;) @ApiImplicitParam(name = &quot;id&quot;, value = &quot;ewordID&quot;, required = true, dataType = &quot;Long&quot;) @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.GET) public EWord getEWord(@PathVariable Long id) { return ewordsMap.get(id); } @ApiOperation(value=&quot;UpdateEwordInfo&quot;, notes=&quot;Update eword by id&quot;) @ApiImplicitParams({ @ApiImplicitParam(name = &quot;id&quot;, value = &quot;ewordID&quot;, required = true, dataType = &quot;Long&quot;), @ApiImplicitParam(name = &quot;user&quot;, value = &quot;ewordDomain&quot;, required = true, dataType = &quot;EWord&quot;) }) @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.PUT) public String putEWord(@PathVariable Long id, @RequestBody EWord newUser) { EWord eword = ewordsMap.get(id); eword.setCnWord(newUser.getCnWord()); eword.setEnWord(newUser.getEnWord()); ewordsMap.put(id, eword); return &quot;success&quot;; } @ApiOperation(value=&quot;DeleteEword&quot;, notes=&quot;Delete eword by id&quot;) @ApiImplicitParam(name = &quot;id&quot;, value = &quot;ewordID&quot;, required = true, dataType = &quot;Long&quot;) @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.DELETE) public String deleteEWord(@PathVariable Long id) { ewordsMap.remove(id); return &quot;success&quot;; }} AnnotationAnnotation 是代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取，并执行相应的处理。Annotation 能被用来为程序元素(类、方法、成员变量等)设置元数据。通过使用 Annotation，程序开发人员可以在不改变原有逻辑的情况下，在源文件嵌入一些补充信息。Annotatio n 就像修饰符一样被使用， 如果希望让程序中的 Annotation 能在运行时起一定的作用，只有通过某种配套的工具对 Annotation 中的信息进行访问的处理，访问和处理 Annotation 的工具统称**APT (Annotation Processing Tool)**。 基本的 AnnotationJava 常见与使用的基本的 Annotation 有三种： 注解 作用 @Override 限定重写父类的方法 @Deprecated 标示已过时 @SuppressWarnings 抑制编译器警告 在后续的版本中加入了更多的注解，比如在 Java8 中有 @FunctionalInterface。 @FunctionalInterface，主要用于编译级错误检查，加上该注解，当你写的接口不符合函数式接口定义的时候，编译器会报错。 接口中包含了两个抽象方法，违反了函数式接口的定义，Eclipse 报错提示其不是函数式接口。Invalid '@FunctionalInterface' annotation; IFunctionalInterfaceDemo is not a functional interface 123456@FunctionalInterfaceinterface GreetingService{ void sayMessage(String message); void sayHi(String message)} 自定义 Annotation我们可以自己定义注解，比如： 12345@Target(ElementType.METHOD)public @interface Login { String username() default &quot;bearfly1990&quot;; String password() default &quot;123456&quot;;} 可以看到在定义注解的时候我们又用到了别的注解，这些注解是用来注解注解的，即元注解:) 四大元注解： @Target：注解能用在哪儿 @Target(ElementType.METHOD) ElementType 可能的值： TYPE 用于 class 定义 CONSTRUCTOR 用于构造方法 METHOD 用于方法 FIELD 用于成员变量 LOCAL_VARIABLE 局部变量声明 PACKAGE 包声明 PARAMETER 方法参数声明 ANNOTATION_TYPE TYPE_PARAMETER TYPE_USE @Retention：注解信息在哪个阶段有效 @Retention(RetentionPolicy.RUNTIME) RetentionPolicy 可能的值： SOURCE：源码阶段，编译时，一般是用来告诉编译器一些信息，将被编译器丢弃 CLASS：注解在 class 文件中可用，会被 VM 丢弃 RUNTIME：运行时，VM 在运行时也保留注解，这个才能通过反射获取到 @ Documented 将此注解包含在 JavaDoc 中 @ Inherited 允许子类继承父类中的注解 下面建了另外一个简单的例子,使用到了@Repeatable (since java8) 1234567891011@Repeatable(TestCases.class)@Retention(RetentionPolicy.RUNTIME)public @interface TestCase { public int id(); public String description() default &quot;&quot;;}@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface TestCases { TestCase[] value();} 下面是使用的方式： 123456789101112131415161718public class TestCaseDemo { @TestCase(id = 1, description = &quot;test username&quot;) public boolean testUsername(String username){ return false; } @TestCase(id = 2, description = &quot;test password&quot;) public boolean testPassword(String password){ return false; } @TestCase(id = 3, description = &quot;test username and password&quot;) @TestCase(id = 4, description = &quot;test login&quot;) public boolean testLogin(String username, String password){ return false; }} 取得注解信息通过反射我们可以得到 Annotation 的信息，下面是例子： 123456789101112131415161718public class AnnotationDemo { public static void main(String[] args) { Method[] methods = TestCaseDemo.class.getMethods(); for (Method method : methods) { Annotation[] annotations = method.getAnnotations(); for (Annotation annotation : annotations) { if (annotation != null &amp;&amp; annotation instanceof TestCase) { System.out.println(((TestCase) annotation).id() + &quot; &quot; + ((TestCase) annotation).description()); } else if (annotation != null &amp;&amp; annotation instanceof TestCases) { TestCase[] repeatTestCases = method.getAnnotationsByType(TestCase.class); for (TestCase testCase : repeatTestCases) { System.out.println(testCase.id() + &quot; &quot; + testCase.description()); } } } } }} 最后相关代码在javalearn 下可以找到。 更多的细节待续… 参考： Java 中 Annotation 用法 Java 9：装 B 之前你必须要会的——泛型，注解，反射","link":"/2018/12/06/2018/12/2018-12-05-JavaAnotiation/"},{"title":"Python Thread Lock","text":"背景这两天在继续修改之前写的测试脚本。这两天想要实现并发跑一些任务，之后还要把结果写要 testresult.csv 中。 这样的话就涉及一个问题，因为是并发跑的，所以可以会出现资源争抢的问题，那么就需要对读写加锁控制。 在网上查了下，发现有第三方文件锁库 fcntl 可以使用，不过可惜的是Unix Specific 所以最直接的还是用线程锁来解决这个问题 threading在 threading 模块中定义了 Lock 类，可以方便的处理锁定： 123456# create lockmutex = threading.Lock()# acquire lockmutex.acquire([timeout])# release lockmutex.release() Demo下面是我写的简单的例子，模拟了实际的情况，每个 job 跑完的时间是不一定的，当他们想要写测试结果文件中写数据的时候要先去获得锁。 123456789101112131415161718192021222324252627import threadingimport timeimport randommutex = threading.Lock()def job_running(): time.sleep(random.randint(1,6))def write_result(txt_file, thread_name): mutex.acquire(10) with open(txt_file, 'a') as f: print(&quot;Thread {0} acquire lock&quot;.format(thread_name)) f.write(&quot;write from thread {0} \\r\\n&quot;.format(thread_name)) time.sleep(random.randint(1,3)) mutex.release() print(&quot;Thread {0} exit&quot;.format(thread_name))def run_job(txt_file): thread_name = threading.currentThread().getName() job_running() write_result(txt_file, thread_name)if __name__ == '__main__': for i in range(5): myThread = threading.Thread(target=run_job, args=(&quot;testResult.txt&quot;,)) myThread.start() 运行的结果，和预期的效果一样： 1234567891011PS C:\\Users\\mayn\\Desktop\\workspace\\python_file_lock&gt; python .\\file_lock_demo.pyThread Thread-3 acquire lockThread Thread-3 exitThread Thread-5 acquire lockThread Thread-5 exitThread Thread-2 acquire lockThread Thread-2 exitThread Thread-1 acquire lockThread Thread-1 exitThread Thread-4 acquire lockThread Thread-4 exit 使用fcntl因为在Windows下无法使用，自己也没去尝试，有兴趣可以通过最后参考中的链接去了解。 最后多线程是个可以探究的很深的问题，还有许多的知识点需要继续学习，遇到问题一定要深究一下。 参考： Python 多线程读写文件加锁 Python 的文件锁使用","link":"/2018/12/11/2018/12/2018-12-11-PythonThreadLock/"},{"title":"Single Node Hadoop","text":"背景今天在阿里云机器上试着搭了一个单结点的 Hadoop, 记录一下过程。 资源准备使用的 jdk 和 Hadoop 版本分别为： jdk-8u40-linux-x64.gz hadoop-2.7.3.tar.gz 系统是阿里云的 CentOS Linux release 7.4.1708 (Core) 配置环境JDKjdk 解压在/usr/local/jdk1.8.0_40目录下。 编辑/etc/profile, 将 jdk 添加到环境变量中。 12345JAVA_HOME=/usr/local/jdk1.8.0_40JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$JAVA_HOME/bin:$PATHexport JAVA_HOME JRE_HOME CLASS_PATH PATH 然后执行source /usr/profile 使配置生效。 Hadoop设置环境变量vim ~/.bash_profile： 123HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.3PATH=$PATH:$HADOOP_HOME/binexport HADOOP_HOME PATH 使配置生效source ~/.bash_profile 创建为 Hadoop 之后使用的目录 123mkdir -p /usr/local/hadoop/tmpmkdir -p /usr/local/hadoop/hdfs/namemkdir -p /usr/local/haddop/data 设置 ssh 免密登录生成公钥： 1ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa 将公钥加入验证： 1cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys 尝试登录验证： 1ssh localhost 设置主机将主机名设置成 Master vim /etc/hostname: 1Master 然后让 IP 和主机名绑定 vim /etc/hosts： 1xxx.xxx.xxx.xxx Master Hadoop 配置与运行Hadoop 的解压路径为 /usr/local/hadoop/ hadoop-2.7.3 在/usr/local/hadoop/ hadoop-2.7.3/hadoop/etc目录下有许多配置文件需要我们配置。 hadoop-env.sh主要是修改使用我们自己的 jdk 路径 1export JAVA_HOME=/usr/local/jdk1.8.0_40 yarn-env.sh同样是修改 JAVA_HOME 路径： 1export JAVA_HOME=/usr/local/jdk1.8.0_40 core-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://Master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml这个文件从 mapred-site.xml.template 拷过来改： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framwork.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 格式化 namenode12cd /usr/local/hadoop/hadoop-2.7.3/bin/./hdfs namenode -format 启动运行 sbin 目录下的 start-all.sh 12cd /usr/local/hadoop/hadoop-2.7.3/sbin/./start-all.sh 然后使用 jps 查看 java 进程jps,可以看到我们想要的都起来了。 12345678[root@izbp162mggaelq3xw6nk65z sbin]# jps1061 Application7765 jar4903 ResourceManager4475 NameNode5003 NodeManager5292 Jps4750 SecondaryNameNode 通过网址访问如果启动成功，我们便能成功访问下列的网址： http://www.bearfly.fun:8088/cluster http://www.bearfly.fun:50070/dfshealth.html#tab-overview 最后还是要想办法真正搭个集群的环境，不行只能虚拟机上了，最好能再搞到一台机器。 参考：网络视频","link":"/2018/12/12/2018/12/2018-12-12-SetupSingleNodeHadoop/"},{"title":"Run Jobs In Parallel","text":"背景这周的一个目标是把之前 Benchmark 的脚本改写，使它支持并发的跑 job。 之前的配置是使用 ini，不适合配置现在这种需求，也不想用之前的 xml 来配置，所以今天就简单的研究了下使用 yaml 来配置并运行。 yaml 配置文件这面是简单的一个例子，希望最后执行的时候，不同的 task 之间是串行在跑的，而同一个 task 里的 job 是并发跑的： 1234567891011121314151617181920212223description: sample to run jobs in paralleltasks: - taskid: 01 desc: task 01 jobs: - jobid: 001 name: job 001 in parallel - jobid: 002 name: job 002 in parallel - jobid: 003 name: job 003 in parallel - jobid: 004 name: job 004 in parallel - taskid: 02 desc: task 02 jobs: - jobid: 005 name: job 005 - taskid: 03 desc: task 03 jobs: - jobid: 06 name: job 006 使用 Queue 控制运行下面直接上代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243import yamlimport threadingimport random, timeimport queueclass Job: def __init__(self, jobid, name): self.jobid = jobid self.name = nameclass JobProcessor(): def __init__(self, queue): self.queue = queue def run_job(self, job): print('start run job {}'.format(job.name)) time.sleep(random.randint(1,10)) print('ended run job {}'.format(job.name)) self.queue.task_done() def run_jobs(self): while not self.queue.empty(): job = self.queue.get() myThread = threading.Thread(target=self.run_job, args=(job,)) myThread.start()def main(): f = open('jobs.yaml','r',encoding='utf-8') yaml_content = f.read() json_content = yaml.load(yaml_content) tasks_list = json_content['tasks'] for task in tasks_list: jobs = task['jobs'] jobs_num = len(jobs) jobs_queue = queue.Queue(jobs_num) for job in jobs: jobs_queue.put(Job(job['jobid'], job['name'])) JobProcessor(jobs_queue).run_jobs() jobs_queue.join()if __name__ == '__main__': main() 仔细看的话，主要是利用 queue 存储了 job 信息，然后利用.task_done和.join方法，实现等待任务的完成的操作。 下面是运行的结果： 12345678910111213C:\\Users\\bearfly1990\\Desktop\\python_test\\task_parallel&gt;python runjobs.pystart run job job 001 in parallelstart run job job 002 in parallelstart run job job 003 in parallelstart run job job 004 in parallelended run job job 001 in parallelended run job job 004 in parallelended run job job 003 in parallelended run job job 002 in parallelstart run job job 005ended run job job 005start run job job 006ended run job job 006 升级配置，增加功能之前（上面）的简单例子是不能够满足实际的需求的，只是多线程的一个简单例子。 这周花了些时间，增加了一些配置项，针对 Job 提供了 JobGroups，每个 Group 之间都是默认并行的，但是也可以通过 pregroup 来限定先后顺序。而 group 内部，我则希望是可以配置的，既可以并行，也可以串行。 下面是新的 yaml 配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859description: sample to run jobsTasks: - taskid: 01 name: task 01 mode: import type: T file: 01.txt - taskid: 02 name: task 02 mode: import type: S file: 02.txt - taskid: 03 name: task 03 mode: jobgroup JobGroups: - id: 1 name: G1 mode: parallel Jobs: - jobid: 001 name: job G1-01 - jobid: 002 name: job G1-02 - jobid: 003 name: job G1-03 - jobid: 004 name: job G1-04 - id: 2 name: G2 mode: parallel Jobs: - jobid: 005 name: job G2-05 - jobid: 006 name: job G2-06 - jobid: 007 name: job G2-07 - jobid: 008 name: job G2-08 - id: 3 name: G3 mode: serial pregroup: 1 Jobs: - jobid: 009 name: job G3-09 - jobid: 010 name: job G3-10 - jobid: 011 name: job G3-11 - jobid: 012 name: job G3-12 - taskid: 04 name: hello world mode: say - taskid: 05 name: test passed mode: wait 下面是具体的实现，已经在一些关键点加了备注： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import yamlimport threadingimport randomimport timeimport queueclass ImportInfo: def __init__(self, name, type, file): self.name = name self.type = type self.file = fileclass Job: def __init__(self, jobid, name): self.jobid = jobid self.name = nameclass JobProcessor(): def __init__(self, queue=None): self.queue = queue def run_job(self, job): print('start run job {}'.format(job.name)) time.sleep(random.randint(1, 7)) print('ended run job {}'.format(job.name)) self.queue.task_done() def run_jobs_parallel(self): while not self.queue.empty(): job = self.queue.get() my_thread = threading.Thread(target=self.run_job, args=(job,)) my_thread.start() self.queue.join() def run_jobs_serial(self): while not self.queue.empty(): job = self.queue.get() self.run_job(job) self.queue.join() def run_job_group(self, job_group, job_group_map): pregroup = job_group.get('pregroup', -1) jobs = job_group['Jobs'] # default mode is parallel mode = job_group.get('mode', 'parallel') # add jobs to the queue jobs_num = len(jobs) jobs_queue = queue.Queue(jobs_num) job_group_map[job_group['id']] = jobs_queue for job in jobs: jobs_queue.put(Job(job['jobid'], job['name'])) # wait for pregroup run finished if have pregroup if(pregroup != -1): while(True): pre_queue = job_group_map.get(pregroup, None) if pre_queue: pre_queue.join() break time.sleep(1) print('### start run jobgroup:{} ###'.format(job_group['name'])) if(mode == 'parallel'): JobProcessor(jobs_queue).run_jobs_parallel() else: JobProcessor(jobs_queue).run_jobs_serial() print('### end run jobgroup:{} ###'.format(job_group['name'])) def run_job_groups(self, job_groups): # map each group queue, so that we could check the group is finished or not. job_group_map = {} thread_list = [] for job_group in job_groups: run_job_group_thread = threading.Thread( target=self.run_job_group, args=(job_group, job_group_map)) thread_list.append(run_job_group_thread) run_job_group_thread.start() # wait for all groups are finished for thread in thread_list: thread.join()def run_import(importInfo): print('import {}'.format(importInfo.name))def main(): fin = open('jobs.yaml', 'r', encoding='utf-8') json_content = yaml.load(fin) task_list = json_content['Tasks'] for task in task_list: print(task['name'], '|', task['mode'], '|', task.get('type', 'None'), '|', task.get('file', 'None')) task_mode = task['mode'].lower() if(task_mode == 'jobgroup'): JobProcessor().run_job_groups(task['JobGroups']) elif(task_mode == 'import'): run_import(ImportInfo(task['name'], task['type'], task['file'])) elif(task_mode == 'say'): print('say {}'.format(task['name'])) elif(task_mode == 'wait'): print('wait {}'.format(task['name']))if __name__ == '__main__': main() 下面是一次跑的例子： 12345678910111213141516171819202122232425262728293031323334353637383940c:\\Users\\mayn\\Desktop\\workspace\\python_yaml&gt;python runjobs.pytask 01 | import | T | 01.txtimport task 01task 02 | import | S | 02.txtimport task 02task 03 | jobgroup | None | None### start run jobgroup:G1 ###### start run jobgroup:G2 ###start run job job G1-01start run job job G1-02start run job job G2-05start run job job G2-06start run job job G1-03start run job job G1-04start run job job G2-07start run job job G2-08ended run job job G2-05ended run job job G1-02ended run job job G1-01ended run job job G2-06ended run job job G1-03ended run job job G2-08ended run job job G1-04ended run job job G2-07### start run jobgroup:G3 ###### end run jobgroup:G1 ###start run job job G3-09### end run jobgroup:G2 ###ended run job job G3-09start run job job G3-10ended run job job G3-10start run job job G3-11ended run job job G3-11start run job job G3-12ended run job job G3-12### end run jobgroup:G3 ###hello world | say | None | Nonesay hello worldtest passed | wait | None | Nonewait test passed 使用 map 和 multiprocessing在晓辉建议下，使用 map 函数结合线程池简化了代码，不用自己控制并发了，api 更简单。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import yamlimport threadingimport randomimport timeimport queuefrom multiprocessing.dummy import Pool as ThreadPoolclass ImportInfo: def __init__(self, name, type, file): self.name = name self.type = type self.file = fileclass Job: def __init__(self, jobid, name): self.jobid = jobid self.name = nameclass JobProcessor(): def __init__(self, queue=None): self.queue = queue def run_job(self, job): print('start run job {}'.format(job.name)) time.sleep(random.randint(1, 7)) print('ended run job {}'.format(job.name)) def run_job_group(self, param): job_group = param['job_group'] job_group_map = param['job_group_map'] pregroup = job_group.get('pregroup', -1) # add jobs to a new list jobs = [Job(job['jobid'], job['name']) for job in job_group['Jobs']] # default mode is parallel mode = job_group.get('mode', 'parallel') jobs_thread_pool = ThreadPool(len(jobs)) job_group_map[job_group['id']] = jobs_thread_pool # wait for pregroup run finished if have pregroup if(pregroup != -1): while(True): pre_group_pool = job_group_map.get(pregroup, None) if pre_group_pool: pre_group_pool.close() pre_group_pool.join() break time.sleep(1) print('### start run jobgroup:{} ###'.format(job_group['name'])) if(mode == 'parallel'): jobs_thread_pool.map(self.run_job, jobs) else: list(map(self.run_job, jobs)) print('### end run jobgroup:{} ###'.format(job_group['name'])) def run_job_groups(self, job_groups): job_group_map = {} groups_thread_pool = ThreadPool(len(job_groups)) job_group_param = [{'job_group':job_group, 'job_group_map':job_group_map} for job_group in job_groups] groups_thread_pool.map(self.run_job_group, job_group_param) groups_thread_pool.close() groups_thread_pool.join()def run_import(importInfo): print('import {}'.format(importInfo.name))def main(): fin = open('jobs.yaml', 'r', encoding='utf-8') json_content = yaml.load(fin) task_list = json_content['Tasks'] for task in task_list: print(task['name'], '|', task['mode'], '|', task.get('type', 'None'), '|', task.get('file', 'None')) task_mode = task['mode'].lower() if(task_mode == 'jobgroup'): JobProcessor().run_job_groups(task['JobGroups']) elif(task_mode == 'import'): run_import(ImportInfo(task['name'], task['type'], task['file'])) elif(task_mode == 'say'): print('say {}'.format(task['name'])) elif(task_mode == 'wait'): print('wait {}'.format(task['name']))if __name__ == '__main__': main() 最后具体实现在真正写的时候需要再调整一下，而且感觉代码还可以优化，感觉目前并不是最好的实现方式，或者说写法有点怪。。。 参考： python 队列 Queue Python 操作 yaml 文件","link":"/2018/12/18/2018/12/2018-12-18-PyRunJobsInParallel/"},{"title":"My 2018","text":"我的 2018这个元旦本没打算回家的，毕竟快一个月上下便是农历新年了，杭州来回也要好几个小时。只是那日想到元旦也没什么事情好做，也有点想念家里了，便还是驱车回来了。 回来这几日也没怎么出门，只是帮母亲跑了些腿，看了下外婆，今天在奶奶家几家子人一起吃了顿饭。其余时间便在家中休息，补了些觉。 记得上个元旦 ，自己与亢叔几人在东北游玩，领略了北国风光，对北方的雪景与暖气印象深刻。这一年里，亢叔入住新家，升职加薪；旭红去了新的公司，似乎工作辛苦了些，但还是过着潇洒的吃吃喝喝的日子，好生羡慕；桃桃倒是流年不利，摔断了腿，修养了好一阵子，不过好在也寻到了如意郎君（￣ ▽ ￣），希望新的一年能够否极泰来，万事顺遂。 自己这一年，也发生了许多的事，倒是体重没有什么变化，波动之后还是处在高位，又要变成 2019 年的目标了（￣ ▽ ￣） 下个月中旬就要去三墩的项目组了，尤记得 2012 年的农历新年前几天，自己去西湖科技园报到，实习入职，亢叔当时便是我的 mentor。当年年轻，也没想着说商量着早些或者干脆过年后再来报到，在学校封校后还在外面挨了两天。不过恒天也是厚道，竟也发了一份过节费给我，也是因这一节吧，对恒天的印象还可以。 今年年初 Publisher 项目结束，便来了 Team Engineering，陶总他们都很 Nice，水平也很高。做测试如果有给力的开发做后盾是非常幸福的一件事，很多事情解决起来都会比较顺利。在 TE 的这近一年不到的时间，也做了点事情，学着用了 python，搭了几个简单的测试流程，实践了一些自动化测试与性能测试方面的知识。而最重要的，便是重新燃起了自己学习些许热情，项目与工作驱动学习，才是最快的一种方式。还没到 TE 之前，那时龙便与我说他们那边有招人的计划，不过坑没有下来，等确定的时候，我已经在 TE 做了一段时间了。Jed 也说到做到，给我提了 promotion，不至于让我特别的尴尬，我也便安心的在组里呆着先。 两年前AMS 项目结束，晶晶另谋高就，而我继续留下来。想着既然这边的报表开发项目结束，那 Leader 应该会另安排别的任务，或许是别的项目组的开发吧。毕竟自己的 Java 底子也还在，也与 CW 提及过。不过后来领导们应该有别的考量，Jed 找到我，提议让我转测试（开发），利用自己的“优势”，在测试这一番天地里试试。说真心话，我之前是没有想过自己要去做测试的，不是别的，只是没想过而已。那我想着，的确自己实际开发的经验不多，Jed 的一翻话也有一些道理。于我自己而言，测试好歹也是贴近真实的项目比较近的工作，一步步来吧。况且当时几位领导也“允诺”但凡有一天我不想做测试了，随时转开发都可以，虽我知这不过是“说辞”，但也明白他们的“心意”，让我心安。于是那年便跟着 Apple 在 Publisher 做些事，开始接触测试的一些概念与工作，包括一些自动化工具与脚本，测试的门道也是特别多的。Apple 和珍珍便算是我测试工作的启蒙导师了。 选择比努力更重要 这回只是随口问龙一句，想着他那应该早就招满员了吧，不过就是这么的巧合，于是便去试了下。我也明白，如果入了这个坑，日子肯定没有在 TE 这边这么好过，凡事要靠自己，至少一开始便要打起 12 分的精神，要学习了解的东西还很多。回过头来说，如果一样的条件摆在我的面前，我还是 55%的概率会选择做开发的工作吧，毕竟还是要靠自己。如果做测试的话，所需要的技能与学习的方向是完全不同的。按我个人的愚见，做测试如果想发展的好，领导很重要，项目组对测试工作的重视程度也很重要，并不单单是工作本身的细节。而测试如果想做的好，态度与责任感最重要。而开发的话，相对会纯粹些，开发能力，经验最重要。当然，如果后期走项目管理与领导路线的话，又是另外一回事了，我也没资格置评。 道富是个不错的公司，至少他的企业文化，或者说那种生活与工作相对独立，是人性化的，也是可以预见到的中国，中国企业继续发展下去的一个样子。我本也是一个“佛系”的人吧，没有远大抱负，只想在自己资质范围内，尽力做好每一件事，努力过好自己的生活罢了。生活，没有退路。加油吧。 今年父亲走的突然，上个春节，一家人还一齐出游，没想到那是三人最后一次一起出去游玩了。父亲这两年多亏了母亲照应，在温州，在上海，好在有母亲陪伴左右。我有时也在想，如果当年离职回家，没有再回杭州，是不是结局会有不一样？或许父亲不会那样倔强。或许我会带着他早点去医院，早点发现病灶吧。没有或许了。从小我对爷爷一点印象都没有，如果以后自己有小孩的话，也是没有爷爷的孩子，这样一想，倒还是有点小伤感了。 处理好父亲的事情，我便回杭州了。在回杭的路上，我与 JY 聊了好些后，也便没有怎么联系了，男儿有泪不轻弹，只是未到情深时吧，泪水真的好不值钱。回去工作之后，好在事情也多，慢慢地便恢复了些情绪。其实我还有一个重要的任务，父亲的离开，影响最大的便是母亲。做为传统的那一代女性，母亲在父亲走后才是最难熬的，时间是最好的良药，但更需要亲人的陪伴。我还好，每天有事可做，还有同事之间的交流，而母亲刚开始却只有一个人呆在家里，还好有婶婶阿姨们偶尔会来看望下母亲。那时起，我每天都会联系下母亲，偶尔发个微信语音，得空视频一下，只求母亲心里好受些。好在母亲平复一些后，便去离家近的厂子里干点轻活，有事可做，又有人交流后，情绪慢慢变好了，到现在我也放心一些了。 那时也曾想过，是否和当年一样干脆回家算了，只赚现在一点工资，呆在这个城市的意义是什么呢？还不如回家那边工作。但也只是想想罢了，不是一个好主意。但也不能再继续这样下去，否则自己只是在这个城市消耗自己的生命，I have no choice。现在的我只能肩负起更多的责任，真的只能靠自己了。 今年，几个要好的兄弟朋友都先后结婚生子了。只是刚好在父亲的事之后，自己心绪不高，更重要的是年假都已经用完了。只在国庆放假的时候，参加了超的婚宴，Bob、兆他们的婚礼都没能现场观礼，实现遗憾，等今年春节前一定要补上当面的祝福。话说起来，今天远超荣升奶爸了，喜得小公主，真的恭喜恭喜。 今年开始，我真正的开始步入被催婚（介绍对象）的年纪了 ╮(￣ ▽ ￣)╭。对于感情，对于婚姻，我始终相信，需要的是两个人的真诚相待，用心经营，相互理解，相互支持，就不知道自己有没有这样的幸运了。不过能找到聊得来的，合适的，真的不容易，需要在合适的时间遇到合适的人，大多数情况下都是我自己的问题吧，哈哈，随缘吧。 父母对于子女并没有生养的恩情，因为那是父母自己的选择，本来就要负起好好抚养的责任。但父母与子女之间的感情却是真实的，也是最珍贵的。最理想的便是父母有自己丰富的生活，与子女之间保持良好的互动。 2019 加油！","link":"/2018/12/31/2018/12/2018-12-31-My2018/"},{"title":"Data Downloader","text":"背景有时候需要从数据库中把数据导出来查看,可以在编辑器中直接拷出来或者导出来。 又或者像 weekly/monthly 的数据，我偶尔导一次，不想再打开 sqldeveloper 去操作 Oracle，所以就写了类似下面的工具。 功能使用 tkniter 来编写了简单的界面： 数据库配置 DB.ini为了省去输入数据库配置的时间，定义了一个配置文件，在刚开始加载的时候就读取进去，支持 Oracle 和 SqlServer 123456[Oracle]host=hostport=25service=instance[SqlServer]service=PC-CX\\SQLEXPRESS 123456def read_config(self): config = configparser.RawConfigParser() config.read('./DB.ini') self.oracle_info = OracleInfo(host=config['Oracle']['host'], port=config['Oracle']['port'], service=config['Oracle']['service']) self.sql_server_info = SqlServerInfo(service=config['SqlServer']['service']) UIUsername &amp; Password按公司规定，用户名和密码是不能写在文件中的，所以每次都需要手动输入，于是把这一块放到 BaseUI 中。 1234def init_user_pwd_entry(self, row=0, user_val=''): self.init_frame() self.entry_user = self.init_input_field(text=&quot;Username:&quot;, val=user_val, row=row, column=0) self.entry_pwd = self.init_input_field(text=&quot;Password:&quot;, row=row + 1, column=0, with_star=True) Sql TextBox把需要执行的query语句写在这里： 12345def init_sql_textbox(self): self.init_frame() tk.Label(self.frame, text='sql:').grid(row=0, column=0, sticky=tk.W) self.entry_sql = tk.Text(self.frame, height=4, width=50) self.entry_sql.grid(row=1, column=0, stick=tk.W) Output path默认输出到文件Output.csv中，当然可以换成别的文件： 123456def init_output_path(self): self.init_frame() tk.Label(self.frame, text='output path:').grid(row=0, column=0, sticky=tk.W) self.label_output_file = tk.Label(self.frame, text='./output.csv') self.label_output_file.grid(row=0, column=1, sticky=tk.W) tk.Button(self.frame, text='...', command=self.select_file).grid(row=0, column=2, sticky=tk.W, pady=4) DB Type可以切换数据库的类型，目前支持SqlServer和Oracle 123456789101112131415161718192021222324def init_db_info_radios(self): self.init_frame() tk.Radiobutton(self.frame, text=&quot;SqlServer&quot;, command=self.change_db_type, variable=self.db_category_var, value=1).grid(row=0, column=0, stick=tk.W) tk.Radiobutton(self.frame, text=&quot;Oracle&quot;, command=self.change_db_type, variable=self.db_category_var, value=2).grid(row=0, column=1, stick=tk.W)def init_db_info_details(self): self.frame_sql_server = self.create_new_frame() self.entry_service_sql_server = self.init_input_field(frame=self.frame_sql_server, text='Service:', row=0, column=0) self.entry_service_sql_server.insert(tk.END, self.sql_server_info.service) self.frame_oracle = self.create_new_frame() self.entry_host_oracle = self.init_input_field(frame=self.frame_oracle, text='Host:', row=0, column=0) self.entry_host_oracle.insert(tk.END, self.oracle_info.host) self.entry_port_oracle = self.init_input_field(frame=self.frame_oracle, text='Port:', row=1, column=0) self.entry_port_oracle.insert(tk.END, self.oracle_info.port) self.entry_service_oracle = self.init_input_field(frame=self.frame_oracle, text='Service:', row=2, column=0) self.entry_service_oracle.insert(tk.END, self.oracle_info.service) self.frame_oracle.pack_forget() Control Buttons最后的开始与退出按钮也作为BaseUI的一部分。 1234def init_control_buttons(self, row=0): self.frame_control = self.create_new_frame() tk.Button(self.frame_control, text='Start', command=self.do_ok).grid(row=row, column=0, sticky=tk.W, pady=4) tk.Button(self.frame_control, text='Quit', command=self.frame.quit).grid(row=row, column=1, sticky=tk.W, pady=4) 关键处理过程绑定执行方法最后的执行方法如下： 1234567891011class DownloadAction(object): @classmethod def start_download(cls, db_info, sql_str, output_file_name): start_time = datetime.now() with db_info.connect() as conn: print(f'start query {sql_str}') sql_query = pd.read_sql_query(sql_str, conn) df = pd.DataFrame(sql_query) df.to_csv(output_file_name, index=False) end_time = datetime.now() print(f'save data to {output_file_name} finished. cost={end_time - start_time}s') 通过初始化UI时，绑定对应的方法： 12def __init__(self, action): self.action = action 把方法当成参数传递： 12if __name__ == '__main__': DownloadUI(DownloadAction().start_download) 在点击Start的时候，执行do_ok方法： 1tk.Button(self.frame_control, text='Start', command=self.do_ok).grid(row=row, column=0, sticky=tk.W, pady=4) do_ok()添加了对用户名和密码的简单判空。 123456789101112131415161718192021def do_ok(self): sql_str = self.entry_sql.get(&quot;1.0&quot;, &quot;end-1c&quot;) if not (self.entry_user.get().strip() and self.entry_pwd.get().strip()): tkinter.messagebox.showerror(title='Error', message='Please input username and password!') return if not sql_str.strip(): tkinter.messagebox.showerror(title='Error', message='Please input sql!') return self.root.withdraw() db_info = self.sql_server_info if self.db_category_var.get() == 2: db_info = self.oracle_info db_info.user = self.entry_user.get() db_info.pwd = self.entry_pwd.get() self.action(db_info, sql_str, self.label_output_file['text']) self.root.deiconify() 最后完整代码在tkinter/demo_02","link":"/2020/01/15/2020/01/2020-01-15-DataDownloader/"},{"title":"Test Result DB Implement","text":"背景上周提到想要把测试的结果保存到数据库中，下面主要介绍实现的思路。 注意：在定义表名和字段的时候，最好使用小写开头，我下面的例子并不规范。 数据结构最后的数据表结构设计成大致如下： sqlalchemy在 Python 中，最有名的 ORM 框架是 SQLAlchemy，它能满足绝大多数对数据库的映射与操作。 网上有许多相关的资料，但是如果不翻墙的话，官网好像访问不了… 下面是使用到的一些类库： 123from sqlalchemy import Column, DateTime, Float, String, create_engine, Integer, ForeignKey, exists, excfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import sessionmaker, relationship TestResult下面是对测试结果的 TestResult 表的映射，需要关联的是 TestDetail 那张表。 TestResult 是单次测试的总结记录，将测试的总体时间，测试的一些环境信息都保存下来，可以根据自己的需求扩充。 1234567891011class TestResult(Base): __tablename__ = 'TestResult' id = Column(Integer, primary_key=True) TestTime = Column(DateTime) TestVersion = Column(String(40)) APPVersion = Column(String(100)) Computer = Column(String(20)) TotalTimeMinutes = Column(Float) CPU = Column(String(20)) Memory = Column(String(20)) TestDetails = relationship('TestDetail') TestCase每一轮测试中，都会有分为不同的 Test Case，而在多次测试时，许多 Case 都是重复的，所以将相关的信息都保存在这个表中。 因为一般性能测试中，与数据条数有关，所以这边我加了一个 Rows，可以根据实际需求更改与添加字段。 123456class TestCase(Base): __tablename__ = 'TestCase' id = Column(Integer, primary_key=True) TestCaseName = Column(String(50)) Rows = Column(Integer) TestDetails = relationship('TestDetail') TestDetail每次测试中，具体的测试结果便存在 TestDetail 这张表中，针对 case 关注的信息不同，可以扩展相应的字段。 可以看到，我在这边建立了他与TestResult, TestCase的外键关联。 1234567class TestDetail(Base): __tablename__ = 'TestDetail' id = Column(Integer, primary_key=True) CostTimeSeconds = Column(Float) CostTimeMinutes = Column(Float) TestResultID = Column(Integer, ForeignKey('TestResult.id')) TestCaseID = Column(Integer, ForeignKey('TestCase.id')) TestUtils下面是设计的一个操作类： 123456789101112131415161718192021222324252627282930313233class TestResultUtils(): def __init__(self, db_sqlnet='xxx', db_name='xxx', db_user='xxx', db_password='xxx'): self.engine = create_engine('mssql+pymssql://{0}:{1}@{2}/{3}'.format(db_user, db_password, db_sqlnet, db_name)) DBSession = sessionmaker(bind=self.engine, autoflush=False) self.session = DBSession() def save_test_result(self, test_result = None, test_case_list = [], test_detail_list = []): count = self.session.query(TestResult).filter(TestResult.TestTime == test_result.TestTime).count() if(count &gt; 0): test_result = self.session.query(TestResult).filter(TestResult.TestTime == test_result.TestTime).first() if(len(test_case_list) != len(test_detail_list)): print(&quot;test case list and test detail list size is not matched.&quot;) return for index, test_detail in enumerate(test_detail_list): test_case = test_case_list[index] count = self.session.query(TestCase).filter(TestCase.TestCaseName == test_case_list[index].TestCaseName and TestCase.Rows == test_case_list[index].TestCaseName.Rows).count() if(count &gt; 0): test_case = self.session.query(TestCase).filter(TestCase.TestCaseName == test_case_list[index].TestCaseName and TestCase.Rows == test_case_list[index].TestCaseName.Rows).first() test_case_list[index] = test_case test_case.TestDetails.append(test_detail) test_result.TestDetails.append(test_detail) for test_case in test_case_list: self.session.add(test_case) self.session.add(test_result) try: self.session.commit() self.session.close() except exc.SQLAlchemyError as e: print(str(e)) 最后今天杭州的雪真的挺大的，晚安zzz 参考： 使用 SQLAlchemy cmutils_test.py DB for TestResult","link":"/2018/12/08/2018/12/2018-12-08-TestResultDBImplement/"},{"title":"Not in replacement","text":"背景最近解决了一个 hive 中报错的问题。HY 从最底层的 view 开始检查，最后发现在当前的 Sandbox/UAT2 环境中，对于 not in 中使用子查询的支持有问题。 这个问题导致了后续引用的 view 在建立时解析出错，会出现 Range Error 和 hive sql 解析出错。 解决的办法是使用别的语句替换，比如 not exists。 今天就想记录一下一些可行的替换方法，主要还是 not exists 和用 join 样例以下面简单的例子为例，有 Student 表和 Score 表， 想要得到分数不小于 60 分的名字Table-Student id name 1 cx 2 xm 3 zs 4 lx Table-Score id score 1 80 2 95.5 3 75 4 55 not in使用 not in的话，直接很好理解，小于60分的不要。 123select name from Student where id not in ( select id from Score where score &lt; 60) not exist使用not exist需要id关联在一起 123select name from Student a where not exists ( select * from Score b where score &lt; 60 and a.id = b.id) join可以直接使用反过来的逻辑使用join： 1234select name from Student a join Score bon a.id = b.id and b.score &gt;= 60 如果想要使用一样的not逻辑的话, 可以使用left join 12345select name from Student a left join Score bon a.id = b.id and b.score &lt; 60where b.id is null 不建议使用not in对null的处理不好以下面为例： 12select 'val' where 3 not in (1, 2, null); Not In可以转换为条件对于每个值进行不等比对，并用逻辑与连接起来，Null值与任意其他值做比较时，结果永远为Null，在Where条件中也就是False，因此3&lt;&gt;null就会导致不返回任何行，导致Not In子句产生的结果在意料之外 12select 'val'where 3&lt;&gt; 1 and 3 &lt;&gt;2 and 3 &lt;&gt; null; 最后的返回结果为空，什么都没有，不是期望的，如果子查询里返回的结果里有Null,那么就会导致错误。 Note: 我在SqlServer, Mysql验证过，表现一样。 Not In导致的查询性能低下Not In需要对结果中可能的Null进行判断，有更多额外的开销，具体可以参考最后的那篇文章，分析的很详细，在一般情况下，都还是使用not exists替代更好。 参考在SQL Server中为什么不建议使用Not In子查询","link":"/2020/03/22/2020/03/2020-03-22-ReplaceNotIn/"},{"title":"Pandas Data Concat","text":"背景最近生成的报表比原来的逻辑多了些字段，想要老的报表也加上，所以用 python 写了脚本去处理，需要用来一些 mapping 的数据。pandas 有许多合并数据的方法，Concat 可以横向与纵向的合并，merge 可以实现横向的合并类似 sql 中的 join，append 则是纵向的合并。 样例以下面简单的例子为例,有下面三个文件， 想要数据合并到一起。 csv-Student id name 1 CX 2 XM 3 ZS 5 BF csv-Score SID Class Score 1 语文 70 1 数学 80 1 英文 68 2 语文 87 2 数学 98 2 英文 56 3 语文 12 3 数学 43 3 英文 76 4 语文 66 4 数学 77 csv-Score2 SID Class Score 5 语文 77 5 数学 88 5 英文 66 实现Concat 支持两个方向的合并，所以我们可以先把 Score 合并在一起，然后再 left out join Student 12345678def concat_student_score(file_student, file_score, file_score2): df_stu = pd.read_csv(file_student) df_score1 = pd.read_csv(file_score) df_score2 = pd.read_csv(file_score2) df_score = pd.concat([df_score1, df_score2], ignore_index=True) # df_score = df_score1.append(df_score2, ignore_index=True) df_merged = pd.merge(df_score, df_stu, how='outer', on=['SID']) print(df_merged) SID Class Score Name 0 1 语文 70 CX 1 1 数学 80 CX 2 1 英文 68 CX 3 2 语文 87 XM 4 2 数学 98 XM 5 2 英文 56 XM 6 3 语文 12 ZS 7 3 数学 43 ZS 8 3 英文 76 ZS 9 4 语文 66 NaN 10 4 数学 77 NaN 11 4 英文 88 NaN 12 5 语文 77 BF 13 5 数学 88 BF 14 5 英文 66 BF 以上是最简单的应用，还有许多的参数可以控制合并的细节，可以参考下面的资料，尤其是最后的官方的与 sql 的对比应用，大部分 sql 能实现的，pandas 都可以 demo05 参考 python3：pandas（合并 concat 和 merge） PANDAS 数据合并与重塑（join/merge 篇） Comparison with SQL","link":"/2020/05/10/2020/05/2020-05-10-PandasMerge/"},{"title":"My Diary 2020-09-06","text":"COVID-192020真的很特殊，年初的疫情，让整个世界陷入了混乱。 好在国内的抗疫还是卓有成效，现在都已经恢复正常的工作生活了。 但是因为美国的疫情，道富还是实行AB组的方式, 两周两周的轮换在家和在公司办公，现在大家都习惯了。 如果国内的asg site也建立起来的话，那在家办公也就会很流畅了。 不得不说这场疫情给改变了我们的很多生活方式。 Work快到两年了，但到现在为止的感觉还是很不好，一开始来这边就是一个错误。 当时发现现实的情况与自己的预期相差非常多，并没有及时的止损，觉得是自己心态的问题，还是想融入新的角色与工作。 另一个角度，是我没有给自己留后路。当时是想着不顺利的话，就回老家好了，母亲一个人在老家生活，而我杭漂，真的一点意义也没有。 但往往事情的发展并不会与预期的一样，受困于当时的焦虑与抑郁，我没有办法做出决定，后来有了小曼，我便也没有了回家这个选项。 从工作性质与工作角色上来说，变化的太多。最主要的是，项目的核心业务逻辑在美国那边，即使到现在，还没有非常清晰的认识。 刚好也是从过年后开始，做Hadoop这块的东西，也是自己带头从无到有从印度团队那么KT过来。 有时候还是觉得非常的累，需要和多个团队打交道，而自己的英文水平还不够好。 很久没有体验过那种对工作的极度排斥，但又没有办法，硬着头皮上的感觉，真的太不舒服了。 内心的焦虑与压力不是一般的大，明显感觉不是那么好的工作状态，直到现在。 除了工作内容本身，因为与YL的同学关系，有一些人有想法，还是能感受到的。 有时候觉得自己太天真了，以为道富内部的组都是比较和谐，大家都比较nice的，只不过自己以前遇到的都是很kind的人。 所以有时候看她们的样子，还是觉得挺搞笑的，至于么。 想起TZ和ZP的话，的确一起工作的人比工作内容本身重要的多了，良好的合作关系真的很让人舒心。 有些话没有办法说，都是自己的选择，认赌服输，苦果只能自己咽。 还是要好好改改自己的臭毛病。 All in all, 做好自己能做的事，硬着头皮撑着，相信总有一天会有转机的。 Family外婆前段时间摔了，母亲照顾的很辛苦，心情也一直不好，好在现在外婆恢复的还可以，她也放松多了。 XM换了工作，毕竟在一个组不太合适。我们现在换了新的地方，更有自己家的感觉了。 但两个人还是有些冲突和矛盾。 尤其最近买房的事情，也怪自己吧，太傻了，没有给力的主导，现在事情变得很奇怪，也只能走一步看一步了。 或许真的是我不够好吧，想要结婚的想法是奢侈的。 Life自从换组以来，很久很久没有真正的放松过了，如果是必经的，那我也只能承受下来。 人生是不能逃避的。 不过话说回来，健康是除一切之外，最最最重要的。 这两年胖了好多。我的体重也是我的身心健康的晴雨表，过分的肥胖就表明我自己有问题了。 加油吧，快30岁的人！！！哈哈哈！","link":"/2020/09/06/2020/09/2020-09-06-MyDiary/"},{"title":"Query hive data","text":"背景有Hue可以用来查询hive和impala的数据，但是使用起来不是特别方便，尤其想要同时把数据导出来的时候。 原来想尝试用java的方式，也向同事要了demo，但是需要keytab，而且还是倾向于用python。 最后尝试了一些python库失败(主要是这些module依赖的东西比较多，需要很多环境的配置，但是都没有权限，比较麻烦)，从同事那得知他们的Tableau本地用的是odbc的连接方式。 之前试过用pyodbc去连接sql server数据库，而且理论上使用odbc，只要数据源配置好就可以了，api的使用是共通的，不管什么语言都可以用odbc的方式去访问。 第三方软件安装 Cloudera hive odbc (用使用impala的话就安装 impala odbc)， 并配置好连接与数据源 MIT Kerberos (需要使用Kerberos认证) 主界面 yaml是用来配置基本信息，像最后文件输出的目录也可以配置，当前默认output，lib放了一些自己封装的类和方法. 主界面可以看到可以选择不同的数据源（在配置文件里配置初始的数据），输入最后要存的文件名字（默认是test.{current_datetime}.csv)，输出的文件类型(csv/excel)，以及你的sql语句。 为了更灵活，当start之前，你可以更新信息，重新query，即支持多线程查询，不过不建议一次查太多，而且要注意输出文件的名字，如果重名的话数据就会被覆盖。 主要查询的代码如下图所示，封装了HiveUtils做一些细节处理。 查询多个table和view上面的工具满足了特殊的sql的需要，但是有时候，我们希望一次能把许多table/view的数据导出来查看。 所以也写了另外的脚本，通过把想要导出来的table/view名字列在配置文件里，一次性起多个线程去查询。 e.g. object.txt 1234db1.table1db1.table2db2.view1db3.view2 把上面的所以数据都查出来，最后放在output目录中，文件名按table/view名字来取，方便查看。 最后后续会继续改进，提高平时的工作效率。","link":"/2020/10/21/2020/10/2020-10-21-QueryPyODBC/"},{"title":"HDFS Space Usage","text":"背景需要统计HDFS的空间使用情况，有命令hadoop fs -du /path支持查看(hdfs dfs等效)。 但这样子只能看一个目录的情况，如果想要遍历所有的目录没有直接支持的。 方案默认-du 是查看所给目录下子目录的占用空间大小，而加上-s则是列出给定的目录所占的空间。所以如果给定所有已知的目录，那么就可以使用类似下面的语法 1hadoop fs -du -s /path1 /path2 /path3 所以我们可以想办法先得到想要的目录,通过-ls -R遍历所有目录和文件: 123456789101112131415161718[root@hadoop00 ~]# hadoop fs -ls -R /drwxr-xr-x - root supergroup 0 2020-06-22 23:25 /cxdrwxr-xr-x - root supergroup 0 2020-03-01 21:25 /cx/test-rw-r--r-- 3 root supergroup 66 2020-03-01 21:25 /cx/test/salary.csvdrwxr-xr-x - root supergroup 0 2020-06-22 23:28 /cx/test2-rw-r--r-- 3 root supergroup 879999912 2020-06-22 23:28 /cx/test2/data.csvdrwxr-xr-x - root supergroup 0 2019-02-17 21:23 /datadrwxr-xr-x - root supergroup 0 2019-02-17 21:25 /data/input-rw-r--r-- 3 root supergroup 68 2019-02-17 21:25 /data/input/word-count-data.txtdrwxr-xr-x - root supergroup 0 2019-02-17 21:23 /data/outputdrwxr-xr-x - root supergroup 0 2019-02-17 22:10 /folder1-rw-r--r-- 3 root supergroup 20 2019-02-17 22:10 /folder1/test.txtdrwx-wx-wx - root supergroup 0 2020-03-01 23:20 /userdrwx-wx-wx - root supergroup 0 2020-03-01 23:27 /user/hivedrwx-wx-wx - root supergroup 0 2020-03-01 23:20 /user/hive/tmpdrwx--x--x - root supergroup 0 2020-03-01 23:51 /user/hive/tmp/rootdrwx-wx-wx - root supergroup 0 2020-03-01 23:27 /user/hive/warehousedrwx-wx-wx - root supergroup 0 2020-03-01 23:30 /user/hive/warehouse/myhive.db 之后通过grep ^d筛选出目录，再使用awk去掉不想要的值(带hive)。 123456789[root@hadoop00 ~]# hdfs dfs -ls -R / | grep ^d | awk '$8!~/.*hive.*/ {print $8}'/cx/cx/test/cx/test2/data/data/input/data/output/folder1/user 最后，把上面的结果作为参数传回给-du命令, 可以得到想要的结果。 123456789[root@hadoop00 ~]# hdfs dfs -du -h -s `hdfs dfs -ls -R / | grep ^d | awk '$8!~/.*hive.*/ {print $8}'`839.2 M /cx66 /cx/test839.2 M /cx/test268 /data68 /data/input0 /data/output20 /folder10 /user -h是把size转成好理解的格式。 参考 Hadoop fs -du -s -h 输出三列数据的含义 Linux awk 命令 Linux三剑客之awk命令","link":"/2020/06/22/2020/06/2020-06-22-CheckHdfsUsage/"},{"title":"Cut and Combine Tiktok Videos","text":"更新 date update 2021-02-17 1. 修复了不同视频大小的压缩算法 2. 修改代码结构并支持并发预处理文件 2021-02-21 1. 调整代码结构，封装到类中 2. 优化对文件夹的批处理 3. 视频最后合成的分辨率由视频本身自动决定 背景最近看到tiktok上有许多有意思的视频，所以想下载下来。但下过来的视频会有水印，主要是视频后3秒会有抖音的视频水印，很影响观感。 对于文字水印，需要用别的方式来下载，不太方便，后面可以再想办法。 现在的需求是：我在手机上下了n个视频，copy到电脑上之后，希望能去掉片尾的视频水印，并合并成一个大的视频。 功能将当前目录下，所以的mp4文件的后3秒（视频水印）去掉，然后合成一个大的视频文件。 主要代码在网上找了许多资料，最后决定使用moviepy。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import imageioimport win_unicode_consolewin_unicode_console.enable()import osfrom moviepy.video.io.VideoFileClip import VideoFileClipfrom moviepy.video.compositing.concatenate import concatenate_videoclipsfrom moviepy.editor import VideoFileClip, clips_array, vfx, CompositeVideoClipimport globif __name__==&quot;__main__&quot;: output_folder = './output' files = glob.glob('**/*.mp4', recursive=True) video_list = [] start_sec = 0 for file in files: try: source = file target = os.path.join(output_folder, source) #拼接文件名路径 if not os.path.exists(os.path.dirname(target)): os.makedirs(os.path.dirname(target)) video = VideoFileClip(source) total_seconds = video.duration start_time = 0 stop_time = total_seconds - 3 video = video.subclip(int(start_time), int(stop_time))#执行剪切操作 video.to_videofile(target, fps=20, remove_temp=True)#输出文件 # os.remove(source) print(video.size[0],video.size[1]) print(video.size[0]/1300,video.size[1]/720) rate_x = video.size[0]/1300 rate_y = video.size[1]/720 rate_max = max(rate_x, rate_y) if rate_max &gt; 1: rate_max = 1/rate_max else: rate_max = 1 video = video.set_start(start_sec).set_pos(&quot;center&quot;).resize(rate_max) print('-=====&gt;', rate_max) start_sec = start_sec + video.duration video_list.append(video)#将加载完后的视频加入列表 except Exception as e: print('have error:',e) finally: print(file, 'done') final_clip = CompositeVideoClip(video_list, size=(1300, 720)) final_clip.to_videofile(os.path.join(output_folder, 'combined.mp4'), fps=20, remove_temp=True) # final_clip = concatenate_videoclips(video_list)#进行视频合并 # final_clip.write_videofile(os.path.join(output_folder, 'combined.mp4'), fps=20, remove_temp=True) # final_clip.to_videofile(os.path.join(output_folder, 'combined.mp4'), fps=20, remove_temp=True)#将合并后的视频输出 合并成一个视频这边对于每个新的视频，都重新设置位置和size，主要是为了支持后面合并不同分辨率做准备。 123video = video.set_start(start_sec).set_pos(&quot;center&quot;).resize(video.size[0]/1300)start_sec = start_sec + video.durationvideo_list.append(video)#将加载完后的视频加入列表 这边使用CompositeVideoClip来合并视频，而上面的start_sec，便是每个视频在合并的视频中，开始播放的时间。如果不设置，就是一所有视频都在0s开始播放，大家可以想到，如果这个时间配置视频的位置，就可以达到同时放多个小视频的效果，而这里使用它，纯粹是为了支持合并多个不同的分辨率 12final_clip = CompositeVideoClip(video_list, size=(1300, 720))final_clip.to_videofile(os.path.join(output_folder, 'combined.mp4'), fps=20, remove_temp=True) 2021-02-17更新修复了不同视频大小的压缩算法之前在合并不同大小的视频时，对于合并的算法有问题，不能适应所有的情况。 更新后如下，根据长和宽，适合最合适的缩放大小。 12345678rate_x = video.size[0]/1300rate_y = video.size[1]/720rate_max = max(rate_x, rate_y)if rate_max &gt; 1: rate_max = 1/rate_maxelse: rate_max = 1VIDEO_LIST[i] = video.set_start(start_sec).set_pos(&quot;center&quot;).resize(rate_max) 修改代码结构并支持并发预处理文件使用线程池来实现并发操作，充分使用电脑性能来做数据预处理。 1234executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)files = glob.glob('**/*.mp4', recursive=True)all_task = [executor.submit(convert_video, (file)) for file in files]wait(all_task, return_when=ALL_COMPLETED) 完整新代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import imageioimport win_unicode_consolewin_unicode_console.enable()import osfrom moviepy.video.io.VideoFileClip import VideoFileClipfrom moviepy.video.compositing.concatenate import concatenate_videoclipsfrom moviepy.editor import VideoFileClip, clips_array, vfx, CompositeVideoClipimport globfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETEDfrom datetime import datetimeOUTPUT_FOLDER = './output'VIDEO_LIST = []MAX_WORKERS = 6def convert_video(file): try: target = os.path.join(OUTPUT_FOLDER, file) # 拼接文件名路径 try: if not os.path.exists(os.path.dirname(target)): os.makedirs(os.path.dirname(target)) except Exception as e: print('have error when create subfolder:',e) video = VideoFileClip(file) total_seconds = video.duration start_time = 0 stop_time = total_seconds - 3 video = video.subclip(int(start_time), int(stop_time))# 执行剪切操作 video.to_videofile(target, fps=20, remove_temp=True)# 输出文件 # os.remove(source) VIDEO_LIST.append(video)# 将加载完后的视频加入列表 except Exception as e: print('have error:',e) finally: print(file, 'done')def combine_videos(): start_sec = 0 for i, video in enumerate(VIDEO_LIST): # print(video.size[0],video.size[1]) # print(video.size[0]/1300,video.size[1]/720) rate_x = video.size[0]/1300 rate_y = video.size[1]/720 rate_max = max(rate_x, rate_y) if rate_max &gt; 1: rate_max = 1/rate_max else: rate_max = 1 VIDEO_LIST[i] = video.set_start(start_sec).set_pos(&quot;center&quot;).resize(rate_max) start_sec = start_sec + video.duration final_clip = CompositeVideoClip(VIDEO_LIST, size=(1300, 720)) final_clip.to_videofile(os.path.join(OUTPUT_FOLDER, 'combined.mp4'), fps=20, remove_temp=True)if __name__==&quot;__main__&quot;: start_time = datetime.now() executor = ThreadPoolExecutor(max_workers=MAX_WORKERS) files = glob.glob('**/*.mp4', recursive=True) all_task = [executor.submit(convert_video, (file)) for file in files] wait(all_task, return_when=ALL_COMPLETED) combine_videos() ended_time = datetime.now() print(f'time cost: {ended_time - start_time}') 2021-02-21更新最后生成的视频分辨率由视频动态决定遍历所有视频时，记录下长和宽，最后取max来当作最后的分辨率 12345self.max_x_list = []self.max_y_list = []self.max_x = max(self.max_x_list)self.max_y = max(self.max_y_list) 根据目录分别生成合成的视频可以一次性按顺序按目录合成视频，而不是之前的所有目录的视频，更加灵活。 123456789files = glob.glob('**/*.mp4', recursive=True)if not files: print('no files found') exit(0)dirs = list(set(['.' if os.path.dirname(file)=='' else os.path.dirname(file) for file in files]))for dir in dirs: tiktok_util = TiktokUtil(input_folder=dir) tiktok_util.preprocess_videos() tiktok_util.combine_videos() 封装到类中下面是完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import imageioimport win_unicode_consolewin_unicode_console.enable()import osfrom moviepy.video.io.VideoFileClip import VideoFileClipfrom moviepy.video.compositing.concatenate import concatenate_videoclipsfrom moviepy.editor import VideoFileClip, clips_array, vfx, CompositeVideoClipimport globfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETEDfrom datetime import datetime&quot;&quot;&quot;author: bearfly1990create at: 02/01/2021description: Utils for send emailChange log:Date Author Version Description02/17/2021 bearfly1990 1.0.1 update combine video hight/width resize logic to adapt all videos.02/21/2021 bearfly1990 1.0.2 Get max hight/max width from all the input videos, not hard code support detail with different folder.&quot;&quot;&quot;class TiktokUtil(object): output_folder = './output' max_workers = 6 def __init__(self, remove_watermark=True, input_folder='', output_name='combined.mp4'): self.remove_watermark = remove_watermark self.output_name = output_name self.input_folder=input_folder self.video_list = [] self.max_x_list = [] self.max_y_list = [] def convert_video(self, file): try: target = os.path.join(self.output_folder, file) try: if not os.path.exists(os.path.dirname(target)): os.makedirs(os.path.dirname(target)) except Exception as e: print('have error when create subfolder:',e) video = VideoFileClip(file) if self.remove_watermark: total_seconds = video.duration start_time = 0 stop_time = total_seconds - 3 video = video.subclip(int(start_time), int(stop_time)) self.max_x_list.append(video.size[0]) self.max_y_list.append(video.size[1]) self.video_list.append(video) except Exception as e: print('have error:',e) finally: print(file, 'done') def combine_videos(self): self.max_x = max(self.max_x_list) self.max_y = max(self.max_y_list) start_sec = 0 for i, video in enumerate(self.video_list): rate_x = video.size[0]/self.max_x rate_y = video.size[1]/self.max_y rate_max = max(rate_x, rate_y) if rate_max &gt; 1: rate_max = 1/rate_max else: rate_max = 1 self.video_list[i] = video.set_start(start_sec).set_pos(&quot;center&quot;).resize(rate_max) start_sec = start_sec + video.duration print(self.max_x, self.max_y) final_clip = CompositeVideoClip(self.video_list, size=(self.max_x, self.max_y)) final_clip.to_videofile(os.path.join(self.output_folder, self.input_folder, self.output_name), fps=20, remove_temp=True) final_clip.close() def preprocess_videos(self): files = glob.glob(f'{self.input_folder}/*.mp4', recursive=True) executor = ThreadPoolExecutor(max_workers=self.max_workers) all_task = [executor.submit(self.convert_video, (file)) for file in files] wait(all_task, return_when=ALL_COMPLETED) if __name__==&quot;__main__&quot;: start_time = datetime.now() files = glob.glob('**/*.mp4', recursive=True) if not files: print('no files found') exit(0) dirs = list(set(['.' if os.path.dirname(file)=='' else os.path.dirname(file) for file in files])) for dir in dirs: tiktok_util = TiktokUtil(input_folder=dir) tiktok_util.preprocess_videos() tiktok_util.combine_videos() ended_time = datetime.now() print(f'time cost: {ended_time - start_time}') 最后完整代码在moviepy 参考： 使用Python+moviepy连接不同尺寸的视频文件 MoviePy不同尺寸视频vedio_clip或者图片image_clip拼接出现花屏 MoviePy问题解决汇总 MoviePy - 中文文档2-快速上手-MoviePy-视频合成 python线程池 ThreadPoolExecutor 使用详解","link":"/2021/02/09/2021/02/2021-02-09-CutCombineTiktok/"},{"title":"Pandas + Openpyxl process excel","text":"更新 date update 2021-03-11 1. Initial 背景最近帮同学写了一个数据处理的小程序，用python开发，用来处理excel数据。 其实excel本身就能实现大部分功能，并不一定要用程序处理，但用代码的话更灵活一些。 功能数据本身不能share，但是计算逻辑应该无所谓，所以代码已经放在haredata。 程序主要用到了python库pandas + openpyxl，写了非常简单的ui界面来选择输入的文件，处理完之后数据输出写入到新的文件里。 为了使用方便，最后打包成了exe。 UI 在做上面的界面的时候，遇到一个问题，就是需要给界面上加图标， 就是上面左上角的钥匙孔。 在打包exe的时候，需要把图片也打包进去，不然就需要带着额外的数据文件，不能放在一个exe中了，会比较挫。 Pyinstaller 使用+打包图片方法 Pyinstaller如何将资源文件一起打包至exe中 上面是网上找到的比较好的两个方法，第一个方法只适合图片，有点绕。 第二个方法能直接解决打包数据文件的问题，可以参考一下。 两种方法都是用临时文件，只不过第二种方法更方便一些，使用系统的临时文件夹，不用自己操心。 Pandas计算在数据处理过程中，用到了pandas的一些数值聚合计算 12345logger.info('计算n...')self.df_sheet_weight['n'] = self.df_sheet_weight.apply(lambda x: self.n(x), axis=1)logger.info('计算n2...')self.df_sheet_weight['n2'] = self.df_sheet_weight.apply(lambda x: self.n2(x), axis=1) 123456789101112def n2(self, x): return self.df_sheet_weight.groupby(['DSDM']).size()[x['DSDM']]def c1_county(self, x): return self.df_sheet_weight.groupby(['QXDM'])['C1/WA'].sum()[x['QXDM']] / x['n']def c1_city(self, x): return self.df_sheet_weight.groupby(['DSDM'])['C1/WA2'].sum()[x['DSDM']] / x['n2']def c1_pro(self, x): # C1pro return self.df_sheet_weight['C1/WA3'].sum() / x['n3'] 单元格长度自适应因为数值比较多，所以添加了cell长度自适应，因为中文字符是英文的两倍，也添加了判断。 12345678910def auto_width(cls, sheet): for i in range(1, sheet.max_column + 1): max_width = 13 for j in range(1, sheet.max_row + 1): cell = f'{OpenpyxlHelper.get_column_letter_from_index(i)}{j}' value_width = 0.7 * len(re.findall('([\\u4e00-\\u9fa5])', str(sheet[cell].value))) + len( str(sheet[cell].value)) if value_width &gt; max_width: max_width = value_width sheet.column_dimensions[OpenpyxlHelper.get_column_letter_from_index(i)].width = max_width + 2 Openpyxl给单元格赋值及应用样式下面是一个例子： 123456789101112131415161718def write_summary(sheet_summary, df_summary, row_start, col_start_char, diff): col_start_char = OpenpyxlHelper.get_column_letter_from_str_by_diff(col_start_char, diff) row_current = row_start for index, row in df_summary.iterrows(): for i, crop in enumerate(CROPS_LIST): cell = sheet_summary.cell(row=row_current, column=OpenpyxlHelper.get_column_index_from_str(col_start_char) + i) cell.value = row[i] cell.number_format = '#,##0.00' cell = sheet_summary.cell( row=row_current, column=OpenpyxlHelper.get_column_index_from_str_by_diff(col_start_char, -1) ) start_cell = f'{col_start_char}{row_current}' ended_cell = f'{OpenpyxlHelper.get_column_letter_from_str_by_diff(col_start_char, len(CROPS_LIST) - 1)}{row_current}' cell.value = f'=sum({start_cell}:{ended_cell})' cell.number_format = '#,##0.00' row_current = row_current + 1 创建了一个辅助类来控制样式和列名的index与letter转换： 12345678910111213141516171819202122class OpenpyxlHelper: align_left = Alignment(horizontal='left', vertical='center', wrap_text=True) align_right = Alignment(horizontal='right', vertical='center', wrap_text=True) def __init__(self): pass @classmethod def get_column_letter_from_index(cls, val_int): return get_column_letter(val_int) @classmethod def get_column_index_from_str(cls, val_str): return column_index_from_string(val_str) @classmethod def get_column_letter_from_str_by_diff(cls, val_str, diff): return cls.get_column_letter_from_index(cls.get_column_index_from_str(val_str) + diff) @classmethod def get_column_index_from_str_by_diff(cls, val_str, diff): return cls.get_column_index_from_str(val_str) + diff 最后内容和细节还有很多，完整代码在haredata 参考： Pyinstaller 使用+打包图片方法 Pyinstaller如何将资源文件一起打包至exe中 PYTHON模块openpyxl在导出EXCEL文件时设置自动列宽","link":"/2021/03/11/2021/03/2021-03-11-HareData/"},{"title":"Folder Tree Info DFS&#x2F;BFS","text":"深度优先与广度优先遍历文件（夹）信息最近做了一个对系统目录进行管理的页面，涉及到了对目录中的文件夹和文件的信息读取的问题，刚好回顾了一下文件夹遍历的算法。 由于需要在页面中体现目录层级关系，需要返回的json中有parent_id这样标志符。当时第一个想法就是用递归的方式来遍历，就可以得到对应的关系，但是这样的话，同一目录的文件（夹）的id就不能连续了，如果使用广度优先遍历的话就可以解决这个问题。 目录结构新建了一个测试的目录，放了一些文件： 下面是更直观的文件的关系示例： Depth-First-Search(DFS)使用递归是最直接的深度优先搜索的方式，直接贴代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041import osdef iterate_folder_files_info_dfs(current_folder, file_list, root_id, level): level = level + 1 current_id = root_id + 1 files_in_folder = os.listdir(current_folder) for file_name in files_in_folder: file_full_path = os.path.join(current_folder, file_name) if os.path.isfile(file_full_path): new_file_info = { 'id': current_id, 'parent_id': root_id, 'level': level, 'type': 'file', 'folder_name':current_folder, 'file_name':file_name, 'full_path':file_full_path, 'file_size':os.path.getsize(file_full_path), 'create_time':os.path.getctime(file_full_path) } file_list.append(new_file_info) current_id = current_id + 1 elif os.path.isdir(file_full_path): new_folder_info = { 'id': current_id, 'parent_id': root_id, 'level': level, 'type': 'folder', 'folder_name':current_folder, 'file_name':file_name, 'full_path':file_full_path, 'create_time':os.path.getctime(file_full_path) } file_list.append(new_folder_info) current_id = iterate_folder_files_info_dfs(file_full_path, file_list, current_id, level) return current_idfile_list = []current_folder = r'C:\\test'iterate_folder_files_info_dfs(current_folder, file_list, -1, 0)with open('result.dfs.json','w+') as f: f.write(str(file_list)) 逻辑还是比较直观的，一旦搜索到的是文件，就直接把信息加到最后的list中。如果是目录，把信息加到最后的list中后，继续向下调用搜索，只要最后一层没有目录了，就逐级向上返回自己的id。 最后生成的id如下图所示：可以看到ID的分布是按上到下生成（按深度）生成的，下面是最后生成的json, parent_id也没有问题。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108[{ 'id': 0, 'parent_id': -1, 'level': 1, 'type': 'folder', 'folder_name': 'C:\\\\test', 'file_name': 'classes', 'full_path': 'C:\\\\test\\\\classes', 'create_time': 1625323109.1690984 }, { 'id': 1, 'parent_id': 0, 'level': 2, 'type': 'file', 'folder_name': 'C:\\\\test\\\\classes', 'file_name': 'class_introduce.txt', 'full_path': 'C:\\\\test\\\\classes\\\\class_introduce.txt', 'file_size': 23, 'create_time': 1625323181.7119198 }, { 'id': 2, 'parent_id': -1, 'level': 1, 'type': 'folder', 'folder_name': 'C:\\\\test', 'file_name': 'scores', 'full_path': 'C:\\\\test\\\\scores', 'create_time': 1625323098.449873 }, { 'id': 3, 'parent_id': 2, 'level': 2, 'type': 'folder', 'folder_name': 'C:\\\\test\\\\scores', 'file_name': 'english', 'full_path': 'C:\\\\test\\\\scores\\\\english', 'create_time': 1625323396.2220247 }, { 'id': 4, 'parent_id': 3, 'level': 3, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\english', 'file_name': 'english1.txt', 'full_path': 'C:\\\\test\\\\scores\\\\english\\\\english1.txt', 'file_size': 23, 'create_time': 1625323470.866679 }, { 'id': 5, 'parent_id': 3, 'level': 3, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\english', 'file_name': 'english2.txt', 'full_path': 'C:\\\\test\\\\scores\\\\english\\\\english2.txt', 'file_size': 55, 'create_time': 1625323470.8686738 }, { 'id': 6, 'parent_id': 2, 'level': 2, 'type': 'folder', 'folder_name': 'C:\\\\test\\\\scores', 'file_name': 'math', 'full_path': 'C:\\\\test\\\\scores\\\\math', 'create_time': 1625323316.5491896 }, { 'id': 7, 'parent_id': 6, 'level': 3, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\math', 'file_name': 'math1.txt', 'full_path': 'C:\\\\test\\\\scores\\\\math\\\\math1.txt', 'file_size': 33, 'create_time': 1625323454.2487762 }, { 'id': 8, 'parent_id': 6, 'level': 3, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\math', 'file_name': 'math2.txt', 'full_path': 'C:\\\\test\\\\scores\\\\math\\\\math2.txt', 'file_size': 31, 'create_time': 1625323460.5313218 }, { 'id': 9, 'parent_id': 2, 'level': 2, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores', 'file_name': 'score_introduce.txt', 'full_path': 'C:\\\\test\\\\scores\\\\score_introduce.txt', 'file_size': 26, 'create_time': 1625323421.2782009 }, { 'id': 10, 'parent_id': -1, 'level': 1, 'type': 'file', 'folder_name': 'C:\\\\test', 'file_name': 'summary.txt', 'full_path': 'C:\\\\test\\\\summary.txt', 'file_size': 20, 'create_time': 1625323064.0133853 }] Breadth-First-Search(BFS)下面是使用广度优先算法的解决方案，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import osdef iterate_folder_files_info_bfs(current_folder, file_info_list, root_id, level): level = level + 1 current_id = root_id + 1 files_in_folder = os.listdir(current_folder) search_list = [] level_files_list = [] for file_name in files_in_folder: file_full_path = os.path.join(current_folder, file_name) level_files_list.append(file_full_path) search_list.append({root_id:level_files_list}) while(search_list): level_files_map = search_list.pop(0) root_id = list(level_files_map.keys())[0] level_files_list = list(level_files_map.values())[0] for file_full_path in level_files_list: level_files_list_new = [] if os.path.isfile(file_full_path): new_file_info = { 'id': current_id, 'parent_id': root_id, 'level': level, 'type': 'file', 'folder_name':os.path.dirname(file_full_path), 'file_name':os.path.basename(file_full_path), 'full_path':file_full_path, 'file_size':os.path.getsize(file_full_path), 'create_time':os.path.getctime(file_full_path) } file_info_list.append(new_file_info) current_id = current_id + 1 elif os.path.isdir(file_full_path): new_folder_info = { 'id': current_id, 'parent_id': root_id, 'level': level, 'type': 'folder', 'folder_name':os.path.dirname(file_full_path), 'file_name':os.path.basename(file_full_path), 'full_path':file_full_path, 'create_time':os.path.getctime(file_full_path) } file_info_list.append(new_folder_info) for sub_file_name in os.listdir(file_full_path): new_file_full_path = os.path.join(file_full_path, sub_file_name) level_files_list_new.append(new_file_full_path) search_list.append({current_id:level_files_list_new}) current_id = current_id + 1 level = level +1file_info_list = []current_folder = r'C:\\test'iterate_folder_files_info_bfs(current_folder, file_info_list, -1, 0)with open('result.bfs.json','w+') as f: f.write(str(file_info_list)) 逻辑相对于DFS稍微多了一些状态的维护，初始化第一层文件和文件夹信息后，把level和名字列表组成{level:[filename, foldername…]} 的dict加入遍历状态list。然后一直循环遍历list,把level信息拿出来，一旦搜索到的是文件，就直接把信息加到最后的list中，如果是目录，把文件信息和Level重新加回list，直到search_list为空。 最后生成的id如下图所示：可以看到ID的分布是按左到右生成（按广度）生成的，下面是最后生成的json, parent_id也没有问题。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108[{ 'id': 0, 'parent_id': -1, 'level': 1, 'type': 'folder', 'folder_name': 'C:\\\\test', 'file_name': 'classes', 'full_path': 'C:\\\\test\\\\classes', 'create_time': 1625323109.1690984 }, { 'id': 1, 'parent_id': -1, 'level': 1, 'type': 'folder', 'folder_name': 'C:\\\\test', 'file_name': 'scores', 'full_path': 'C:\\\\test\\\\scores', 'create_time': 1625323098.449873 }, { 'id': 2, 'parent_id': -1, 'level': 1, 'type': 'file', 'folder_name': 'C:\\\\test', 'file_name': 'summary.txt', 'full_path': 'C:\\\\test\\\\summary.txt', 'file_size': 20, 'create_time': 1625323064.0133853 }, { 'id': 3, 'parent_id': 0, 'level': 2, 'type': 'file', 'folder_name': 'C:\\\\test\\\\classes', 'file_name': 'class_introduce.txt', 'full_path': 'C:\\\\test\\\\classes\\\\class_introduce.txt', 'file_size': 23, 'create_time': 1625323181.7119198 }, { 'id': 4, 'parent_id': 1, 'level': 3, 'type': 'folder', 'folder_name': 'C:\\\\test\\\\scores', 'file_name': 'english', 'full_path': 'C:\\\\test\\\\scores\\\\english', 'create_time': 1625323396.2220247 }, { 'id': 5, 'parent_id': 1, 'level': 3, 'type': 'folder', 'folder_name': 'C:\\\\test\\\\scores', 'file_name': 'math', 'full_path': 'C:\\\\test\\\\scores\\\\math', 'create_time': 1625323316.5491896 }, { 'id': 6, 'parent_id': 1, 'level': 3, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores', 'file_name': 'score_introduce.txt', 'full_path': 'C:\\\\test\\\\scores\\\\score_introduce.txt', 'file_size': 26, 'create_time': 1625323421.2782009 }, { 'id': 7, 'parent_id': 4, 'level': 4, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\english', 'file_name': 'english1.txt', 'full_path': 'C:\\\\test\\\\scores\\\\english\\\\english1.txt', 'file_size': 23, 'create_time': 1625323470.866679 }, { 'id': 8, 'parent_id': 4, 'level': 4, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\english', 'file_name': 'english2.txt', 'full_path': 'C:\\\\test\\\\scores\\\\english\\\\english2.txt', 'file_size': 55, 'create_time': 1625323470.8686738 }, { 'id': 9, 'parent_id': 5, 'level': 5, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\math', 'file_name': 'math1.txt', 'full_path': 'C:\\\\test\\\\scores\\\\math\\\\math1.txt', 'file_size': 33, 'create_time': 1625323454.2487762 }, { 'id': 10, 'parent_id': 5, 'level': 5, 'type': 'file', 'folder_name': 'C:\\\\test\\\\scores\\\\math', 'file_name': 'math2.txt', 'full_path': 'C:\\\\test\\\\scores\\\\math\\\\math2.txt', 'file_size': 31, 'create_time': 1625323460.5313218 }] 最后深度优先搜索可以利用递归，而广度优先则需要维护好遍历的顺序，以便同一层搜索完后再向下一层搜索。","link":"/2021/07/04/2021/07/2021-07-04-folder-tree/"},{"title":"jaydebeapi","text":"JayDeBeApi一般来说，不同的数据库针对Python都有对应的Module去访问，他们基本上都使用统一的Python DB-API,除了连接上会有点区别，别的使用方式都基本一样。 数据库 模块 mysql pymysql sql server pymssql Oracle cx_Oracle Teradata teradatasql etc… etc… 但不可避免的，有些库需要系统安装一些依赖，比如Oracle，就需要安装有对应的oralce client，cx_Oracle才能正常使用。 此外很多数据库提供odbc driver，那么就可以在安装driver之后，统一使用pyodbc去连接。在PA的写的在windows下query hive和impala数据的小工具，使用的就是pyodbc的方式。 但是当需要安装除模块本身外的依赖都会显得有些麻烦，今天主要是记录一下在python中使用jdbc(jar包)的方式连接sql server例子，使用的module就是JayDeBeApi The JayDeBeApi module allows you to connect from Python code to databases using Java JDBC. It provides a Python DB-API v2.0 to that database. It works on ordinary Python (cPython) using the JPype Java integration or on Jython to make use of the Java JDBC driver. In contrast to zxJDBC from the Jython project JayDeBeApi let’s you access a database with Jython AND Python with only minor code modifications. JayDeBeApi’s future goal is to provide a unique and fast interface to different types of JDBC-Drivers through a flexible plug-in mechanism. 安装1pip install JayDeBeApi 使用针对不同的数据库，保证连接的字符串、注册的drvier class和对应的Jar包都对应的上就行。 这边我本地的sql server 2017的版本，所以我去官网就下了mssql-jdbc-7.4.1.jre8.jar. 12345678910111213import jaydebeapiurl = 'jdbc:sqlserver://localhost:1433;databasename=BFTest'user = 'xiche'password = 'xxxxxx'dirver = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'jar = './lib/mssql-jdbc-7.4.1.jre8.jar'sql = 'select * from Test_Output'conn = jaydebeapi.connect(dirver, url, [user, password], jar)curs=conn.cursor()curs.execute(sql)result=curs.fetchall()print(result)curs.close() output: pandas一般我们拿到connection之后，可以直接传给pandas，让他来存储和处理数据，非常方便。 1234567891011import pandas as pdimport jaydebeapiurl = 'jdbc:sqlserver://localhost:1433;databasename=BFTest'user = 'xiche'password = 'xxxxxx'dirver = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'jar = './lib/mssql-jdbc-7.4.1.jre8.jar'sql = 'select * from Test_Output'conn = jaydebeapi.connect(dirver, url, [user, password], jar)df = pd.read_sql_query(con=conn,sql=sql)print(df) output: 最后由于Java使用的广泛性和Python本身的一些局限性，如果遇到问题时有使用Jar包的解决方案，还是比较舒服的。","link":"/2021/07/21/2021/07/2021-07-21-jaydebeapi/"},{"title":"NanHu","text":"监工今天天气还不错，好久没有出去放放风了。 到崇贤还是有点距离的，高架上堵了一会儿，半个小时才到。 总体和上次来看的时候差不多，还是要看内部的进度，还有一年呢，早着。 陈记饭馆上次来的时候，因为名字带陈字便挑了这家店，不过饭菜确实也不错，今天点了三个菜，吃的很撑。 南湖吃饱喝足，寻摸着地图上未探寻过的点，选了南湖。崇贤到南湖也不近，开了快一个小时，结果临了开到了去临安的国道上。 不过也是巧合，关了导航，随便找了个车能过的口子钻了进去，走到了村子里。 第一次看到了在树上的石榴。好多水稻好多片的玉米，真想掰几个:)不过大部分也没成熟路边还有好多好看的树花在乡间小路上，两边都是稻田，真的很美很舒服 村里的二傻子 和村花的合影 湖呢？？？由于二傻子只顾着开车，沿着湖堤看湖光山色时，忘记停下来多拍几张照片了😂 南湖周边今天还去众安绿城的南湖明月售楼部借了个厕所，高层算是真的湖景房了，住这边的话的确会比较舒服，不过周边也都还在开发，尤其是湖的东北侧。从开发商的地图中也能看到周边产业的现状和规划，未来肯定还是不错的。回程的时候，也路过了达摩院那侧（湖西南），这边的道路都好新，好几次开到还封闭着的断头路，只能掉头😂。 最后还算是比较充实的一天了吧😃","link":"/2021/07/17/2021/07/2021-07-17-NanHu/"},{"title":"box SDK for python","text":"BoxBox (http://www.box.com) 是公司在用的网盘工具，是平时保存分享资料的重要地方。最近参与的项目客户会把文件放在网盘中，给我们发送网盘路径，让我们自己去取，这里就涉及到权限的问题。今天主要试了一下用box-sdk-python的方式去上传和下载文件。 Note:今天访问的一些资源可能需要大家自己会科学上网。 Box App 配置最后代码的使用比较简单，主要是前面的一些配置需要知道一下。 首先要注册并登录box账号，默认的10G空间大小足够了。 进入到网盘后，点击左下脚的 Dev Console 这里可以新建App，到时候就是一个独立的文件存储的空间 选择Custom App 再选择JWT方式并输出App Name 然后在Configurationtab我们就能看到很多Credentials验证的信息 需要的一些权限要勾上主要点击下面的Generate a Public/Private Keypair，就会在生成密钥，并下载相关配置的json文件。 注意: 第一次用的时候需要你开启双重验证，需要手机下载Authenticator 在提示的另一个页面扫描二维码把动态口令牌导入你的手机，后面一些重要的操作都需要输入。 下面是json 配置文件的信息，一会儿连接的时候需要用到的信息都在里面。 123456789101112{ &quot;boxAppSettings&quot;: { &quot;clientID&quot;: &quot;r76ashyr9ehltsuq8mfbtbxml1ugzqxb&quot;, &quot;clientSecret&quot;: &quot;WK0Sp3T1Hvb8hVvFYScY2JSolLfIjHSi&quot;, &quot;appAuth&quot;: { &quot;publicKeyID&quot;: &quot;4885s2em&quot;, &quot;privateKey&quot;: &quot;-----BEGIN ENCRYPTED PRIVATE KEY-----\\nMIIFDjBABgkqhkiG9w0BBQ0wMzAbBgkqhkiG9w0BBQwwDgQIBQLozbR/T3oCAggA\\nMBQGCCqGSIb3DQMHBAgbdNCj60pETwSCBMjTF6vr+mqyEeSoy/x9RtRQbIBUYZnT\\n6OHkeIgnQAZbs4VADyWitrbasEOQSD5OLfk0TFYtN/O4RQ6pj/dirwrWYCXyvB8x\\nPfZ+MjzY2vAJbJle5L7dT6Eq3eZLzbluZ6IsD+KenwTpK4iZcWqgRIjTJpKOgfOl\\nmP8R0KH+vnADXllnhaAQIUnbe9fsg+sM58rciFvOGxb7slyF/xecZ8vhMH6RTDA6\\nhwC+8HtF76lZ0tZ7Lqo1ZMn3tasrEBIGhFnPDzNG0lFVzxUWfyhf61WSWcirMwV5\\nLqt2ZPiLl5wKlVHtAjLQw8ylfv9kDRhIlPTLUr50tdfP9rxGq2yrwjr9ksoTUw9k\\nK48vcp1j2nW5ekDwmhRwYbEu/hkg4t26msuOAEKiHtv2lpiEJyORinbIWJhM6BDO\\nJvr1EgAStz3WgOoVIXjm99w0LaLa8TexR+wH6bmxWR7GaEgISlxqEhDxsPfg9yZl\\noH/3ab81uAxWKUU9D/JaYlc6MN5D0OKFlJZll9Zfd/yIvU5fTYoAzNC/VmKcXqWz\\nGIj7Y6bzB7U9glsAVGkwlrN5ReI7SZb6pisDWvNyJcSZUsjOyKFcNwMWZccdKFy+\\nMg4FT/KxCtIWP+S46kkBy0RWpxKGgqtNUi/LVbw9S9GqDDrvICJiwKKQ6EGvy+8f\\nCdluDENzMBQvsY9CfoYBpbNbACSxhyEePHqitOYsVucVaTtxPyPD5CYU7ZeB5XBk\\nN0Ygq33jS3Exnp3b5eEArLm14L/ajmpitizhtuRIhHkHLeuP1bXGwq2ZofD4wSY9\\nQw7u6uN+v6q8qRccZODSip6WsYcNzlF7E4C8Ubk7om1sniXN3Y8cPpitp8zClWun\\nnQKroK9rUyw7mVTYEse8syeL/pMvSwXnkmWtppr2mMuCZ3R2MAzXLkaIMnk0yPuf\\n70J9rnYOV+l3npyeGcVz0C+rograaXMaYH3ZdSe6vIJqISfibzowCzJuZDAkJCl7\\nCbt5lWEAm4dhSTwYDjdA2putodh22olgFbcsm7VMOcs3+N/eQQ501NzAYlVLFgfK\\n9LLtNVySjuEOaH4xSzdnIjUIquHZMlVjivxyeG+z6EdVNUYYXapzwo66al2IDu0e\\nHBF+qE59MbJyZW6l7HeLOhPtB8+LqJ46h8ngSy0clCep6VNvCAcxVGGBeVx2TrGr\\nwxUs/MSgONArAB+D/zA8IR/o9+uIPkJWVvImAEc9oGNxfmzPnzpS7756gHe3+Oqt\\nttgxGP0Jlf0iPvaU1NDM/cJxw0kliwOad50+9QdufuZTuvxEbejxibwLNir/U3NB\\n9sRFpdvOu+U3UocmfYy8jXdGTVWczk6vk/vB9WHnARTctPLAwuLyFo2jVZgjb5u3\\nDzoYH7jNseI8s/vz/M0KUTt1dhNM0/nCdkXia7CAeC9L671Z5OaTUawuyvYqsrJt\\nqOfHdOUraXaE9pESThjF9G5qlqZwuQlHv2sRpwTOOxrqFWveOWGEeIC8YaGYgidi\\nX3f2lNY8BozAueANEwwYIkTeU7MMQ3diCC84J1+lk93DmeIrqfHRjap9Lu6ckF2a\\nYITCOdpMT23GfJcRTT2MfW5Sy5nY2S2uh2L6zJwLWi/hO5xoRWEMwhvJC91mQmMq\\neJs=\\n-----END ENCRYPTED PRIVATE KEY-----\\n&quot;, &quot;passphrase&quot;: &quot;e95a0532aeccf28116ee3be84761297d&quot; } }, &quot;enterpriseID&quot;: &quot;836874028&quot;} 下面还有最重要的一步，需要开放这个App的访问权限，不然也还是没有办法使用。 在提交了请求之后，注册的邮件中就能收到request点击Review App之后，就跳转到Review页面，点击Authorize再回到TestDemo就是Enable的状态了。 Box SDK PythonBox为各种语言都提供了访问的SDK API，python的话有box-python-sdk 这里有文档详细的说明了怎么操作网盘https://github.com/box/box-python-sdk/tree/main/docs/usage 下面是主要的简单演示: 我们首先要把json中的rsa private key单独放.pem文件中e.g.rsa_private_key.cxdemo.pem 注意：这里需要把json文件中的\\n字符串替换成真正的换行符。 如下： 123456789101112131415161718192021222324252627282930-----BEGIN ENCRYPTED PRIVATE KEY-----MIIFDjBABgkqhkiG9w0BBQ0wMzAbBgkqhkiG9w0BBQwwDgQIBQLozbR/T3oCAggAMBQGCCqGSIb3DQMHBAgbdNCj60pETwSCBMjTF6vr+mqyEeSoy/x9RtRQbIBUYZnT6OHkeIgnQAZbs4VADyWitrbasEOQSD5OLfk0TFYtN/O4RQ6pj/dirwrWYCXyvB8xPfZ+MjzY2vAJbJle5L7dT6Eq3eZLzbluZ6IsD+KenwTpK4iZcWqgRIjTJpKOgfOlmP8R0KH+vnADXllnhaAQIUnbe9fsg+sM58rciFvOGxb7slyF/xecZ8vhMH6RTDA6hwC+8HtF76lZ0tZ7Lqo1ZMn3tasrEBIGhFnPDzNG0lFVzxUWfyhf61WSWcirMwV5Lqt2ZPiLl5wKlVHtAjLQw8ylfv9kDRhIlPTLUr50tdfP9rxGq2yrwjr9ksoTUw9kK48vcp1j2nW5ekDwmhRwYbEu/hkg4t26msuOAEKiHtv2lpiEJyORinbIWJhM6BDOJvr1EgAStz3WgOoVIXjm99w0LaLa8TexR+wH6bmxWR7GaEgISlxqEhDxsPfg9yZloH/3ab81uAxWKUU9D/JaYlc6MN5D0OKFlJZll9Zfd/yIvU5fTYoAzNC/VmKcXqWzGIj7Y6bzB7U9glsAVGkwlrN5ReI7SZb6pisDWvNyJcSZUsjOyKFcNwMWZccdKFy+Mg4FT/KxCtIWP+S46kkBy0RWpxKGgqtNUi/LVbw9S9GqDDrvICJiwKKQ6EGvy+8fCdluDENzMBQvsY9CfoYBpbNbACSxhyEePHqitOYsVucVaTtxPyPD5CYU7ZeB5XBkN0Ygq33jS3Exnp3b5eEArLm14L/ajmpitizhtuRIhHkHLeuP1bXGwq2ZofD4wSY9Qw7u6uN+v6q8qRccZODSip6WsYcNzlF7E4C8Ubk7om1sniXN3Y8cPpitp8zClWunnQKroK9rUyw7mVTYEse8syeL/pMvSwXnkmWtppr2mMuCZ3R2MAzXLkaIMnk0yPuf70J9rnYOV+l3npyeGcVz0C+rograaXMaYH3ZdSe6vIJqISfibzowCzJuZDAkJCl7Cbt5lWEAm4dhSTwYDjdA2putodh22olgFbcsm7VMOcs3+N/eQQ501NzAYlVLFgfK9LLtNVySjuEOaH4xSzdnIjUIquHZMlVjivxyeG+z6EdVNUYYXapzwo66al2IDu0eHBF+qE59MbJyZW6l7HeLOhPtB8+LqJ46h8ngSy0clCep6VNvCAcxVGGBeVx2TrGrwxUs/MSgONArAB+D/zA8IR/o9+uIPkJWVvImAEc9oGNxfmzPnzpS7756gHe3+OqtttgxGP0Jlf0iPvaU1NDM/cJxw0kliwOad50+9QdufuZTuvxEbejxibwLNir/U3NB9sRFpdvOu+U3UocmfYy8jXdGTVWczk6vk/vB9WHnARTctPLAwuLyFo2jVZgjb5u3DzoYH7jNseI8s/vz/M0KUTt1dhNM0/nCdkXia7CAeC9L671Z5OaTUawuyvYqsrJtqOfHdOUraXaE9pESThjF9G5qlqZwuQlHv2sRpwTOOxrqFWveOWGEeIC8YaGYgidiX3f2lNY8BozAueANEwwYIkTeU7MMQ3diCC84J1+lk93DmeIrqfHRjap9Lu6ckF2aYITCOdpMT23GfJcRTT2MfW5Sy5nY2S2uh2L6zJwLWi/hO5xoRWEMwhvJC91mQmMqeJs=-----END ENCRYPTED PRIVATE KEY----- 然后读取json中的配置，创建client. 1234567891011121314151617181920212223import jsonfrom boxsdk import JWTAuthfrom boxsdk import Clientjson_file = open ('836874028_4885s2em_config.json', &quot;r&quot;)config = json.load(json_file)#param2不能少，这里可以拿到临时的token，虽然这里我们用不到。def your_store_tokens_callback_method(token, param2): print(token)auth = JWTAuth( client_id=config['boxAppSettings']['clientID'], client_secret=config['boxAppSettings']['clientSecret'], enterprise_id=config['enterpriseID'], jwt_key_id=config['boxAppSettings']['appAuth']['publicKeyID'], rsa_private_key_file_sys_path='rsa_private_key.cxdemo.pem', rsa_private_key_passphrase=config['boxAppSettings']['appAuth']['passphrase'], store_tokens=your_store_tokens_callback_method,)access_token = auth.authenticate_instance()client = Client(auth) 使用client就可以做许多操作了。 1234567891011121314151617181920212223root_folder = client.root_folder().get()subfolder = root_folder.create_subfolder('Test')print('Created subfolder with ID {0}'.format(subfolder.id))# subfolder = client.folder('0').create_subfolder('Test')subfolder = root_folder.create_subfolder('Test2')print('Created subfolder with ID {0}'.format(subfolder.id))subfolder = root_folder.create_subfolder('Test3')print('Created subfolder with ID {0}'.format(subfolder.id))client.folder(folder_id=f'{subfolder.id}').delete()uploade_file = client.folder('0').upload('./test.txt')print('File &quot;{0}&quot; uploaded to Box with file ID {1}'.format(uploade_file.name, uploade_file.id))# Write the Box file contents to diskoutput_file = open('./download.txt', 'wb')client.file(f'{uploade_file.id}').download_to(output_file)# items = client.folder(folder_id='0').get_items()items = root_folder.get_items()for item in items: print(f'{item.type.capitalize()} {item.id} is named &quot;{item.name}&quot;') 最后，我们可以进入到Admin Console-Content中去看我们App中的文件与目录","link":"/2021/08/12/2021/08/2021-08-12-box-python-sdk/"},{"title":"在Markdown中使用音频与视频","text":"Audio and Vedio in markdownMarkdown默认是支持Html标签的，所以我想到视频和音频应该也是支持的吧。 下面是简单的使用，Mark一下。 简单的使用Vedio123&lt;video id=&quot;video&quot; controls=&quot;&quot; preload=&quot;none&quot; poster=&quot;/img/bf/python.jpg&quot;&gt; &lt;source id=&quot;mp4&quot; src=&quot;/vedio/test.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/video&gt; Audio123&lt;audio id=&quot;audio&quot; controls=&quot;&quot; preload=&quot;none&quot;&gt; &lt;source id=&quot;mp3&quot; src=&quot;/audio/test.mp3&quot;&gt;&lt;/audio&gt;","link":"/2021/09/09/2021/09/markdown-vedio-audio/"},{"title":"Analyze 1.usa.gove from bit.ly","text":"背景最近在看数据分析相关的知识点，同事那借了本 Python for Data Analysis 在看，接下来会记录一下学习心得和书上的例子，温故知新。 数据分析的门道还是挺多的，Python 的一些库(pandas, numpy, matplotlib)也很好用，不用自己去用标准库辛苦的写了，之前分析 log 如果了解这些知识点的话，效率会很高。 今天介绍书中的例子。 数据源今天分析的数据是 URL 缩短服务 bit.ly 与美国政府网站 usa.gov 合作，从生成.gov 或.mil 短链接的用户那里收集来的匿名数据（该服务已停止）。 我们可以看到下面的文档例子，里面每一行都是 json 数据。 example.txt 下面是其中一行数据的例子： 123456789101112131415161718{ 'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'c': 'US', 'nk': 1, 'tz': 'America/New_York', 'gr': 'MA', 'g': 'A6qOVH', 'h': 'wfLQtf', 'l': 'orofrog', 'al': 'en-US,en;q=0.8', 'hh': '1.usa.gov', 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991', 't': 1331923247, 'hc': 1331822918, 'cy': 'Danvers', 'll': [42.576698, -70.954903]} 像 tz 就是表示 time zone, a 则是用户浏览器的信息 agent，主要分析的数据便是他们，我们一步一步来看。 下面是对数据的初步处理，将文件读取进来，加入 Dict 12345678import jsonimport pandas as pd; import numpy as npimport matplotlib.pyplot as pltfrom pandas import DataFrame, Seriesexample_path = 'example.txt'records = [json.loads(line) for line in open(example_path)] 对时区(tz)进行计数这里我们就用到了 pandas 中的最重要的数据结构 DataFrame, 它用于将数据表示为一个表格。 下面的代码就是将之前读取的数据创建成 DataFrame，然后通过.value_counts()方法非常容易的对时区进行了计数, 并且也进行了排序，所以我们取前面 10 个便是 top 10。 123frame = DataFrame(records)tz_counts = frame['tz'].value_counts()print(tz_counts[:10]) output: 1234567891011America/New_York 1251 521America/Chicago 400America/Los_Angeles 382America/Denver 191Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35America/Sao_Paulo 33Name: tz, dtype: int64 那可能有人会好奇，这个生成的 DataFrame 对象是什么样的呢？下面是print(frame)的结果，可以看出它将 key value 值表格化（行转列了）。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364 _heartbeat_ a ... tz u0 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... America/New_York http://www.ncbi.nlm.nih.gov/pubmed/224159911 NaN GoogleMaps/RochesterNY ... America/Denver http://www.monroecounty.gov/etc/911/rss.php2 NaN Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ... ... America/New_York http://boxer.senate.gov/en/press/releases/0316...3 NaN Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)... ... America/Sao_Paulo http://apod.nasa.gov/apod/ap120312.html4 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... America/New_York http://www.shrewsbury-ma.gov/egov/gallery/1341...5 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... America/New_York http://www.shrewsbury-ma.gov/egov/gallery/1341...6 NaN Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1... ... Europe/Warsaw http://www.nasa.gov/mission_pages/nustar/main/...7 NaN Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/2... ... http://www.nasa.gov/mission_pages/nustar/main/...8 NaN Opera/9.80 (X11; Linux zbov; U; en) Presto/2.1... ... http://www.nasa.gov/mission_pages/nustar/main/...9 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... http://apod.nasa.gov/apod/ap120312.html10 NaN Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.2)... ... America/Los_Angeles https://www.nysdot.gov/rexdesign/design/commun...11 NaN Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.4... ... America/New_York http://oversight.house.gov/wp-content/uploads/...12 NaN Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.2)... ... America/New_York https://www.nysdot.gov/rexdesign/design/commun...13 1.331923e+09 NaN ... NaN NaN14 NaN Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US... ... America/New_York http://toxtown.nlm.nih.gov/index.php15 NaN Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1... ... Asia/Hong_Kong http://www.ssd.noaa.gov/PS/TROP/TCFP/data/curr...16 NaN Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1... ... Asia/Hong_Kong http://www.usno.navy.mil/NOOC/nmfc-ph/RSS/jtwc...17 NaN Mozilla/5.0 (Macintosh; Intel Mac OS X 10.5; r... ... America/New_York http://www.usda.gov/wps/portal/usda/usdahome?c...18 NaN GoogleMaps/RochesterNY ... America/Denver http://www.monroecounty.gov/etc/911/rss.php19 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... Europe/Rome http://www.nasa.gov/mission_pages/nustar/main/...20 NaN Mozilla/5.0 (compatible; MSIE 9.0; Windows NT ... ... Africa/Ceuta http://voyager.jpl.nasa.gov/imagesvideo/uranus...21 NaN Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6... ... America/New_York http://www.nasa.gov/mission_pages/nustar/main/...22 NaN Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ... ... America/New_York http://portal.hud.gov/hudportal/documents/hudd...23 NaN Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3)... ... America/New_York http://www.tricare.mil/mybenefit/ProfileFilter...24 NaN Mozilla/5.0 (Windows; U; Windows NT 5.1; es-ES... ... Europe/Madrid http://www.nasa.gov/mission_pages/nustar/main/...25 NaN Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1... ... Asia/Kuala_Lumpur http://www.nasa.gov/mission_pages/nustar/main/...26 NaN Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1... ... Asia/Nicosia http://www.nasa.gov/mission_pages/nustar/main/...27 NaN Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)... ... America/Sao_Paulo http://apod.nasa.gov/apod/ap120312.html28 NaN Mozilla/5.0 (iPad; CPU OS 5_0_1 like Mac OS X)... ... https://www.nysdot.gov/rexdesign/design/commun...29 NaN Mozilla/5.0 (iPad; U; CPU OS 3_2 like Mac OS X... ... http://www.ed.gov/news/media-advisories/us-dep...... ... ... ... ... ...3530 NaN Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.1... ... America/Los_Angeles http://www.nasa.gov/multimedia/imagegallery/im...3531 NaN Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6... ... http://www.nasa.gov/mission_pages/nustar/main/...3532 NaN Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.2)... ... America/New_York http://portal.hud.gov/hudportal/HUD?src=/press...3533 NaN Mozilla/5.0 (iPad; CPU OS 5_1 like Mac OS X) A... ... America/New_York http://apod.nasa.gov/apod/3534 NaN Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)... ... America/Chicago https://www.nysdot.gov/rexdesign/design/commun...3535 NaN Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/... ... America/Chicago http://ntl.bts.gov/lib/44000/44300/44374/FHWA-...3536 NaN Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; e... ... http://www.nasa.gov/mission_pages/hurricanes/a...3537 NaN Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.2)... ... America/Tegucigalpa http://apod.nasa.gov/apod/ap120312.html3538 NaN Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Ma... ... America/Los_Angeles http://healthypeople.gov/2020/connect/webinars...3539 NaN Mozilla/5.0 (compatible; Fedora Core 3) FC3 KDE ... America/Los_Angeles http://www.federalreserve.gov/newsevents/press...3540 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... America/Denver http://www.nasa.gov/mission_pages/nustar/main/...3541 NaN Mozilla/5.0 (X11; U; OpenVMS AlphaServer_ES40;... ... America/Los_Angeles http://www.federalreserve.gov/newsevents/press...3542 NaN Mozilla/5.0 (compatible; MSIE 9.0; Windows NT ... ... America/Los_Angeles http://www.sba.gov/community/blogs/community-b...3543 1.331927e+09 NaN ... NaN NaN3544 NaN Mozilla/5.0 (Windows NT 6.1; WOW64; rv:5.0.1) ... ... America/Chicago https://www.nysdot.gov/rexdesign/design/commun...3545 NaN Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.2)... ... America/Chicago https://www.nysdot.gov/rexdesign/design/commun...3546 NaN Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Ma... ... America/Los_Angeles http://healthypeople.gov/2020/connect/webinars...3547 NaN Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8)... ... America/New_York http://www.epa.gov/otaq/regs/fuels/additive/e1...3548 NaN Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Ma... ... America/Chicago http://www.fda.gov/Safety/Recalls/ucm296326.htm3549 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... Europe/Stockholm http://www.nasa.gov/mission_pages/WISE/main/in...3550 NaN Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ... ... America/New_York http://www.nlm.nih.gov/medlineplus/news/fullst...3551 NaN Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi... ... http://www.nasa.gov/mission_pages/nustar/main/...3552 NaN Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US... ... America/Chicago http://travel.state.gov/passport/passport_5535...3553 NaN Mozilla/4.0 (compatible; MSIE 7.0; Windows NT ... ... America/New_York http://www.shrewsbury-ma.gov/egov/gallery/1341...3554 NaN Mozilla/4.0 (compatible; MSIE 7.0; Windows NT ... ... America/New_York http://www.shrewsbury-ma.gov/egov/gallery/1341...3555 NaN Mozilla/4.0 (compatible; MSIE 9.0; Windows NT ... ... America/New_York http://www.fda.gov/AdvisoryCommittees/Committe...3556 NaN Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.1... ... America/Chicago http://www.okc.gov/PublicNotificationSystem/Fo...3557 NaN GoogleMaps/RochesterNY ... America/Denver http://www.monroecounty.gov/etc/911/rss.php3558 NaN GoogleProducer ... America/Los_Angeles http://www.ahrq.gov/qual/qitoolkit/3559 NaN Mozilla/4.0 (compatible; MSIE 8.0; Windows NT ... ... America/New_York http://herndon-va.gov/Content/public_safety/Pu...[3560 rows x 18 columns] 对时区数据进行微处理并生成图片有一些数据的tz值是''的，我们当成 Unknown，也有一些是缺省的，没有这个 key，我们就设成 Missing 12345clean_tz = frame['tz'].fillna('Missing')# print(clean_tz)clean_tz[clean_tz == ''] = 'Unknown'tz_counts = clean_tz.value_counts()print(tz_counts[:10]) output: 1234567891011America/New_York 1251Unknown 521America/Chicago 400America/Los_Angeles 382America/Denver 191Missing 120Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35Name: tz, dtype: int64 之后我们可以画出对应的柱状图，如下所示： 12tz_counts[:10].plot(kind='barh', rot=0)plt.show() 解析 Agent 信息上面我们可以看到 Agent 的初始信息很多，字符串很长，但是我们可以根据特征使用字符串函数与正则表达式先解析出来。 下面就是将空值去掉并进行计数： 123results = Series([x.split()[0] for x in frame.a.dropna()])agent_counts = results.value_counts()[:8]print(agent_counts) output: 123456789Mozilla/5.0 2594Mozilla/4.0 601GoogleMaps/RochesterNY 121Opera/9.80 34TEST_INTERNET_AGENT 24GoogleProducer 21Mozilla/6.0 5BlackBerry8520/5.0.0.681 4dtype: int64 在 Agent 信息中我们可以看到一些有 Windows 的字样，表示是在 windows 系统下访问的，而没有的话，我们就认真是非 Windows 的。 1234frame.loc[frame.tz=='', 'tz'] = 'Unknown'cframe = frame[frame.a.notnull()]operation_system = np.where(cframe['a'].str.contains('Windows'), 'Windows', 'Not Windows')print(operation_system[:5]) 我们可以看到前面 5 条数据 agent 中的情况output: 1['Windows' 'Not Windows' 'Windows' 'Not Windows' 'Windows'] 根据时区和系统进行分组解析上面我们可以得到时区和所使用系统的分布，下面我们就可以根据这些来进行分组，得知在一个时区里系统的使用情况。 123by_tz_os = cframe.groupby(['tz', operation_system])agg_counts = by_tz_os.size().unstack().fillna(0)print(agg_counts[:10]) output:下面是取前面的 10 个值（还没有排序） 123456789101112 Not Windows Windowstz 245.0 276.0Africa/Cairo 0.0 3.0Africa/Casablanca 0.0 1.0Africa/Ceuta 0.0 2.0Africa/Johannesburg 0.0 1.0Africa/Lusaka 0.0 1.0America/Anchorage 4.0 1.0America/Argentina/Buenos_Aires 1.0 0.0America/Argentina/Cordoba 0.0 1.0America/Argentina/Mendoza 0.0 1.0 接下来主要目的是选取最常出现的时区（即 Not Windows 和 Windows 的数量加起来的数量最多的时区，其实直接 count 时区是一样的），所以我们要先构造一个间接索引数组。然后通过 take 按照这个顺序截取了最后 10 行,因为是升序，所以是 top10 123456789101112131415# order by ascindexer = agg_counts.sum(1).argsort()count_subset = agg_counts.take(indexer)[-10:]print(count_subset)# draw the real numbercount_subset.plot(kind='barh', stacked=True)# draw with %normed_subset = count_subset = count_subset.div(count_subset.sum(1), axis=0)normed_subset.plot(kind='barh', stacked=True)plt.show() output: 1234567891011tzAmerica/Sao_Paulo 13.0 20.0Europe/Madrid 16.0 19.0Pacific/Honolulu 0.0 36.0Asia/Tokyo 2.0 35.0Europe/London 43.0 31.0America/Denver 132.0 59.0America/Los_Angeles 130.0 252.0America/Chicago 115.0 285.0Unknown 245.0 276.0America/New_York 339.0 912.0 这里画了两张图，一张为实际数值: 另一张则为百分比： 最后在学习的过程中，不出意外的我遇到了 SettingwithCopyWarning 的坑，这个是由于链式赋值（链式索引和赋值的组合）的引起的，还是有点意思的，下面的参考中就有一些资料帮助理解。虽然只是warning，但还是不应该出现并且避免，因为说明有风险，赋值操作可能与想的不一样。 参考： pandas 的 SettingWithCopyWarning 警告出现的原因和如何避免 Pandas 中 SettingwithCopyWarning 的原理和解决方案 Python 笔试集（1）：关于 Python 链式赋值的坑","link":"/2019/01/05/2019/01/2019-01-05-PyAnalysisLearn01/"},{"title":"Performance Test Training (2)","text":"Performance Test Training (2)性能测试培训的第二天，主要内容是性能测试一些实例，这些要总结好难，还是要实际操作。 核心的规则还是从进程，到线程，CPU，内存，堆栈，IO，网络一步步分析。 培训上基本上是都是 linux 的命令，我下面简单罗列了一些window下的命令，而且很不全，参考意义不大。 之后有时候再一块一块深入，每一块都有好多细节。。。今天就算给自己交个差吧（捂脸） Performance Monitor (Link)性能计数器是windows提供的核心监控数据，可以根据自己需要去获取。 Processor: % Processor Time CPU当前利用率，百分比 Memory: Available MBytes 当前可用内存，兆字节（虚拟内存不需要监控，只有当物理内存不够时才会使用虚拟内存，物理内存已有监控） LogicalDisk: % Free Space 逻辑分区可用空间，百分比（物理磁盘IO由于RAID级别不同，或者有的机器没有RAID，无法定义统一的监控阈值） Network Interface: Bytes Total/sec 网卡流量：发送+接收，字节 TCPv4: Connections Established 当前连接数（Established + Close-Wait） CPU:%Processor Time%Priviliaged Time CPU在特权模式下处理线程所花的时间百分比。一般的系统服务，进城管理，内存管理等一些由操作系统自行启动的进程属于这类 %User Time 与%Privileged Time计数器正好相反，指的是在用户状态模式下（即非特权模式）的操作所花的时间百分比。如果该值较大，可以考虑是否通过算法优化等方法降低这个值。如果该服务器是数据库服务器，导致此值较大的原因很可能是数据库的排序或是函数操作消耗了过多的CPU时间，此时可以考虑对数据库系统进行优化。 %DPC Time 处理器在网络处理上消耗的时间，该值越低越好。在多处理器系统中，如果这个值大于50%并且%Processor Time非常高，加入一个网卡可能会提高性能。 Memory:Available BytesPages/sec该计数器显示由于页面不在物理内存中而需要从磁盘读取的页面数。Pages/sec 的值很大不一定表明内存有问题，而可能是运行使用内存映射文件的程序所致，操作系统经常会利用磁盘交换的方式提高系统可用的内存量或是提高内存的使用效率。（注意该计数器与 Page Faults/sec 的区别，后者只表明数据不能在内存的指定工作集中立即使用，包括硬错误和软错误） Page Faults/sec计数器可以确保磁盘活动不是由分页导致的。在 Windows 中，换页的原因包括：配置进程占用了过多内存 或者 文件系统活动。 如果在同一硬盘上有多个逻辑分区，需要使用 Logical Disk计数器而非 Physical Disk计数器。查看逻辑磁盘计数器有助于确定哪些文件被频繁访问。当发现磁盘有大量读/写活动时，请查看读写专用计数器以确定导致每个逻辑卷负荷增加的磁盘活动类型，例如，Logical Disk: Disk Write Bytes/sec。 Page Input/sec表示为了解决硬错误而写入硬盘的页数（参考值：&gt;=Page Reads/sec） Page Reads/sec 表示为了解决硬错误而从硬盘上读取的页数。（参考值： &lt;=5） 如果怀疑有内存泄露，请监视 Memory/Available Bytes 和 Memory/ Committed Bytes，以观察内存行为，并监视你认为可能在泄露内存的进程的 Process/ Private Bytes、Process/ Working Set 和Process/ Handle Count。如果怀疑是内核模式进程导致了泄露，则还应该监视 Memory/ Pool Nonpaged Bytes、Memory/ Pool Nonpaged Allocs 和 Process(process_name)/ Pool Nonpaged Bytes 如果发生了内存泄漏,process\\private bytes计数器和process\\working set 计数器的值往往会升高,同时avaiable bytes的值会降低 private Bytes 是指进程所分配的无法与其他进程共享的当前字节数量。该计数器主要用来判断进程在性能测试过程中有无内存泄漏。 例如：对于一个IIS之上的web应用，我们可以重点监控inetinfo进程的Private Bytes，如果在性能测试过程中，该进程的Private Bytes计数器值不断增加，或是性能测试停止后一段时间，该进程的Private Bytes仍然持续在高水平，则说明应用存在内存泄漏。 Disk：PhysicalDisk\\Avg. Disk sec/Read以秒计算的在此盘上读取数据的所需平均时间。 Physical Disk\\ Disk Reads/sec在读取操作时从磁盘上传送的字节平均数。 PhysicalDisk\\ Avg. Disk sec/Write以秒计算的在此盘上写入数据的所需平均时间。 Physical Disk\\ DiskWrites/sec在写入操作时从磁盘上传送的字节平均数。 Physical Disk\\ Avg.Disk sec/Transfer反映磁盘完成请求所用的时间。较高的值表明磁盘控制器由于失败而不断重试该磁盘。这些故障会增加平均磁盘传送时间。 %Disk Time 和 Avg.Disk Queue LengthRAID 磁盘中的 % Disk Time 计数器会指示大于 100% 的值。如果出现这种情况，则使用 PhysicalDisk: Avg.Disk Queue Length计数器来确定等待进行磁盘访问的平均系统请求数量。 如果不是RAID，则使用 % Disk Time 和 Current Disk Queue Length计数器确定是否磁盘存在瓶颈，如果这两个计数器的值一直很高，则可能是磁盘存在瓶颈 Physical Disk： DiskTransfers/sec 磁盘IOPS % Disk Time 当前物理磁盘利用率，如果是RAID，该值会大于100% Current Disk Queue Length 等待进行磁盘访问的当前系统请求数量 Avg.Disk Queue Length 等待进行磁盘访问的平均系统请求数量，用于RAID Tasktasklist1234567891011PS C:\\Users\\mayn\\Desktop&gt; tasklist映像名称 PID 会话名 会话# 内存使用========================= ======== ================ =========== ============System Idle Process 0 Services 0 8 KSystem 4 Services 0 128 KRegistry 108 Services 0 13,504 Ksmss.exe 376 Services 0 1,168 Kcsrss.exe 580 Services 0 4,932 Kwininit.exe 672 Services 0 6,576 Kcsrss.exe 680 Console 1 5,368 K tskill/taskkill1tskill notepad CPUwmic cpu上面显示的有位宽，最大始终频率， 生产厂商，二级缓存等信息 123PS C:\\Users\\mayn\\Desktop&gt; wmic cpuAddressWidth Architecture AssetTag Availability Caption Characteristics ConfigManagerErrorCode ConfigManagerUserConfig CpuStatus CreationClassName CurrentClockSpeed CurrentVoltage DataWidth Description DeviceID ErrorCleared ErrorDescription ExtClock Family InstallDate L2CacheSize L2CacheSpeed L3CacheSize L3CacheSpeed LastErrorCode Level LoadPercentage Manufacturer MaxClockSpeed Name NumberOfCores NumberOfEnabledCore NumberOfLogicalProcessors OtherFamilyDescription PartNumber PNPDeviceID PowerManagementCapabilities PowerManagementSupported ProcessorId ProcessorType Revision Role SecondLevelAddressTranslationExtensions SerialNumber SocketDesignation Status StatusInfo Stepping SystemCreationClassName SystemName ThreadCount UniqueId UpgradeMethod Version VirtualizationFirmwareEnabled VMMonitorModeExtensions VoltageCaps64 9 To Be Filled By O.E.M. 3 Intel64 Family 6 Model 158 Stepping 10 236 1 Win32_Processor 3000 10 64 Intel64 Family 6 Model 158 Stepping 10 CPU0 100 205 1536 9216 0 6 13 GenuineIntel 3000 Intel(R) Core(TM) i5-8500 CPU @ 3.00GHz 6 6 6 To Be Filled By O.E.M. FALSE BFEBFBFF000906EA 3 CPU TRUE To Be Filled By O.E.M. LGA1151 OK 3 Win32_ComputerSystem PC-CX 6 1 FALSE TRUE Memorywmic memorychip可以显示出来 2 条内存，各 8G 速度 2666MHz 1234PS C:\\Users\\mayn\\Desktop&gt; wmic memorychipAttributes BankLabel Capacity Caption ConfiguredClockSpeed ConfiguredVoltage CreationClassName DataWidth Description DeviceLocator FormFactor HotSwappable InstallDate InterleaveDataDepth InterleavePosition Manufacturer MaxVoltage MemoryType MinVoltage Model Name OtherIdentifyingInfo PartNumber PositionInRow PoweredOn Removable Replaceable SerialNumber SKU SMBIOSMemoryType Speed Status Tag TotalWidth TypeDetail Version1 BANK 0 8589934592 物理内存 2666 1200 Win32_PhysicalMemory 64 物理内存 ChannelA-DIMM1 8 1 1 G-Skill 0 0 0 物理内存 F4-2666C19-8GNT 00000000 26 2666 Physical Memory 0 64 1281 BANK 2 8589934592 物理内存 2666 1200 Win32_PhysicalMemory 64 物理内存 ChannelB-DIMM1 8 1 2 G-Skill 0 0 0 物理内存 F4-2666C19-8GNT 00000000 26 2666 Physical Memory 1 64 128 Diskwmic logicaldisk可以看到有几个盘，每一个盘的文件系统和剩余空间 12345PS C:\\Users\\mayn\\Desktop&gt; wmic logicaldiskAccess Availability BlockSize Caption Compressed ConfigManagerErrorCode ConfigManagerUserConfig CreationClassName Description DeviceID DriveType ErrorCleared ErrorDescription ErrorMethodology FileSystem FreeSpace InstallDate LastErrorCode MaximumComponentLength MediaType Name NumberOfBlocks PNPDeviceID PowerManagementCapabilities PowerManagementSupported ProviderName Purpose QuotasDisabled QuotasIncomplete QuotasRebuilding Size Status StatusInfo SupportsDiskQuotas SupportsFileBasedCompression SystemCreationClassName SystemName VolumeDirty VolumeName VolumeSerialNumber0 C: FALSE Win32_LogicalDisk 本地固定磁盘 C: 3 NTFS 74612744192 255 12 C: 106530430976 FALSE TRUE Win32_ComputerSystem PC-CX 04C2790D0 D: FALSE Win32_LogicalDisk 本地固定磁盘 D: 3 NTFS 342652858368 255 12 D: 372486508544 FALSE TRUE Win32_ComputerSystem PC-CX 000048230 E: FALSE Win32_LogicalDisk 本地固定磁盘 E: 3 NTFS 364452909056 255 12 E: 1000202768384 FALSE TRUE Win32_ComputerSystem PC-CX 00004823 wmic volume1234567PS C:\\Users\\mayn\\Desktop&gt; wmic volumeAccess Automount Availability BlockSize BootVolume Capacity Caption Compressed ConfigManagerErrorCode ConfigManagerUserConfig CreationClassName Description DeviceID DirtyBitSet DriveLetter DriveType ErrorCleared ErrorDescription ErrorMethodology FileSystem FreeSpace IndexingEnabled InstallDate Label LastErrorCode MaximumFileNameLength Name NumberOfBlocks PageFilePresent PNPDeviceID PowerManagementCapabilities PowerManagementSupported Purpose QuotasEnabled QuotasIncomplete QuotasRebuilding SerialNumber Status StatusInfo SupportsDiskQuotas SupportsFileBasedCompression SystemCreationClassName SystemName SystemVolume TRUE 4096 TRUE 106530430976 C:\\ FALSE \\\\?\\Volume{81b9fa95-b225-44ef-84fe-e6b4bcd49f21}\\ C: 3 NTFS 74612363264 TRUE 255 C:\\ TRUE 79853837 TRUE TRUE PC-CX FALSE TRUE 4096 FALSE 844099584 \\\\?\\Volume{d9656dfe-1091-41c2-b677-aed0eda4ea25}\\ FALSE \\\\?\\Volume{d9656dfe-1091-41c2-b677-aed0eda4ea25}\\ 3 NTFS 369049600 TRUE 255 \\\\?\\Volume{d9656dfe-1091-41c2-b677-aed0eda4ea25}\\ FALSE 2759807823 TRUE TRUE PC-CX FALSE TRUE 4096 FALSE 372486508544 D:\\ FALSE \\\\?\\Volume{da460ecc-d1db-4ffc-8fd3-9445c2154bf4}\\ D: 3 NTFS 342652858368 TRUE 255 D:\\ FALSE 18467 TRUE TRUE PC-CX FALSE TRUE 4096 FALSE 1000202768384 E:\\ FALSE \\\\?\\Volume{673bf2eb-d8ce-4689-98fe-add079591f66}\\ E: 3 NTFS 364452909056 TRUE 255 E:\\ FALSE 18467 TRUE TRUE PC-CX FALSE TRUE 2048 FALSE 102539264 \\\\?\\Volume{e9b34647-3273-4702-9f11-7296af3707f2}\\ \\\\?\\Volume{e9b34647-3273-4702-9f11-7296af3707f2}\\ 3 FAT 76158976 255 \\\\?\\Volume{e9b34647-3273-4702-9f11-7296af3707f2}\\ FALSE 18467 FALSE FALSE PC-CX TRUE 每个盘的剩余空间量，其实上一个命令也可以查看的 1234PS C:\\Users\\mayn\\Desktop&gt; fsutil volume diskfree c:可用字节总数 : 74612154368字节总数 : 106530430976可用的尚未使用的字节总数 : 74612154368 这个命令查看每一个卷的容量信息是很方便 BIOSwmic bios123PS C:\\Users\\mayn\\Desktop&gt; wmic biosBiosCharacteristics BIOSVersion BuildNumber Caption CodeSet CurrentLanguage Description EmbeddedControllerMajorVersion EmbeddedControllerMinorVersion IdentificationCode InstallableLanguages InstallDate LanguageEdition ListOfLanguages Manufacturer Name OtherTargetOS PrimaryBIOS ReleaseDate SerialNumber SMBIOSBIOSVersion SMBIOSMajorVersion SMBIOSMinorVersion SMBIOSPresent SoftwareElementID SoftwareElementState Status SystemBiosMajorVersion SystemBiosMinorVersion TargetOperatingSystem Version{7, 10, 11, 12, 15, 16, 17, 19, 23, 24, 25, 26, 27, 28, 29, 32, 33, 40, 42, 43} {&quot;ALASKA - 1072009&quot;, &quot;0401&quot;, &quot;American Megatrends - 5000D&quot;} 0401 en|US|iso8859-1 0401 255 255 9 {&quot;en|US|iso8859-1&quot;, &quot;fr|FR|iso8859-1&quot;, &quot;zh|TW|unicode&quot;, &quot;zh|CN|unicode&quot;, &quot;ja|JP|unicode&quot;, &quot;de|DE|iso8859-1&quot;, &quot;es|ES|iso8859-1&quot;, &quot;ru|RU|iso8859-5&quot;, &quot;ko|KR|unicode&quot;} American Megatrends Inc. 0401 TRUE 20180319000000.000000+000 System Serial Number 0401 3 1 TRUE 0401 3 OK 5 13 0 ALASKA - 1072009","link":"/2018/08/27/2018/08/2018-08-27-PerfTestTraining_02/"},{"title":"Java Developement Manual","text":"Java 开发手册(alibaba)闲来无聊，手打一份手册，当做自己在网络的备份，方便以后查看，顺便当练习打字了，哈哈哈哈，我的五笔都生疏了，嘿嘿。 第一章 编程规约本章是传统意义上的代码规范，包括变量名、代码风格、控制语句、代码注释等基本的编程习惯，以及从高并发场景中提炼出来的集合处理技巧和并发多线程的注意事项。 1.1 命名风格【强制】 代码中的命名均不能以下画线或美元符号开始，也不能以下画线或美元符号结束。 代码中的命名严禁使用拼与英文混合的方式，更不允许直接使用中文的方式。 类名使用 UpperCamelCamel 风格， 但 DO/BO/DTO/VO/AO/PO 等情形例外。 方法名、参数名、成员变量 、局部变量都统一使用 lowerCamelCase 风格，必须遵从驼峰的形式。 常量命名全部大写，单词间用下画线隔开，力求语义表达完整清楚，不要嫌名字长。 1e.g. MAX_STOCK_COUNT / PRIZE_NUMBER_EVERYDAY 抽象类名使用 Abstract 或 Base 开头；异常类使用 Exception 结尾；测试类命名要以它测试的类名开始，以 Test 结尾。 类型与中括号之间无空格相连定义数组。 1e.g. int[] arrayDemo; POJO 类中布尔类型的变量都不要加 is 前缀，否则部分框架解析会引起序列化错误。 12e.g. Boolean isDeleted, it's method is isDeleted(), Error happened when RPC Framwork try to get property name deleted) 包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，则类名可以使用复数形式。 1e.g. com.alibaba.ai.util.MessageUtils 杜绝完全不规范的缩写，避免词不达义。 【推荐】 为了达到代码自解释的目标，任何自定义编程元素在命名时，使用尽量完整的单词组合来表达其意。 1e.g 在JDK中，对某个对象引用的volatile字段进行原子更新的类名为： AtomicReferenceFieldUpdater。 如果模块、接口、类、方法使用了设计模式 ，应在命名时体现出具体模式。 123public class OrderFactory;public class LoginProxy;public class ResourceObserver; 接口类中的方法和属性不要加任何修饰符号(public 也不要加)， 保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义变量，如果一定要定义变量，必须是与接口方法相关的，并且是整个应用的基础常量。 1234接口方法签名：void commit();接口基础常量：String COMPANY = &quot;alibaba&quot;;c.g. public abstract void commit();如果JDK8中接口允许有默认实现，那么这个default方法，是对所有实现类都有价值的默认实现。 接口和实现类的命名有两套规则： 【强制】对于 Service 和 DAO 类，基于 SOA 的理念，暴露出来的服务一定是接口，内部的实现类用以 Impl 后缀与接口区别。e.g. CacheServiceImpl实现CacheService接口 【推荐】如果是形容能力的接口名称，取对应的形容词为接口名（通常是-able 的形式）。e.g. AbstractTranslator 实现 Translatable 参考 枚举类名建议带上 Enum 后缀，枚举成员名称需要全大写， 单词间用下画线隔开。 12枚举其实就是特殊的常量类，且构造方法被默认强制为私有。e.g.枚举名字为ProcessStatusEnum的成员名称：SUCCESS / UNKNOWN_REASON 各层命名规约： Service/DAO 层方法命名规约如下： 获取单个对象的方法用 get 作为前缀。 获取多个对象的方法用 list 作为前缀。 获取统计值的方法用 count 作为前缀。 插入的方法用 sava / insert 作为前缀。 删除的方法用 remove / delete 作为前缀。 修改的方法用 update 作为前缀 领域模型命名规约如下: 数据对象： xxxDO, xxx 为数据表名。 数据传输对象： xxxDTO, xxx 为业务领域相关的名称。 展示对象： xxxVO, xxx 一般为网页名称。 POJO 是 DO/DTO/BO/VO 的统称，禁止命名成 xxxPOJO。 1.2 常量定义强制 不允许任何魔法值（即未经预先定义的常量）直接出现在代码中。 long 或者 Long 初始赋值时，使用大写的L，不能是小写的l。小写的l容易跟数字1混淆，造成误解。 推荐 不要使用一个常量类维护所有常量，要按常量功能进行归类，分开维护。 常量的复用层次有 5 层：跨应用共享常量、应用内共享常量、子工程内共享常量、包内共享常量、类内共享常量。 - 跨应用共享常量：放置在二方库中，通常是在 client.jar 中的 constant 目录下。 - 应用内共享常量：旋转在一方库中，通常是在子模块中的 constant 目录下。 - 子工程内部共享常量：即在当前子工程的 constant 目录下。 - 包内共享常量：即在当前包下单独的 constant 目录下。 - 类内共享常量：直接在类内部以private static final的方式定义。 如果变量值仅在一个范围内变化，则用 enum 类型来定义。如果存在名称之外的延伸属性，则使用 enum 类型，下面正例的数字就是延伸信息，表示一年中的第几个季节。1234567public enum SeasonEnum { SPRINT(1), SUMMER(2), AUTUMN(3), WINTER(4); private int seq; SeasonEnum(int seq){ this.seq = seq; }} 1.3 代码格式强制 大括号的使用约定。如果大括号内为空，则简洁地写成{}即可，也不需要换行；如果是非空代码，则： 左大括号前不换行。 左大括号后换行。 右大括号前换行。 右大括号后还有 else 等代码刚不换行； 表示终止的右大括号必须换行。 左小括号和字符之间不出现空格；同样，右小括号和字符之间也不出现空格。 if / for / while / switch / do 等保留字与括号之间都必须加上空格。 任何二目、三目运算符的左右两边都需要加一个空格。采用 4 个空格缩进，禁止使用 Tab 控制符。 注释的双斜线与注释内容之间有且仅有一个空格。 12345678910111213141516171819public static void main(String[] args){ // 缩进4个空格 String say = &quot;hello&quot;; // 运算符的左右必须有1个空格 int flag = 0; // 关键字 if 与括号之间必须有 1 个空格， // 括号内的 f 与左括号、0 与右括号之间不需要空格 if (flag == 0) { System.out.println(say); } // 左大括号前加空格且不换行；左大括号后换行 if (flag == 1) ｛ System.out.println(&quot;world&quot;); // 右大括号前换行，右大括号后有else，不用换行 } else { System.out.println(&quot;ok&quot;); // 在右大括号后直接结束，则必须换行 }} 单行字符数不超过 120 个，超出则需要换行，换行是遵循如下原则： 第二行相对第一行缩进 4 个空格，从第三行开始，不再持续缩进，参考示例。 运算符与下文一起换行。 方法调用的点符号与下方一起换行。 方法调用中的多个参数需要换行时，在逗号后进行。 在括号前不要换行。1234StringBuffer sb = new StringBuffer();sb.append(&quot;you &quot;).append(&quot;are &quot;)... .append(&quot;so &quot;)... .append(&quot;kind!&quot;); 方法参数在定义和传入时，多个参数逗号后面必须加空格。 1method(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;); IDE 的 text file encoding 设置为 UTF-8；IDE 中文件的换行符使用 UNIX 格式， 不要使用 Windows 格式。 推荐 没有必要增加若干空格来使某一行的字符与上一行对应位置的字符对齐。 不同逻辑、不同语义、不同业务的代码之间插入一个空行分隔开来，以提升可读性。（没必要插入多个空行来隔开。） OOP 规约强制 避免通过一个类的对象引用访问此类的静态变量或静态方法，造成无谓增加编译器解析成本，直接用类名来访问即可。 所以的覆写方法，必须加@Override注解 相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object。 12可变参数必须放置在参数列表的最后（建议工程师们尽量不用可变参数编程）。public User listUsers(String type, Long... ids){...} 对外部正在调用或者二方库依赖的接口，不允许修改方法接口，以避免对接口调用方产生影响。若接口过时，必须加 @Deprecated注解，并清晰地说明采用的新接口或者新服务是什么。 不能使用过时的类或方法。 Object 的 equals 方法容易抛空指针，应使用常量或者确定有值的对象来调用 equals。 12e.g. &quot;test&quot;.equals(object)c.g. object.equest(&quot;test&quot;); 所有相同类型的包装类对象之间值的比较，全部使用 equals 方法。 12345对于 Integer var = ? 在 - 128~127范围内的赋值， Integer 对象是在IntegerCache.cache中产生 的。会复用已有对象， 这个区间内的Integer值可以直接使用 == 进行判断， 但是这个区间外的所有数据， 都会在堆上产生，并不会利用已有对象。这是个大坑，推荐使用equals方法进行判断。 关于基本数据类型与包装数据类型的使用标准如下： 【强制】所有的POJO类属性必须使用包装数据类型。 【强制】RPC 方法的返回值和参数必须使用包装数据类型。 【推荐】所有的局部变量使用基本数据类型。 在定义 DO/DTO/VO 等 POJO类时，不要设定任何属性默认值。 123c.g. `POJO`类的gmtCreate默认值为 new Date();但是这个属性在数据提取时并没有置入具体值，在更新其它字段时又附带更新了此字段，导致创建时间被修改成当前时间。 当序列化类新增属性时，请不要修改serialVersionUID字段，以避免反序列失败；如果完全不兼容升级， 避免反序列化混乱，那么请修改serialVersionUID值 1注意`serialVersionUID`不一致会抛出序列化运行时异常。 构造方法里禁止加入任何业务逻辑，如果有初始化逻辑，请放在 init 方法中。 POJO类必须写 toString方法。在使用 IDE 中的工具 source&gt; generate toString时， 如果继承了另一个POJO类，注意在前面加一下 super.toString。 推荐 当使用索引访问用 String 的 split 方法等到的数组时，需在最后一个分隔符后面做有无内容的检查，否则会有抛 IndexOutofBoundsException的风险。 当一个类有多个构造方法，或者多个同名方法时，这些方法应该按顺序放置在一起，便于阅读。 类内方法定义的顺序是： 公有方法或保护方法＞私有方法＞ getter / setter 方法。 在 setter 方法中， 参数名称与类成员变量名称一致，this.成员名 = 参数名。在 getter / setter 方法中，不要增加业务逻辑， 否则会增加排查问题的难度。 在循环体内，字符串的连接方式使用StringBuilder的append方法进行扩展。 final 可以声明类、成员变量、方法及本地变量，下列情况使用 final 关键字： - 不允许被继承的类，如：String类。 - 不允许修改引用的域对象， 如：POJO类的域变量。 - 不允许被重写的方法，如： POJO类的 setter 方法。 - 不允许运行过程中重新赋值的局部变量。 - 避免上下文重复使用一个变量，使用 final 描述可以强制重新定义一个新变量，方便更好地进行重构 慎用 Object 的 clone 方法来拷贝对象。 类成员与方法访问控制从严： - 如果不外部直接通过 new 来创建对象，那么构造方法必须限制为private。 - 工具类不允许有 public 或 default 构造方法。 - 类非static成员变量并且与子类共享，必须限制为protected。 - 类非static成员变量并且仅在本类使用，必须限制为private。 - 类static成员变量如果公在本类使用，必须限制为private。 - 若是static 成员变量，必须考虑是否为final。 - 类成员方法只供类内部调用，必须限制为private。 - 类成员方法只对继承类公开，限制为protected. 1.5 集合处理强制 关于hashCode 和equals的处理，遵循如下规则： 只要重写 equals, 就必须重写 hashCode。 因为 Set存储的是不重复的对象，依据 hashCode 和 equals 进行判断，所以 Set 存储的对象必须重写这两种方法。 如果自定义对象作为Map的键，那么必须重写 hashCode 和 equals。 ArrayList的subList结果不可强转成ArrayList, 否则会抛出ClassCastExcpetion异常，即 1java.util.RandomAccessSubList cannot be cast to java.util.ArrayList` 在subList场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生ConcurrentModificationException异常。 使用集合转数组的方法，必须使用集合的toArray(T[] array)，传入的是类型完全一样的数据，大小就是 list.size()。 在使用工具类Arrays.asList()把数组转换成集合时，不能使用其修改集合相关的方法，它的add/remove/clear方法会抛出UnsupportedOperationException异常。 123456asList 的返回对象是一个Arrays内部类，并没有实现集合的修改方法。Arrays.asList体现的是适配器模式，只是转换接口，后台的数据仍是数组。String[] strs = new String[] {&quot;you&quot;, &quot;are&quot;};List list = Arrays.asList(strs);scenario 1: list.add(&quot;chenxiong&quot;) 运行时异常。scenario 2: 如果 str[0] = &quot;xiche&quot;; 那么list.get(0)也会随之修改。 泛型通配符&lt;? extends T&gt;用来接收返回的数据，此写法的泛型集合不能使用 add 方法，而&lt;? super T&gt;不能使用 get 方法， 因为 其作为接口调用赋值时易出错。 123扩展说一下 PECS(Producer Extends Consumer Super)原则： 第一，频繁往外读取内容的，适合用&lt;? extends T&gt;; 第二，经常往里插入的，适合用&lt;? super T&gt;。 不要在foreach循环里进行元素的remove/add操作。remove 元素请用 Iterator方式，如果并发操作，需要对 Iterator 对象加锁。 在 JDK7 及以上版本中， Comparator要满足如下三个条件，不然Arrays.sort, Collections.sort会报IllegalArgumentException异常。 1234567891011三个条件如下： 1) x, y 的比较结果和y, x的比较结果相反。 2) x&gt;y, y&gt;z, 则 x&gt;z。 3) x=y, 则x, z 比较结果和y, z比较结果相同c.g. 下例中没有处理相等的情况，交换两个对象判断结果并不互反，不符合第一个条件，在实际使用中可能会出现异常。 new Comparator&lt;Student&gt;(){ @Override public int compare(Student o1, Student o2){ return 01.getId() &gt; o2.getId() ? 1:-1; } } 推荐 在集合初始化时，指定集合初始值大小。 使用entrySet遍历Map类集合 K/V， 而不是用 keySet 方式遍历。 123keySet 其实遍历了2次，一次是转为Iterator对象，另一次是从hashMap中取出key反对应的value。如果是JDK8，使用Map.foreach方法。 高度注意Map类集合 K/V 能不能存储 null 值的情况 集合类 Key Value Super Comment Hashtable no null no null Dictionary Thread Safe ConcurrentHashMap no null no null AbstractMap 锁分段技术 (JDK8:CAS) TreeMap no null null AbstractMap Thread Unsafe HashMap null null AbstractMap Thread Unsafe 1由于HashMap的干扰，很多人认为ConcurrentHashMap是可以置入null值的，而事实上，存储null值会抛出NPE异常 参考 合理利用集合的有序性(sort)和稳定性(order)，避免集合的无序性(unsort)和不稳定性(unorder)带来的负面影响。 12345有序性批遍历的结果是按某种比较规则依次排列的。稳定性指的是集合每次遍历的元素次序是一定的。如：ArrayList 是 order/unsort;HashMap 是unorder/unsort;TreeSet 是order/sort. 利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的 contains 方法进行遍历、对比、去重操作。 未完待续…└(^o^)┘","link":"/2018/11/17/2018/11/2018-11-17-JavaDevManual/"},{"title":"Swagger In One Page","text":"背景利用 Swagger 官方的 Editor 和 UI 已经能够展示文档，但最后做为交付的文档，肯定是需要能直接打开的文档(e.g. html 格式)才行。所以这两天研究了之后，找到了几个解决方案 pretty-swag spectacle bootprint-openapi html-inline (把 html,js,css 整合到一个 html 中) 他们都是基于 Node.js 的，所以要先安装较新版的 node.js(v10)，下面分别来介绍下怎么使用。 在下面打开的一些完整示例的时候，除 pretty-swag 外都是支持手机端的，不过在电脑上的效果最好。 pretty-swag安装和使用都非常简单，安装完后直接运行，加载 swagger json 或者 yaml 文档，就能生成单个 html 文件。 下面是简单的使用方式，还有别的参数可以设置主题等，可以查看官方 github 得到更多信息： 12npm install pretty-swag -gpretty-swag -i input.json -o output.html 最后的样子如下，完整示例在这里： spectaclespectacle 最后的样式我个人觉得还是不错的，不过貌似 leader 不是很喜欢。 12npm install -g spectacle-docsspectacle -d input.json 运行了上述的命令后，会生成一个 public 文件夹，最后的文档就在其中： 最后的样子如下，完整示例在这里： bootprint-openapi没想到最后领导还是喜欢这个样式，哈哈，主要就像他说的，他喜欢 model 的定义。 使用的方法如下： 123npm install -g bootprintnpm install -g bootprint-openapibootprint openapi input.json bootprint-openapi 运行了上述的命令后，会生成一个指定的文件夹，最后的文档就在其中： 最后的样子如下，完整示例在这里： html-inline在 bootprint-openapi 项目里就有介绍，这个工具是把分开的 js,css 都合到一个 html 中。 使用方式如下： 12npm -g install html-inlinehtml-inline target/index.html 我在尝试的过程中发现，他目前只支持本地的引入，也就是说如果用的是在线的 js 库，比如像下面这样的，就不能正确生成。 如果要实现，我想自己应该可以去改一下源代码来实现，多一些判断与资源的获取。 1&lt;script src=&quot;https://code.jquery.com/jquery-3.1.1.min.js&quot; /&gt; 最后其实 swagger ui 的样式也已经有办法能拿到，就是利用他自己本身的配置，将.json 或者.yaml 直接以对象的形式写进去。 使用的配置是 spec。 12345678910111213var SwaggerUIBundle = require('swagger-ui-dist').SwaggerUIBundleconst ui = SwaggerUIBundle({ //url: &quot;https://petstore.swagger.io/v2/swagger.json&quot;, spec:{&quot;swagger&quot;:&quot;2.0&quot;,&quot;info&quot;:{&quot;description&quot;:&quot;This is a sample server Petstore server. You can find out more about Swagger at [http://swagger.io](http://swagger.io) or on [irc.freenode.net, #swagger](http://swagger.io/irc/). For this sample, you can use the api key `special-key` to test the authorization filters.&quot;,&quot;version&quot;:&quot;1.0.0&quot;,&quot;title&quot;:&quot;Swagger Petstore&quot;,&quot;termsOfService&quot;:&quot;http://swagger.io/terms/&quot;,&quot;contact&quot;:{&quot;email&quot;:&quot;apiteam@swagger.io&quot;},&quot;license&quot;:{&quot;name&quot;:&quot;Apache 2.0&quot;,&quot;url&quot;:&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;}},&quot;host&quot;:&quot;petstore.swagger.io&quot;,&quot;basePath&quot;:&quot;/v2&quot;,&quot;tags&quot;:[{&quot;name&quot;:&quot;pet&quot;,&quot;description&quot;:&quot;Everything about your Pets&quot;,&quot;externalDocs&quot;:{&quot;description&quot;:&quot;Find out more&quot;,&quot;url&quot;:&quot;http://swagger.io&quot;}},{&quot;name&quot;:&quot;store&quot;,&quot;description&quot;:&quot;Access to Petstore orders&quot;},{&quot;name&quot;:&quot;user&quot;,&quot;description&quot;:&quot;Operations about user&quot;,&quot;externalDocs&quot;:{&quot;description&quot;:&quot;Find out more about our store&quot;,&quot;url&quot;:&quot;http://swagger.io&quot;}}],&quot;schemes&quot;:[&quot;https&quot;,&quot;http&quot;],&quot;paths&quot;:{&quot;/pet&quot;:{&quot;post&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;Add a new pet to the store&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;addPet&quot;,&quot;consumes&quot;:[&quot;application/json&quot;,&quot;application/xml&quot;],&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;in&quot;:&quot;body&quot;,&quot;name&quot;:&quot;body&quot;,&quot;description&quot;:&quot;Pet object that needs to be added to the store&quot;,&quot;required&quot;:true,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/Pet&quot;}}],&quot;responses&quot;:{&quot;405&quot;:{&quot;description&quot;:&quot;Invalid input&quot;}},&quot;security&quot;:[{&quot;petstore_auth&quot;:[&quot;write:pets&quot;,&quot;read:pets&quot;]}]},&quot;put&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;Update an existing pet&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;updatePet&quot;,&quot;consumes&quot;:[&quot;application/json&quot;,&quot;application/xml&quot;],&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;in&quot;:&quot;body&quot;,&quot;name&quot;:&quot;body&quot;,&quot;description&quot;:&quot;Pet object that needs to be added to the store&quot;,&quot;required&quot;:true,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/Pet&quot;}}],&quot;responses&quot;:{&quot;400&quot;:{&quot;description&quot;:&quot;Invalid ID supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;Pet not found&quot;},&quot;405&quot;:{&quot;description&quot;:&quot;Validation exception&quot;}},&quot;security&quot;:[{&quot;petstore_auth&quot;:[&quot;write:pets&quot;,&quot;read:pets&quot;]}]}},&quot;/pet/findByStatus&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;Finds Pets by status&quot;,&quot;description&quot;:&quot;Multiple status values can be provided with comma separated strings&quot;,&quot;operationId&quot;:&quot;findPetsByStatus&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;status&quot;,&quot;in&quot;:&quot;query&quot;,&quot;description&quot;:&quot;Status values that need to be considered for filter&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;enum&quot;:[&quot;available&quot;,&quot;pending&quot;,&quot;sold&quot;],&quot;default&quot;:&quot;available&quot;},&quot;collectionFormat&quot;:&quot;multi&quot;}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;$ref&quot;:&quot;#/definitions/Pet&quot;}}},&quot;400&quot;:{&quot;description&quot;:&quot;Invalid status value&quot;}},&quot;security&quot;:[{&quot;petstore_auth&quot;:[&quot;write:pets&quot;,&quot;read:pets&quot;]}]}},&quot;/pet/findByTags&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;Finds Pets by tags&quot;,&quot;description&quot;:&quot;Muliple tags can be provided with comma separated strings. Use tag1, tag2, tag3 for testing.&quot;,&quot;operationId&quot;:&quot;findPetsByTags&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;tags&quot;,&quot;in&quot;:&quot;query&quot;,&quot;description&quot;:&quot;Tags to filter by&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;collectionFormat&quot;:&quot;multi&quot;}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;$ref&quot;:&quot;#/definitions/Pet&quot;}}},&quot;400&quot;:{&quot;description&quot;:&quot;Invalid tag value&quot;}},&quot;security&quot;:[{&quot;petstore_auth&quot;:[&quot;write:pets&quot;,&quot;read:pets&quot;]}],&quot;deprecated&quot;:true}},&quot;/pet/{petId}&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;Find pet by ID&quot;,&quot;description&quot;:&quot;Returns a single pet&quot;,&quot;operationId&quot;:&quot;getPetById&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;petId&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;ID of pet to return&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/Pet&quot;}},&quot;400&quot;:{&quot;description&quot;:&quot;Invalid ID supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;Pet not found&quot;}},&quot;security&quot;:[{&quot;api_key&quot;:[]}]},&quot;post&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;Updates a pet in the store with form data&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;updatePetWithForm&quot;,&quot;consumes&quot;:[&quot;application/x-www-form-urlencoded&quot;],&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;petId&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;ID of pet that needs to be updated&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},{&quot;name&quot;:&quot;name&quot;,&quot;in&quot;:&quot;formData&quot;,&quot;description&quot;:&quot;Updated name of the pet&quot;,&quot;required&quot;:false,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;status&quot;,&quot;in&quot;:&quot;formData&quot;,&quot;description&quot;:&quot;Updated status of the pet&quot;,&quot;required&quot;:false,&quot;type&quot;:&quot;string&quot;}],&quot;responses&quot;:{&quot;405&quot;:{&quot;description&quot;:&quot;Invalid input&quot;}},&quot;security&quot;:[{&quot;petstore_auth&quot;:[&quot;write:pets&quot;,&quot;read:pets&quot;]}]},&quot;delete&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;Deletes a pet&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;deletePet&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;api_key&quot;,&quot;in&quot;:&quot;header&quot;,&quot;required&quot;:false,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;petId&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;Pet id to delete&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;}],&quot;responses&quot;:{&quot;400&quot;:{&quot;description&quot;:&quot;Invalid ID supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;Pet not found&quot;}},&quot;security&quot;:[{&quot;petstore_auth&quot;:[&quot;write:pets&quot;,&quot;read:pets&quot;]}]}},&quot;/pet/{petId}/uploadImage&quot;:{&quot;post&quot;:{&quot;tags&quot;:[&quot;pet&quot;],&quot;summary&quot;:&quot;uploads an image&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;uploadFile&quot;,&quot;consumes&quot;:[&quot;multipart/form-data&quot;],&quot;produces&quot;:[&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;petId&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;ID of pet to update&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},{&quot;name&quot;:&quot;additionalMetadata&quot;,&quot;in&quot;:&quot;formData&quot;,&quot;description&quot;:&quot;Additional data to pass to server&quot;,&quot;required&quot;:false,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;file&quot;,&quot;in&quot;:&quot;formData&quot;,&quot;description&quot;:&quot;file to upload&quot;,&quot;required&quot;:false,&quot;type&quot;:&quot;file&quot;}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/ApiResponse&quot;}}},&quot;security&quot;:[{&quot;petstore_auth&quot;:[&quot;write:pets&quot;,&quot;read:pets&quot;]}]}},&quot;/store/inventory&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;store&quot;],&quot;summary&quot;:&quot;Returns pet inventories by status&quot;,&quot;description&quot;:&quot;Returns a map of status codes to quantities&quot;,&quot;operationId&quot;:&quot;getInventory&quot;,&quot;produces&quot;:[&quot;application/json&quot;],&quot;parameters&quot;:[],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;additionalProperties&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int32&quot;}}}},&quot;security&quot;:[{&quot;api_key&quot;:[]}]}},&quot;/store/order&quot;:{&quot;post&quot;:{&quot;tags&quot;:[&quot;store&quot;],&quot;summary&quot;:&quot;Place an order for a pet&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;placeOrder&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;in&quot;:&quot;body&quot;,&quot;name&quot;:&quot;body&quot;,&quot;description&quot;:&quot;order placed for purchasing the pet&quot;,&quot;required&quot;:true,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/Order&quot;}}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/Order&quot;}},&quot;400&quot;:{&quot;description&quot;:&quot;Invalid Order&quot;}}}},&quot;/store/order/{orderId}&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;store&quot;],&quot;summary&quot;:&quot;Find purchase order by ID&quot;,&quot;description&quot;:&quot;For valid response try integer IDs with value &gt;= 1 and &lt;= 10. Other values will generated exceptions&quot;,&quot;operationId&quot;:&quot;getOrderById&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;orderId&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;ID of pet that needs to be fetched&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;integer&quot;,&quot;maximum&quot;:10,&quot;minimum&quot;:1,&quot;format&quot;:&quot;int64&quot;}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/Order&quot;}},&quot;400&quot;:{&quot;description&quot;:&quot;Invalid ID supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;Order not found&quot;}}},&quot;delete&quot;:{&quot;tags&quot;:[&quot;store&quot;],&quot;summary&quot;:&quot;Delete purchase order by ID&quot;,&quot;description&quot;:&quot;For valid response try integer IDs with positive integer value. Negative or non-integer values will generate API errors&quot;,&quot;operationId&quot;:&quot;deleteOrder&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;orderId&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;ID of the order that needs to be deleted&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;integer&quot;,&quot;minimum&quot;:1,&quot;format&quot;:&quot;int64&quot;}],&quot;responses&quot;:{&quot;400&quot;:{&quot;description&quot;:&quot;Invalid ID supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;Order not found&quot;}}}},&quot;/user&quot;:{&quot;post&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Create user&quot;,&quot;description&quot;:&quot;This can only be done by the logged in user.&quot;,&quot;operationId&quot;:&quot;createUser&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;in&quot;:&quot;body&quot;,&quot;name&quot;:&quot;body&quot;,&quot;description&quot;:&quot;Created user object&quot;,&quot;required&quot;:true,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/User&quot;}}],&quot;responses&quot;:{&quot;default&quot;:{&quot;description&quot;:&quot;successful operation&quot;}}}},&quot;/user/createWithArray&quot;:{&quot;post&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Creates list of users with given input array&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;createUsersWithArrayInput&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;in&quot;:&quot;body&quot;,&quot;name&quot;:&quot;body&quot;,&quot;description&quot;:&quot;List of user object&quot;,&quot;required&quot;:true,&quot;schema&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;$ref&quot;:&quot;#/definitions/User&quot;}}}],&quot;responses&quot;:{&quot;default&quot;:{&quot;description&quot;:&quot;successful operation&quot;}}}},&quot;/user/createWithList&quot;:{&quot;post&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Creates list of users with given input array&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;createUsersWithListInput&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;in&quot;:&quot;body&quot;,&quot;name&quot;:&quot;body&quot;,&quot;description&quot;:&quot;List of user object&quot;,&quot;required&quot;:true,&quot;schema&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;items&quot;:{&quot;$ref&quot;:&quot;#/definitions/User&quot;}}}],&quot;responses&quot;:{&quot;default&quot;:{&quot;description&quot;:&quot;successful operation&quot;}}}},&quot;/user/login&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Logs user into the system&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;loginUser&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;username&quot;,&quot;in&quot;:&quot;query&quot;,&quot;description&quot;:&quot;The user name for login&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;password&quot;,&quot;in&quot;:&quot;query&quot;,&quot;description&quot;:&quot;The password for login in clear text&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;string&quot;}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;headers&quot;:{&quot;X-Rate-Limit&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int32&quot;,&quot;description&quot;:&quot;calls per hour allowed by the user&quot;},&quot;X-Expires-After&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;date-time&quot;,&quot;description&quot;:&quot;date in UTC when token expires&quot;}}},&quot;400&quot;:{&quot;description&quot;:&quot;Invalid username/password supplied&quot;}}}},&quot;/user/logout&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Logs out current logged in user session&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;logoutUser&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[],&quot;responses&quot;:{&quot;default&quot;:{&quot;description&quot;:&quot;successful operation&quot;}}}},&quot;/user/{username}&quot;:{&quot;get&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Get user by user name&quot;,&quot;description&quot;:&quot;&quot;,&quot;operationId&quot;:&quot;getUserByName&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;username&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;The name that needs to be fetched. Use user1 for testing. &quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;string&quot;}],&quot;responses&quot;:{&quot;200&quot;:{&quot;description&quot;:&quot;successful operation&quot;,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/User&quot;}},&quot;400&quot;:{&quot;description&quot;:&quot;Invalid username supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;User not found&quot;}}},&quot;put&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Updated user&quot;,&quot;description&quot;:&quot;This can only be done by the logged in user.&quot;,&quot;operationId&quot;:&quot;updateUser&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;username&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;name that need to be updated&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;string&quot;},{&quot;in&quot;:&quot;body&quot;,&quot;name&quot;:&quot;body&quot;,&quot;description&quot;:&quot;Updated user object&quot;,&quot;required&quot;:true,&quot;schema&quot;:{&quot;$ref&quot;:&quot;#/definitions/User&quot;}}],&quot;responses&quot;:{&quot;400&quot;:{&quot;description&quot;:&quot;Invalid user supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;User not found&quot;}}},&quot;delete&quot;:{&quot;tags&quot;:[&quot;user&quot;],&quot;summary&quot;:&quot;Delete user&quot;,&quot;description&quot;:&quot;This can only be done by the logged in user.&quot;,&quot;operationId&quot;:&quot;deleteUser&quot;,&quot;produces&quot;:[&quot;application/xml&quot;,&quot;application/json&quot;],&quot;parameters&quot;:[{&quot;name&quot;:&quot;username&quot;,&quot;in&quot;:&quot;path&quot;,&quot;description&quot;:&quot;The name that needs to be deleted&quot;,&quot;required&quot;:true,&quot;type&quot;:&quot;string&quot;}],&quot;responses&quot;:{&quot;400&quot;:{&quot;description&quot;:&quot;Invalid username supplied&quot;},&quot;404&quot;:{&quot;description&quot;:&quot;User not found&quot;}}}}},&quot;securityDefinitions&quot;:{&quot;petstore_auth&quot;:{&quot;type&quot;:&quot;oauth2&quot;,&quot;authorizationUrl&quot;:&quot;http://petstore.swagger.io/oauth/dialog&quot;,&quot;flow&quot;:&quot;implicit&quot;,&quot;scopes&quot;:{&quot;write:pets&quot;:&quot;modify pets in your account&quot;,&quot;read:pets&quot;:&quot;read your pets&quot;}},&quot;api_key&quot;:{&quot;type&quot;:&quot;apiKey&quot;,&quot;name&quot;:&quot;api_key&quot;,&quot;in&quot;:&quot;header&quot;}},&quot;definitions&quot;:{&quot;Order&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:{&quot;id&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},&quot;petId&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},&quot;quantity&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int32&quot;},&quot;shipDate&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;format&quot;:&quot;date-time&quot;},&quot;status&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;description&quot;:&quot;Order Status&quot;,&quot;enum&quot;:[&quot;placed&quot;,&quot;approved&quot;,&quot;delivered&quot;]},&quot;complete&quot;:{&quot;type&quot;:&quot;boolean&quot;,&quot;default&quot;:false}},&quot;xml&quot;:{&quot;name&quot;:&quot;Order&quot;}},&quot;Category&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:{&quot;id&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},&quot;name&quot;:{&quot;type&quot;:&quot;string&quot;}},&quot;xml&quot;:{&quot;name&quot;:&quot;Category&quot;}},&quot;User&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:{&quot;id&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},&quot;username&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;firstName&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;lastName&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;email&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;password&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;phone&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;userStatus&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int32&quot;,&quot;description&quot;:&quot;User Status&quot;}},&quot;xml&quot;:{&quot;name&quot;:&quot;User&quot;}},&quot;Tag&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:{&quot;id&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},&quot;name&quot;:{&quot;type&quot;:&quot;string&quot;}},&quot;xml&quot;:{&quot;name&quot;:&quot;Tag&quot;}},&quot;Pet&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;required&quot;:[&quot;name&quot;,&quot;photoUrls&quot;],&quot;properties&quot;:{&quot;id&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int64&quot;},&quot;category&quot;:{&quot;$ref&quot;:&quot;#/definitions/Category&quot;},&quot;name&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;example&quot;:&quot;doggie&quot;},&quot;photoUrls&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;xml&quot;:{&quot;name&quot;:&quot;photoUrl&quot;,&quot;wrapped&quot;:true},&quot;items&quot;:{&quot;type&quot;:&quot;string&quot;}},&quot;tags&quot;:{&quot;type&quot;:&quot;array&quot;,&quot;xml&quot;:{&quot;name&quot;:&quot;tag&quot;,&quot;wrapped&quot;:true},&quot;items&quot;:{&quot;$ref&quot;:&quot;#/definitions/Tag&quot;}},&quot;status&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;description&quot;:&quot;pet status in the store&quot;,&quot;enum&quot;:[&quot;available&quot;,&quot;pending&quot;,&quot;sold&quot;]}},&quot;xml&quot;:{&quot;name&quot;:&quot;Pet&quot;}},&quot;ApiResponse&quot;:{&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:{&quot;code&quot;:{&quot;type&quot;:&quot;integer&quot;,&quot;format&quot;:&quot;int32&quot;},&quot;type&quot;:{&quot;type&quot;:&quot;string&quot;},&quot;message&quot;:{&quot;type&quot;:&quot;string&quot;}}}},&quot;externalDocs&quot;:{&quot;description&quot;:&quot;Find out more about Swagger&quot;,&quot;url&quot;:&quot;http://swagger.io&quot;}}, dom_id: '#swagger-ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIBundle.SwaggerUIStandalonePreset ], layout: &quot;StandaloneLayout&quot; }) 能想到的暂时就这几个办法，有发现更好的就继续更新。 参考： Writing OpenAPI (Swagger) Specification Tutorial Swagger 从入门到精通 OpenAPI Specification 2.0 Converting Swagger specification JSON to HTML documentation","link":"/2018/11/29/2018/11/2018-11-29-SwaggerOnePage/"},{"title":"Kafka Learn - 01","text":"Kafka最近报名了公司的Kafka Training , 主要是自己看视频。今天主要记录一下使用到的命令和练习的Java Code，前期的概念有时间再总结下。从CLI和Java Code的这几节来说，主要练习了Producer和Comsumer的使用，数据的推送和读取应该是一般人使用Kafka最基本和最常用的操作了吧。 安装与配置http://kafka.apache.org/downloads 不同系统的配置大同小异，为了省事我用的是windows，需要把D:\\ProgramsDev\\kafka_2.13-2.8.0\\bin\\windows加入到环境变量中，linux到bin就够了。 在data目录下新建kafka和zookeeper目录。 目前要动的配置文件就两个，把上面新建的目录路径加入到配置文件中： server.properties1234567# A comma separated list of directories under which to store log fileslog.dirs=D:/ProgramsDev/kafka_2.13-2.8.0/data/kafka# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=3 zookeeper.properties12# the directory where the snapshot is stored.dataDir=D:/ProgramsDev/kafka_2.13-2.8.0/data/zookeeper 启动服务注意配置文件的相对路径 kafka bootstrap-server 127.0.0.1:9092 1kafka-server-start ./config/server.properties zookeeper 127.0.0.1:2181 1zookeeper-server-start ./config/zookeeper.properties CLI使用视频教材中演示的很好，这边就只是简单的把Command列一下，可以自己试试，理解下。 1234567891011121314151617181920212223242526272829303132kafka-topics --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 3 --replication-factor 1kafka-topics --zookeeper 127.0.0.1:2181 --topic second_topic --create --partitions 6 --replication-factor 1kafka-topics --zookeeper 127.0.0.1:2181 --listkafka-topics --zookeeper 127.0.0.1:2181 --topic first_topic --describekafka-topics --zookeeper 127.0.0.1:2181 --topic second_topic --describekafka-topics --zookeeper 127.0.0.1:2181 --topic second_topic --deletekafka-console-producer --broker-list 127.0.0.1:9092 --topic first_topickafka-console-producer --broker-list 127.0.0.1:9092 --topic first_topic --producer-property acks=allkafka-topics --zookeeper 127.0.0.1:2181 --listkafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-second-application --from-beginningkafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-first-applicationkafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --from-beginningkafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topickafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-third-applicationkafka-consumer-groups --bootstrap-server localhost:9092 --listkafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-first-applicationkafka-consumer-groups --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --shift-by 2 --execute --topic first_topickafka-consumer-groups --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute --topic first_topic Offset Explorer (formerly Kafka Tool)https://www.kafkatool.com/ 这是一个可以查看Kafka信息(Broker,Topic, Consumer, Position…)的工具。 Java Code Practice实际操作其实和CLI差不多的，只是用Java来实现。 pom.xml1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;fun.bearfly&lt;/groupId&gt; &lt;artifactId&gt;kafka-learn&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.13&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.7.32&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; ProducerDemo最简单的一个Producer 1234567891011121314151617181920212223242526272829package fun.bearfly.kafka.learn;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import java.util.Properties;public class ProducerDemo { public static void main(String[] args) { String bootstrapServers = &quot;127.0.0.1:9092&quot;; Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, bootstrapServers); properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); //create producer record ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;first_topic&quot;, &quot;hello world&quot;); producer.send(record); producer.flush(); producer.close(); }} ProducerDemoKeys带Keys 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package fun.bearfly.kafka.learn;import org.apache.kafka.clients.producer.*;import org.apache.kafka.common.serialization.StringSerializer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Properties;import java.util.concurrent.ExecutionException;public class ProducerDemoKeys { public static void main(String[] args) throws ExecutionException, InterruptedException { Logger logger = LoggerFactory.getLogger(ProducerDemoKeys.class); String bootstrapServers = &quot;127.0.0.1:9092&quot;; Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, bootstrapServers); properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); //create producer record for (int i = 0; i &lt; 10; i++) { String topic = &quot;first_topic&quot;; String value = &quot;hello world &quot; + Integer.toString(i); String key = &quot;id_&quot; + Integer.toString(i); logger.info(&quot;Key: &quot; + key); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, key, value); producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) { if (e == null) { StringBuilder sb = new StringBuilder(); sb.append(&quot;Received new metadata. \\n&quot;); sb.append(&quot;topic:&quot;).append(recordMetadata.topic()).append(&quot;\\n&quot;); sb.append(&quot;partition:&quot;).append(recordMetadata.partition()).append(&quot;\\n&quot;); sb.append(&quot;offset:&quot;).append(recordMetadata.offset()).append(&quot;\\n&quot;); sb.append(&quot;timestamp:&quot;).append(recordMetadata.timestamp()).append(&quot;\\n&quot;); logger.info(sb.toString()); } else { logger.error(&quot;Error while producing&quot;, e); } } }).get(); } producer.flush(); producer.close(); }} ProducerDemoWithCallBack带回调 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package fun.bearfly.kafka.learn;import org.apache.kafka.clients.producer.*;import org.apache.kafka.common.serialization.StringSerializer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Properties;public class ProducerDemoWithCallBack { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(ProducerDemoWithCallBack.class); String bootstrapServers = &quot;127.0.0.1:9092&quot;; Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, bootstrapServers); properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); //create producer record for (int i = 0; i &lt; 10; i++) { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;first_topic&quot;, &quot;hello world&quot; + Integer.toString(i)); producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) { if (e == null) { StringBuilder sb = new StringBuilder(); sb.append(&quot;Received new metadata. \\n&quot;); sb.append(&quot;topic:&quot;).append(recordMetadata.topic()).append(&quot;\\n&quot;); sb.append(&quot;partition:&quot;).append(recordMetadata.partition()).append(&quot;\\n&quot;); sb.append(&quot;offset:&quot;).append(recordMetadata.offset()).append(&quot;\\n&quot;); sb.append(&quot;timestamp:&quot;).append(recordMetadata.timestamp()).append(&quot;\\n&quot;); logger.info(sb.toString()); } else { logger.error(&quot;Error while producing&quot;, e); } } }); } producer.flush(); producer.close(); }} ConsumerDemo订阅Topic 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package fun.bearfly.kafka.learn;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.StringDeserializer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.time.Duration;import java.util.Arrays;import java.util.Properties;public class ConsumerDemo { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(ConsumerDemo.class.getName()); String bootstrapServers = &quot;127.0.0.1:9092&quot;; String groupId = &quot;my-fourth-application&quot;; String topic = &quot;first_topic&quot;; Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, bootstrapServers); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(properties); consumer.subscribe(Arrays.asList(topic)); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); for (ConsumerRecord&lt;String, String&gt; record : records) { logger.info(&quot;Key: &quot; + record.key() + &quot;, Value: &quot; + record.value()); logger.info(&quot;Partition: &quot; + record.partition() + &quot;, Offset: &quot; + record.offset()); } } }} ConsumerDemoGroups带Group 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package fun.bearfly.kafka.learn;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.StringDeserializer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.time.Duration;import java.util.Arrays;import java.util.Properties;public class ConsumerDemoGroups { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(ConsumerDemoGroups.class.getName()); String bootstrapServers = &quot;127.0.0.1:9092&quot;; String groupId = &quot;my-fifth-application&quot;; String topic = &quot;first_topic&quot;; Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, bootstrapServers); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(properties); consumer.subscribe(Arrays.asList(topic)); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); for (ConsumerRecord&lt;String, String&gt; record : records) { logger.info(&quot;Key: &quot; + record.key() + &quot;, Value: &quot; + record.value()); logger.info(&quot;Partition: &quot; + record.partition() + &quot;, Offset: &quot; + record.offset()); } } }} ConsumerDemoWithThread使用线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118package fun.bearfly.kafka.learn;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.errors.WakeupException;import org.apache.kafka.common.serialization.StringDeserializer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.time.Duration;import java.util.Arrays;import java.util.Properties;import java.util.concurrent.CountDownLatch;public class ConsumerDemoWithThread { public static void main(String[] args) { new ConsumerDemoWithThread().run(); } private ConsumerDemoWithThread() { } private void run() { Logger logger = LoggerFactory.getLogger(ConsumerDemoWithThread.class.getName()); String bootstrapServers = &quot;127.0.0.1:9092&quot;; String groupId = &quot;my-sixth-application&quot;; String topic = &quot;first_topic&quot;; CountDownLatch latch = new CountDownLatch(1); logger.info(&quot;Creating the consumer thread&quot;); Runnable myConsumerThread = new ConsumerRunnable( bootstrapServers, groupId, topic, latch ); Thread myThread = new Thread(myConsumerThread); myThread.start(); Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; { logger.info(&quot;Caught shutdown hook&quot;); ((ConsumerRunnable) myConsumerThread).shutdown(); try { latch.await(); } catch (InterruptedException e) { e.printStackTrace(); } logger.info(&quot;Application has exited&quot;); } )); try { latch.await(); } catch (InterruptedException e) { logger.error(&quot;Application got interrupted&quot;, e); } finally { logger.info(&quot;Application is closing&quot;); } } public class ConsumerRunnable implements Runnable { private CountDownLatch latch; private KafkaConsumer&lt;String, String&gt; consumer; private Logger logger = LoggerFactory.getLogger(ConsumerRunnable.class.getName()); public ConsumerRunnable(String bootstrapServers, String groupId, String topic, CountDownLatch latch ) { this.latch = latch; Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, bootstrapServers); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;); consumer = new KafkaConsumer&lt;String, String&gt;(properties); consumer.subscribe(Arrays.asList(topic)); } @Override public void run() { try { while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); for (ConsumerRecord&lt;String, String&gt; record : records) { logger.info(&quot;Key: &quot; + record.key() + &quot;, Value: &quot; + record.value()); logger.info(&quot;Partition: &quot; + record.partition() + &quot;, Offset: &quot; + record.offset()); } } } catch (WakeupException e) { logger.info(&quot;Received shutdown signal!&quot;); } finally { consumer.close(); latch.countDown(); } } public void shutdown() { // special method to interrupt consumer.poll() // will throw the exception WakeUpException consumer.wakeup(); } }} ConsumerDemoAssignSeek指定topic offset 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package fun.bearfly.kafka.learn;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.TopicPartition;import org.apache.kafka.common.serialization.StringDeserializer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.time.Duration;import java.util.Arrays;import java.util.Properties;public class ConsumerDemoAssignSeek { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(ConsumerDemoAssignSeek.class.getName()); String bootstrapServers = &quot;127.0.0.1:9092&quot;; String groupId = &quot;my-seven-application&quot;; String topic = &quot;first_topic&quot;; Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, bootstrapServers); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());// properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(properties); // assign and seek are mostly used to replay data or fetch a specific message // assign TopicPartition partitionToReadFrom = new TopicPartition(topic, 0); long offsetToReadFrom = 15L; consumer.assign(Arrays.asList(partitionToReadFrom)); //seek consumer.seek(partitionToReadFrom, offsetToReadFrom);// consumer.subscribe(Arrays.asList(topic)); int numberOfMessagesToRead = 5; boolean keepOnReading = true; int numberOfMessagesReadSoFar = 0; while (keepOnReading) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); for (ConsumerRecord&lt;String, String&gt; record : records) { numberOfMessagesReadSoFar += 1; logger.info(&quot;Key: &quot; + record.key() + &quot;, Value: &quot; + record.value()); logger.info(&quot;Partition: &quot; + record.partition() + &quot;, Offset: &quot; + record.offset()); if (numberOfMessagesReadSoFar &gt;= numberOfMessagesToRead) { keepOnReading = false; break; } } } logger.info(&quot;Exiting the application&quot;); }} 最后都是一些基本操作，深入学习后再回头总结下理论概念，实践出真知。","link":"/2021/08/08/2021/08/2021-08-09-kafka-learn-01/"},{"title":"Set up markdown wiki with mkdocs","text":"mkdocsmkdocs 是一个基于python的第三方库，MkDocs中文文档 MkDocs是一个快速、简单、华丽的静态网站生成器，适用于构建项目文档。文档源文件以Markdown编写，并使用一个YAML文件来进行配置。 基础安装使用安装主库1pip install mkdocs 安装 mkdocs-material 主题mkdocs有自带mkdocs和readthedocs两个主题，个人比较喜欢material这个主题。 pymdown-extersions增加了许多markdown的一些功能，比如一些图标。 12pip install mkdocs-materialpip install pymdown-extensions 创建项目下面是mkdocs内置的几个常用命令。 1234mkdocs new [dir-name] - Create a new project.mkdocs serve - Start the live-reloading docs server.mkdocs build - Build the documentation site.mkdocs -h - Print help message and exit. 创建项目就可以使用mkdocs new mywiki，会新建一个目录: docs目录存放的便是原始的我们编写的markdown文件 mkdocs.yml是mkdocs的主配置文件。 编辑文档试着直接在docs下编辑文件 然后启动服务. 12345mkdocs serveINFO - Building documentation...INFO - Cleaning site directoryINFO - Documentation built in 0.10 secondsINFO - [16:51:31] Serving on http://127.0.0.1:8000/ 打开链接，可以看到mkdocs会根据目录结构自动生成导航，最里面的标题和文档的一级Header是一致的。 mkdocs.yml像上面的导航，除了自动生成，也可以在mkdocs.yml里自定义配置。 123456site_name: My Docssite_url: https://example.com/nav: - Home: index.md - MyPage: - TestPage: folder/file.md 不过个人觉得还是自动生成方便，除非需要对特别的一些页面进行配置。 下面是我用的mkdocs.yml，也是从网上拷过来改的，主要支持了github自动发布。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129site_name: BFWikisite_url: http://localhost:8080/BFWikirepo_url: https://github.com/bearfly1990/BF-Wiki/tree/gh-pagessite_author: bearfly1990site_description: wiki for bearfly1990# copyright:# nav:# - Home: index.md# - About: about.md# - Kafka:# - Learn Note01: 2021/08/2021-08-09-kafka-learn-01.md# pages:# - [index.md, Home]# - [about.md, About]theme: # name: readthedocs name: material language: 'en' # logo: img/xxx.ico # favicon: img/facicon.ico/ primary: &quot;Blue Grey&quot; # accent: &quot;Pink&quot; features: # - tabs: true # - toc.integrate # - navigation.tracking - navigation.tabs - navigation.top highlightjs: true hljs_languages: - yaml - rust # nav_style: dark# extra:# search:# language: 'zh'markdown_extensions: - admonition - codehilite: guess_lang: false linenums: false - toc: permalink: &quot;#&quot; - footnotes - meta - def_list - pymdownx.arithmatex - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_generator: !!python/name:pymdownx.emoji.to_png - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist - pymdownx.tilde - pymdownx.highlight# site_name: JetBot# theme:# name: &quot;material&quot;# logo: images/logo.png# favicon: images/favicon.png# font: Incosolata# palette:# scheme: nvgreen# features:# - navigation.expand## repo_url: https://github.com/NVIDIA-AI-IOT/jetbot## plugins:# - search# use_directory_urls: false## edit_uri: blob/master/docs# markdown_extensions:# - pymdownx.tabbed# - pymdownx.keys# - pymdownx.snippets# - pymdownx.inlinehilite# - pymdownx.highlight:# use_pygments: true# - admonition# - pymdownx.details# - pymdownx.superfences# - attr_list # for image sizes https://github.com/mkdocs/mkdocs/issues/1678# # use_directory_urls - False to fix broken raw html image links# # https://github.com/mkdocs/mkdocs/issues/991### nav:## - Home: index.md# - Getting Started: getting_started.md# - Bill of Materials: bill_of_materials.md# - Hardware Setup: hardware_setup.md# - Software Setup:# - Using SD Card Image: software_setup/sd_card.md# - Using Docker Container: software_setup/docker.md# - Examples:# - Basic Motion: examples/basic_motion.md# - Teleoperation: examples/teleoperation.md# - Collision Avoidance: examples/collision_avoidance.md# - Road Following: examples/road_following.md# - Object Following: examples/object_following.md# - Reference:# - Third Party Kits: third_party_kits.md# - 3D Printing: 3d_printing.md# - Contributing: CONTRIBUTING.md# - Changes: CHANGELOG.md# - Wi-Fi setup: software_setup/wifi_setup.md# - Docker Tips: reference/docker_tips.md## extra_css:# - css/version-select.css# - css/colors.css# extra_javascript:# - js/version-select.js## google_analytics:# - UA-135919510-2# - auto 对了，如果想要支持中文搜索，在主题配置这边加上语言就行了。 1234theme: # name: readthedocs name: material language: 'zh' Build上面提到的链接访问是通过本地起的python http服务，会动态重新加载每次更新后的文档。如果想要把网站部署到服务器上，就需要先编译成静态网站(html/js/css)。 操作很简单，在项目目录下执行一下mkdocs build就可以，会在项目下生成site目录，这就是编译后的结果。 Deploy To Github另外，如果你配置过本地的github ssh登录，那你执行下面的命令，就能很方便的在github io上发布你的文档。 1mkdocs gh-deploy 比如我的，他会在项目本身的repository - https://github.com/bearfly1990/BF-Wiki/建立一个新的branch - gh-pages, 并把编译后的静态文件上传上去。 接着就可以在https://bearfly1990.github.io/BF-Wiki/看到我的文档。","link":"/2021/10/03/2021/10/2021-10-03-setup-mkdocs/"},{"title":"Files sharing by pywebio","text":"背景之前试过用pyftpdlib来使用ftp的方式来分享文件，但是发现文件比较大的时候会有问题，而且需要登录很不方便。 最近发现一个很好用的来建立web应用的库pywebio可以达到我想要的文件共享的效果，主要是构建真的很方便，不需要写前端，直接用python代码就可以了。 安装库https://github.com/pywebio/PyWebIO 1pip3 install pywebio 代码下面直接上代码，大家可以看到非常简洁，控件的定义也比较优雅，像一些简单的GUI界面都可以迁移过来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from pywebio.input import input, FLOAT, file_uploadfrom pywebio.output import put_text, put_file,close_popup, popup, put_buttons, put_markdownfrom pywebio.session import set_env, holdfrom pywebio import start_serverimport timeimport globimport osimport pywebio.output as outputfrom functools import partialdef upload(): list_files() files = file_upload(&quot;Upload a file&quot;, multiple=True, max_size='4G') for f in files: open(os.path.join(&quot;Shared&quot;, f['filename']), 'wb').write( f['content']) # alert(f&quot;Upload {f['filename']} Successfully!&quot;)def list_files(): all_files = glob.glob(&quot;shared/**&quot;, recursive=True) file_output_list = [] for filename in all_files: print(all_files) if os.path.isdir(filename): continue else: file_output_list.append(filename) with output.use_scope('files', clear=True): output.put_table([ ['file', 'full name', 'action'], *input_file_local(file_output_list) ]) def input_file_local(filename_list): put_files = [] for filename in filename_list: with open(filename, 'rb') as fh: put_files.append([put_file(name=os.path.basename(filename), content=fh.read()), filename, put_buttons(['delete'], onclick=partial(edit_row, filename=filename))]) return put_filesdef edit_row(choice, filename): print(choice, filename) # os.remove(filename) # list_files() alert(&quot;Delete is disabled&quot;)def alert(message, title='Info'): popup(title, [ put_text(message), put_buttons(['OK'], onclick=lambda _:close_popup()) ])def main(): while(True): upload()start_server(main, port=8080, debug=True, max_total_size=&quot;5G&quot;, max_payload_size=&quot;5G&quot;) 从代码里可以看出，我们上传的文件都放在shared这个目录，所以目前需要手动建立，后面上传的文件都会存在这里。 这里为了支持大文件的上传，加到了5G。 效果演示 这里我们把删除的操作给disable了，需要的话可以加回去。不过一般还是自己控制比较好。 123def edit_row(choice, filename): os.remove(filename) list_files() 参考 PyWebIo | 快速构建web应用","link":"/2021/11/18/2021/11/2021-11-18-file-share-pywebio/"},{"title":"Set up ftp server with pyftpdlib","text":"背景公司内部现在少了很多共享的路径，分享文件就很不方便。 在家的时候，想把文件从手机上传到电脑，也不方便（华为共享算一种方法）。 之前用python自带的http服务(python -m http.server 8080)可以方便分享文件给其他人，但是不能上传。 原来想着说写个页面支持上传文件，一直没有弄（虽然Alpha Test Platform支持，但没有更直接的）。 这两天想到了如果能实现简单的FTP服务，这样就方便文件交互了。试了一些方法，最后用pyftpdlib。 基础安装使用安装pyftpdlib1pip install pyftpdlib 直接使用命令启动1python -m pyftpdlib 一个简单的FTP服务器已经搭建完成，访问 ftp://127.0.0.1:2121 即可, 共享的路径是用户目录。 （默认IP为 127.0.0.1 、端口为 2121 ） 编写简易代码启动下面是mkdocs内置的几个常用命令。 123456789101112131415from pyftpdlib.authorizers import DummyAuthorizerfrom pyftpdlib.handlers import FTPHandlerfrom pyftpdlib.servers import FTPServer authorizer = DummyAuthorizer()# username, password, shared folder, permission for filesauthorizer.add_user('test', 'test', '.', perm='elradfmwMT')# authorizer.add_anonymous('/home/nobody') handler = FTPHandlerhandler.authorizer = authorizer server = FTPServer(('0.0.0.0', 21), handler)server.serve_forever() 访问ftpftp 启动之后，在windows文件夹浏览，就可以通过 ftp://pc-cx/访问了（pc-cx是启动服务的机器的名字，可以用IP，默认端口是21所以可以不用写）。 这个时候需要输入上面代码中设定的用户名(test)和密码(test)，就可以看到文件，并上传和下载。 在手机上，可以安装ftp客户端，然后输入配置参数，连接成功后就可以与电脑文件进行交互。 文件权限设置下面这句代码设置了用户名密码，ftp的路径，还有权限(perm) 1234567891011121314151617authorizer.add_user('test', 'test', '.', perm='elradfmwMT')perm权限选项读取权限：&quot;e&quot; =更改目录（CWD，CDUP命令）&quot;l&quot; =列表文件（LIST，NLST，STAT，MLSD，MLST，SIZE命令）&quot;r&quot; =从服务器检索文件（RETR命令）###写入权限：&quot;a&quot; =将数据追加到现有文件（APPE命令）&quot;d&quot; =删除文件或目录（DELE，RMD命令）&quot;f&quot; =重命名文件或目录（RNFR，RNTO命令）&quot;m&quot; =创建目录（MKD命令）&quot;w&quot; =将文件存储到服务器（STOR，STOU命令）&quot;M&quot;=更改文件模式/权限（SITE CHMOD命令）&quot;T&quot;=更改文件修改时间（SITE MFMT命令） 参考 Pyftpdlib文档 Pyftpdlib 使用方法","link":"/2021/10/27/2021/10/2021-10-27-pyftpdlib/"},{"title":"Combine Videos by ffmpeg","text":"背景之前写过文章Cut and Combine Tiktok Videos 使用的是Python的一个第三方库来moviepy来操作剪辑合并视频。 最近发现一个比较好用的视频处理组件ffmpeg - http://ffmpeg.org/，也可以实现一样的效果，并且试过同样6个并发的时候，performance好很多。 1ffmpeg -i input.mp4 output.avi 下载ffmpeg组件从官网下载Windows的编译好的版本，这次我们就只用到其中两个。 ffmpeg.exe ffplay.exe ffprobe.exe 分析因为视频的信息与处理都是通过命令行，所以定义一个Video类来储存基本信息。 123456789101112131415161718192021222324class Vedio(object): def __init__(self, file_name): vedio_attr = self.get_vedio_attribute(file_name) self.file_name = file_name self.duration = float(vedio_attr['format']['duration']) self.width = float(vedio_attr['streams'][0]['width']) self.height = float(vedio_attr['streams'][0]['height']) self.temp_file = &quot;&quot; self.start_time = 0 self.ended_time = 0 def cut_video(self, width=300, height=300): cmd = f'ffmpeg -ss 00:00:00 -t {self.ended_time} -i {self.file_name} -vf &quot;scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2&quot; -c:v libx264 -crf 23 -c:a copy -bsf:v h264_mp4toannexb -f mpegts {self.temp_file}.ts -y' print(cmd) os.system(cmd) def get_vedio_attribute(self, file): cmd = f'ffprobe -select_streams v -show_entries format=duration,size,bit_rate,filename -show_streams -v quiet -of csv=&quot;p=0&quot; -of json -i {file}' p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) p.wait() strout, strerr = p.communicate() attr_json = json.loads(strout) return attr_json 在这里，为了剪切后的视频能自适应新的视频大小，所以使用了pad这个过滤器,自动计算大小。 1-vf &quot;scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2&quot; get_vedio_attribute返回的是视频的一些基础信息,今天就只用到了视频的总时间(duration)和长(height)宽(width) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374{ &quot;streams&quot;: [ { &quot;index&quot;: 0, &quot;codec_name&quot;: &quot;h264&quot;, &quot;codec_long_name&quot;: &quot;H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10&quot;, &quot;profile&quot;: &quot;High&quot;, &quot;codec_type&quot;: &quot;video&quot;, &quot;codec_tag_string&quot;: &quot;avc1&quot;, &quot;codec_tag&quot;: &quot;0x31637661&quot;, &quot;width&quot;: 1280, &quot;height&quot;: 1280, &quot;coded_width&quot;: 1280, &quot;coded_height&quot;: 1280, &quot;closed_captions&quot;: 0, &quot;film_grain&quot;: 0, &quot;has_b_frames&quot;: 2, &quot;sample_aspect_ratio&quot;: &quot;1:1&quot;, &quot;display_aspect_ratio&quot;: &quot;1:1&quot;, &quot;pix_fmt&quot;: &quot;yuv420p&quot;, &quot;level&quot;: 40, &quot;color_range&quot;: &quot;tv&quot;, &quot;color_space&quot;: &quot;bt709&quot;, &quot;color_transfer&quot;: &quot;bt709&quot;, &quot;color_primaries&quot;: &quot;bt709&quot;, &quot;chroma_location&quot;: &quot;left&quot;, &quot;field_order&quot;: &quot;progressive&quot;, &quot;refs&quot;: 1, &quot;is_avc&quot;: &quot;true&quot;, &quot;nal_length_size&quot;: &quot;4&quot;, &quot;id&quot;: &quot;0x1&quot;, &quot;r_frame_rate&quot;: &quot;30/1&quot;, &quot;avg_frame_rate&quot;: &quot;30/1&quot;, &quot;time_base&quot;: &quot;1/90000&quot;, &quot;start_pts&quot;: 4140, &quot;start_time&quot;: &quot;0.046000&quot;, &quot;duration_ts&quot;: 8937000, &quot;duration&quot;: &quot;99.300000&quot;, &quot;bit_rate&quot;: &quot;1225216&quot;, &quot;bits_per_raw_sample&quot;: &quot;8&quot;, &quot;nb_frames&quot;: &quot;2979&quot;, &quot;disposition&quot;: { &quot;default&quot;: 1, &quot;dub&quot;: 0, &quot;original&quot;: 0, &quot;comment&quot;: 0, &quot;lyrics&quot;: 0, &quot;karaoke&quot;: 0, &quot;forced&quot;: 0, &quot;hearing_impaired&quot;: 0, &quot;visual_impaired&quot;: 0, &quot;clean_effects&quot;: 0, &quot;attached_pic&quot;: 0, &quot;timed_thumbnails&quot;: 0, &quot;captions&quot;: 0, &quot;descriptions&quot;: 0, &quot;metadata&quot;: 0, &quot;dependent&quot;: 0, &quot;still_image&quot;: 0 }, &quot;tags&quot;: { &quot;language&quot;: &quot;und&quot;, &quot;handler_name&quot;: &quot;VideoHandler&quot;, &quot;vendor_id&quot;: &quot;[0][0][0][0]&quot; } } ], &quot;format&quot;: { &quot;filename&quot;: &quot;output.mp4&quot;, &quot;duration&quot;: &quot;99.347000&quot;, &quot;size&quot;: &quot;16912977&quot;, &quot;bit_rate&quot;: &quot;1361931&quot; }} 完整的代码主体思路就是遍历当前目录中的mp4文件，根据所有视频最大的长和宽，每个视频都生成一个临时ts文件，然后再把ts文件合并在一个mp4中。 后面顺便生成mp3可以到时做铃声。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179import shutilimport osimport globfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETEDfrom datetime import datetimeimport subprocessimport jsonimport tracebackimport pathlib&quot;&quot;&quot;author: bearfly1990create at: 02/01/2021description: Utils for send emailChange log:Date Author Version Description02/17/2021 bearfly1990 1.0.1 update combine video hight/width resize logic to adapt all videos.02/21/2021 bearfly1990 1.0.2 Get max hight/max width from all the input videos, not hard code11/01/2021 bearfly1990 1.0.3 Using ffmpeg/ffprobe to deal with the image&quot;&quot;&quot;# def hcf(x, y):# if x &gt; y:# smaller = y# else:# smaller = x# for i in range(1,smaller + 1):# if((x % i == 0) and (y % i == 0)):# hcf = i# return hcfclass Vedio(object): def __init__(self, file_name): vedio_attr = self.get_vedio_attribute(file_name) self.file_name = file_name self.duration = float(vedio_attr['format']['duration']) self.width = float(vedio_attr['streams'][0]['width']) self.height = float(vedio_attr['streams'][0]['height']) self.temp_file = &quot;&quot; self.start_time = 0 self.ended_time = 0 def cut_video(self, width=300, height=300): # cmd_cut = f&quot;ffmpeg -ss 00:00:00 -t {time_period} -i {input_file} -s {width}x{height} -codec copy {output_file}&quot; # cmd = f'ffmpeg -ss 00:00:00 -t {time_period} -i {input_file} -vf &quot;scale=1920:-2&quot; -c:v libx264 -crf 1 -c:a copy {output_file}' # cmd = f'ffmpeg -ss 00:00:00 -t {self.ended_time} -i {self.file_name} -vf &quot;scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2&quot; -c:v libx264 -crf 23 -c:a copy {self.temp_file} -y' cmd = f'ffmpeg -ss 00:00:00 -t {self.ended_time} -i {self.file_name} -vf &quot;scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2&quot; -c:v libx264 -crf 23 -c:a copy -bsf:v h264_mp4toannexb -f mpegts {self.temp_file}.ts -y' # -s {width_max}*{height_max} ,setsar={width_rate}:{height_rate} # cmd = fr'ffmpeg -ss 00:00:00 -t 15.509000000000002 -i .\\test.mp4 -vf &quot;scale=300:300&quot; -c:v libx264 -crf 1 -c:a copy ./temp\\.\\test1111.mp4' print(cmd) os.system(cmd) # p = subprocess.Popen(cmd,stdout = subprocess.PIPE, stderr = subprocess.STDOUT) # p.wait() def get_vedio_attribute(self, file): cmd = f'ffprobe -select_streams v -show_entries format=duration,size,bit_rate,filename -show_streams -v quiet -of csv=&quot;p=0&quot; -of json -i {file}' # result = subprocess.Popen([&quot;ffprobe&quot;, &quot;video.mp4&quot;],stdout = subprocess.PIPE, stderr = subprocess.STDOUT) p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) p.wait() strout, strerr = p.communicate() attr_json = json.loads(strout) return attr_jsonclass TiktokUtil(object): output_folder = './output' temp_folder = './temp' max_workers = 6 def __init__(self, remove_watermark=True, input_folder='', output_name='combined.mp4'): self.remove_watermark = remove_watermark self.output_name = output_name self.input_folder = input_folder self.video_list = [] self.max_x_list = [] self.max_y_list = [] def cut_video(self, video: Vedio): self.max_x = max(self.max_x_list) self.max_y = max(self.max_y_list) video.cut_video(self.max_x, self.max_y) def call_combine_video(self, list_file=&quot;list.txt&quot;, output_file=&quot;output.mp4&quot;): # cmd = f&quot;ffmpeg -safe 0 -f concat -i {list_file} -c copy {output_file}&quot; concat_files = '|'.join([f'temp\\{video.file_name}.ts' for video in self.video_list]) cmd = f'ffmpeg -i &quot;concat:{concat_files}&quot; -c copy -bsf:a aac_adtstoasc -movflags +faststart output.mp4 -y' print(cmd) os.system(cmd) def transfer_video_to_audio(self, input_file=&quot;output.mp4&quot;, output_file=&quot;output.mp3&quot;): cmd = f&quot;ffmpeg -i {input_file} -b:a 192K -vn {output_file}&quot; print(cmd) os.system(cmd) def get_vedio_duration(self, file): return float(self.get_vedio_attribute(file)['format']['duration']) def convert_video(self, file): try: # target = os.path.join(self.output_folder, file) # 拼接文件名路径 # try: # if not os.path.exists(os.path.dirname(target)): # os.path.isdir(os.path.join(root, output)) os.path.join(root, output) # os.makedirs(os.path.dirname(target)) # except Exception as e: # print('have error when create subfolder:',e) target = os.path.join(self.temp_folder, file) # 拼接文件名路径 try: # print('==================', os.path.exists(os.path.dirname(target))) # os.path.isdir(os.path.join(root, output)) os.path.join(root, output) if not os.path.exists(os.path.dirname(target)): os.makedirs(os.path.dirname(target)) except Exception as e: print('have error when create temp folder:', e) traceback.print_exc() video = Vedio(file) if self.remove_watermark: total_seconds = video.duration video.ended_time = total_seconds - 3.6 video.temp_file = os.path.join(self.temp_folder, file) self.max_x_list.append(video.width) self.max_y_list.append(video.height) self.video_list.append(video) # 将加载完后的视频加入列表 except Exception as e: print('have error:', e) traceback.print_exc() finally: print(file, 'done') def combine_videos(self): self.max_x = max(self.max_x_list) self.max_y = max(self.max_y_list) executor = ThreadPoolExecutor(max_workers=self.max_workers) all_task = [executor.submit(self.cut_video, (video)) for video in self.video_list] wait(all_task, return_when=ALL_COMPLETED) current_folder = pathlib.Path(__file__).parent.resolve() with open('./list.txt', 'w', encoding='utf-8') as fin: fin.writelines([f&quot;file '{os.path.join(current_folder, video.temp_file)}'\\r\\n&quot;.replace( '\\\\', '/') for video in self.video_list]) self.call_combine_video() self.transfer_video_to_audio() shutil.rmtree(self.temp_folder) if os.path.exists( self.temp_folder) else None def preprocess_videos(self): files = glob.glob(f'{self.input_folder}/*.mp4', recursive=True) executor = ThreadPoolExecutor(max_workers=self.max_workers) all_task = [executor.submit(self.convert_video, (file)) for file in files] wait(all_task, return_when=ALL_COMPLETED)if __name__ == &quot;__main__&quot;: start_time = datetime.now() files = glob.glob('**/*.mp4', recursive=True) if not files: print('no files found') exit(0) dirs = list(set(['.' if os.path.dirname(file) == '' else os.path.dirname(file) for file in files])) for dir in dirs: tiktok_util = TiktokUtil(input_folder=dir) tiktok_util.preprocess_videos() tiktok_util.combine_videos() ended_time = datetime.now() print(f'time cost: {ended_time - start_time}') 参考 ffmpeg合并多个MP4视频 ffmpeg 命令详解","link":"/2021/11/04/2021/11/2021-11-04-combine-video-by-ffmpeg/"},{"title":"My 2021","text":"Overview这篇文章涉及了比较多别的信息，感觉不太好，所以隐藏了。","link":"/2021/12/26/2021/12/2021-12-27-My-2021/"},{"title":"LuNiao Town Outing","text":"Overview前两天的一个晚上，有小伙伴想要出去走走，晒晒太阳。 我想到之前有友人在朋友圈发过去鸬鸟镇玩的状态，查了下路程，便提议可以去那边玩，一拍即合。 鸬鸟镇鸬鸟镇在杭州余杭的西北方向，靠近安吉，与径山很近。 洪校家庭农场这次说走就走的旅程，没有预定饭馆和去玩的地方，直接先导航到之前友人去过的一个农家乐。 到那边的时候，已经是中午 12 点了，便在那里解决了中饭，人均还是有点贵的，味道也一般吧，150 一份的鸡汤成了一个梗。 农场里有桃子和李子树，当季还可以采摘，不过现在过去就只是一片萧条了。 话说农场里的狗狗真的很热情，旁边还有四岭水库延伸的溪水，夏天来玩水应该不错。 吃饭的时候，看了看周边可以玩的地方。最近的就是山沟沟了，听老板说的时候还以为是随便取的名字，结果还真是一个山沟沟景区。 山沟沟景区有两个主要的景点-汤坑和茅塘，门票在携程上提前定的话成人 45 左右，如果是余杭人，有余杭市民卡就可以免费（不确定公告上说的身份证或者户口本会不会真的查，哈哈）。 此外还有额外的“玻璃桥”和“魔毯”消费项目。 我们想着先上去看看，就没有先买票，并且当天预定的话优惠就不大了。 茅塘我们直接驱车去了茅塘，没有发现所谓的古村落的入口，反而误入了村子里的竹林。 路过一群 150 之后，我们按一个大妈的指引，穿着竹林向上爬，中途我们迟疑了几次，怕上面没有别的路最后还是要回头，但最终还是让我们找到了新的路走了出去。​ 沿途粗壮的竹子和散落的竹叶子，给我们带来了很多新奇的感觉，也想到了武侠片中常见的竹林场景。 出了竹林，有一段水泥路，应该是村里人上山作业用的。在往下走的时候，还遇到了大叔在收集竹叶，一时挡住了我们的去路。 走下了山路，倒是看到了一处禁止爬山的告示。 回到了停车的村子，偶遇了几只喵喵，有只脏脏的白喵真的让我们笑的停不下来，哈哈哈。 彩虹谷结束之后，已经有点晚了，我们便驱车离开去彩虹谷，路上遇到了晾晒竹条的场地，刚开始还以为是长面条，还说买回去吃，太搞笑了。 彩虹谷比我们想像的要小很多，路上一个向一位大爷问路，结果老大爷一直说这里没啥东西都是吹出来的，于是我们便没有兴致，改去找草莓大棚，看看有没有机会可以自己采摘。 长桥路上遇到了一座有点年头的长桥，横亘在水库上。 草莓最终我们找到了一家草莓大棚，但还是不支持采摘。据老板说今年太冷，草莓到现在都还没有完全成熟，要延后了，据说亏了很多。他家的草莓的确挺大挺甜的。 微辣买了些草莓之后，我们便开车回杭州，去了小伙伴推荐的衢州菜，饭馆的名字就叫微辣，味道真的不错。用了优惠套餐券之后，性价比超高！ 总结这次临时起意，没有很好的攻略，不过反倒有些意外的惊喜，玩的很开心！ 下了高速之后，沿途的风景真的很美，青山绿水，别野村庄。 短途的旅程总是很短暂，半天的时间也还是有些紧，下次可以再早些出发，就更完美啦。","link":"/2022/01/01/2022/01/2022-01-01-LuNiang-Outing/"},{"title":"API Tool V1","text":"Overview前段时间Leader有提到我们测试API的方式，有没有适合Business的人用的。 一般来说，除了写代码之外，常用的就是Postman和Soup-UI。 Postman界面更友好一些，不过不太适合写自动化的Case，而Soup-UI则更专业一些，也是我之前常用的工具。 但如果给Business的人用的话，以上都需要安装本地软件，还要帮忙配置环境，需要比较多的学习成本。 像Swagger也是可以直接用，只要用户准备好json就行，但和Postman类似，数据都需要本地维护好。 所以今天我尝试初步写了一个小程序，可以方便点进行手动和自动call api. 这次主要用到的库： 123456789101112131415161718certifi==2021.10.8charset-normalizer==2.0.12colorama==0.4.4colorlog==6.6.0et-xmlfile==1.1.0idna==3.3numpy==1.22.2openpyxl==3.0.9pandas==1.4.1python-dateutil==2.8.2pytz==2021.3pywebio==1.5.2requests==2.27.1six==1.16.0tornado==6.1ua-parser==0.10.0urllib3==1.26.8user-agents==2.2.0 API Mock up首先我在本地用flask写了几个简单的api来用来测试。 第三个api模拟404， 第四个为了测试url中带参数的情况。 1234/sample/api01/sample/api02/sample/api03_error/sample/api04 1234567891011121314151617181920212223242526272829from flask import Flask, jsonifyfrom flask import requestapp = Flask(__name__)@app.route('/sample/api01', methods=['POST'])def api01(): print(request.json) return jsonify({'data': {'api-name': 'api01'}})@app.route('/sample/api02', methods=['POST'])def api02(): return jsonify({'data': {'api-name': 'api02'}})@app.route('/sample/api03_error', methods=['POST'])def api03(): return jsonify({'data': {'api-name': 'api03'}})@app.route('/sample/api04', methods=['GET','POST'])def api04(): page = request.args['page'] return jsonify({'data': {'api-name': f'api04-page{page}'}})if __name__ == '__main__': app.run(debug=True, port=8888) Sample Excel参考之前SSDD自动化项目中的API数据文档，我准备了一个Sample文件如下： 上面这个是准备的测试数据，Request Template是对应的sheet-API Template中的具体的API的信息，因为API是复用的，而不是写死在这里。 Response Code是期望的返回的Http Code，目前我只对比了这个来判断是否call api成功，针对测试的话后期可以增加别的判定，比如返回的数据。 {asOfDate} {eventName} {page} 这三个写大括号的，是到时URL和Request Body里需要替换的值。 如果你有新的别的值需要替换，就可以直接加一列就好了，在需要的地方。 上面的图是API的定义，字段的含义还是比较明显的。 可以看到request body中都使用了Testcase中的参数。在template的URL中，{page}的值也是从TestCase那边填充进去的。 Config这边就可以配置一些公用的变量，后面看需要可以增加，代码也要修改。 像Headers这边，只要按json字符串的格式添加就可以了。 核心代码下面是主要的文档处理和Call API的代码，写在同一个包中。 定义了几个类来存Excel中的数据。APIProcessor是这边的主入口，只要将符合模板的的Excel的IO对象传进来就可以。 不过为了前端的展示，还是揉合了一些pywebio的代码，后面看看怎么抽离出来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162import jsonimport loggingimport timeimport pandas as pdimport requests# from cmutils.log import loggerfrom pywebio.output import put_loading, put_info, put_warning, put_error, put_success, put_table, put_link, put_tabs, \\ put_markdown, put_code, put_textfrom cmutils.log import Loggerfrom utils import loggerTCID = 'TCID'DESCRIPTION = 'Description'REQUEST_TEMPLATE = 'Request Template'REQUEST_METHOD = 'Request Method'RESPONSE_CODE = 'Response Code'REQUEST_BODY = 'Request Body'URL = 'URL'class APITemplate: def __init__(self, request_template, request_method, url, request_body): self.request_template = request_template self.request_method = request_method self.url = url self.request_body = request_bodyclass APITestCase: MANDATORY_FIELDS = [TCID, DESCRIPTION, REQUEST_TEMPLATE, RESPONSE_CODE] def __init__(self, tcid, description, request_template, response_code, parameter_map): self.tcid = tcid self.description = description self.request_template = request_template self.response_code = response_code self.parameter_map = parameter_map # print(parameter_map)class APIRequest: env = '' host = 'localhost' headers = {'Content-Type': 'application/json'} params = {} POST = 'POST' GET = 'GET' def __init__(self, env, host, headers={}, params={}): self.env = env self.host = host headers = json.loads(headers) if type(headers) == str else headers params = json.loads(params) if type(params) == str else params self.headers = {**self.headers, **headers} self.params = {**self.params, **params} def call(self, url, method, data, headers={}, params={}): headers = json.loads(headers) if type(headers) == str else headers params = json.loads(params) if type(params) == str else params data = data if type(data) == str else json.dumps(data) methods = { self.GET: self.get, self.POST: self.post, } logger.info(''.join([url, '&gt;', method, '&gt;', str(data), '&gt;', str(headers), '&gt;', str(params)])) return methods[method](url, data, headers, params) def post(self, url, data, headers=None, params=None): # requests.post('https://api.github.com/some/endpoint', data=json.dumps({'some': 'data'})) print(f'{self.host}{url}') return requests.post(f'{self.host}{url}', data=data, headers=headers, params=params) def get(self, url, data, headers={}, params={}): headers = headers if headers else self.headers params = params if params else self.params return requests.get(f'{self.host}{url}', data=data, headers=headers, params=params)def prepare_request_data(request_body, parameter_map): for key, value in parameter_map.items(): request_body = str(request_body).replace(str(key), str(value)) return request_bodydef put_result_tab(tcid, description, request_data, response_data, error_info=None): tabs = [] tabs.append({'title': 'TCID', 'content': tcid}) tabs.append({'title': 'Description', 'content': description}) tabs.append({'title': 'Request data', 'content': put_code(request_data, language='json')}) tabs.append({'title': 'Response data', 'content': put_code(response_data, language='json')}) if error_info: tabs.append({'title': 'Markdown', 'content': put_text(error_info).style('color:red')}) else: pass put_tabs(tabs=tabs)class APIProcessor: def __init__(self, io_excel): # io = pd.io.excel.ExcelFile('./sample.xlsx') self.df_test_cases = pd.read_excel(io_excel, sheet_name='Test Cases', dtype=str) self.df_api_template = pd.read_excel(io_excel, sheet_name='API Template', dtype=str) self.df_config = pd.read_excel(io_excel, sheet_name='Config', dtype=str) self.df_test_cases = self.df_test_cases.where(self.df_test_cases.notnull(), '') self.df_api_template = self.df_api_template.where(self.df_api_template.notnull(), '') self.df_config = self.df_config.where(self.df_config.notnull(), '') self.config_map = {} self.test_case_list = [] self.api_template_map = {} for index, row in self.df_api_template.iterrows(): # print(row) template = APITemplate(row[REQUEST_TEMPLATE], row[REQUEST_METHOD], row[URL], row[REQUEST_BODY]) self.api_template_map[row[REQUEST_TEMPLATE]] = template for index, row in self.df_test_cases.iterrows(): # print(index, '---', row) # print(row.keys()) parameter_map = {} for key in row.keys(): if key not in APITestCase.MANDATORY_FIELDS: parameter_map[key] = row[key] testcase = APITestCase(row[TCID], row[DESCRIPTION], row[REQUEST_TEMPLATE], row[RESPONSE_CODE], parameter_map) self.test_case_list.append(testcase) for index, row in self.df_config.iterrows(): self.config_map[row['Key']] = row['Value'] def run_apis(self): api_request = APIRequest(self.config_map['Environment'], self.config_map['Host'], self.config_map['Headers']) for test_case in self.test_case_list: test_case.request_template template_name = test_case.request_template template = self.api_template_map[template_name] request_data = prepare_request_data(template.request_body, test_case.parameter_map) request_url = prepare_request_data(template.url, test_case.parameter_map) response = api_request.call(request_url, template.request_method, request_data) response_json = response.text logger.info(response.status_code) if str(response.status_code) == str(test_case.response_code): logger.info('Post Successfully!') # put_info('this is info message!') # put_warning('this is warning message!') put_success(f&quot;{template.request_method} {self.config_map['Host']}{request_url} {response.status_code}&quot;) put_result_tab(test_case.tcid, test_case.description, request_data, response_json) else: logger.error('Post Failed!') put_error(f&quot;{template.request_method} {self.config_map['Host']}{request_url} {response.status_code}&quot;) put_result_tab(test_case.tcid, test_case.description, request_data, response_json) time.sleep(1) # assert() # exit() 前端入口使用pywebio来写了简单的页面，主要用来上传需要跑的测试文件。 1234567891011121314151617181920212223242526272829303132import osimport pywebioimport pywebio.output as outputimport pywebio.input as inputfrom api_element import APIProcessorcurrent_workspace = os.path.join(os.path.dirname(os.path.abspath(__file__)))def main(): output.put_markdown('# API Tool') output.put_markdown('Features：') output.put_markdown(&quot;&quot;&quot;- Choose api test cases excel- Call the apis automatically &quot;&quot;&quot;) content = open(os.path.join(current_workspace, 'sample.xlsx'), 'rb').read() output.put_file(name='template.xlsx', content=content, label='Download Template File') file = input.file_upload('Choose an excel file', '.xlsx') # df_config = pd.read_excel(file['content'], sheet_name='Config', dtype=str) # output.put_html(df_config.head(10).to_html()) with output.put_loading(shape='border', color='dark'): api_processor = APIProcessor(file['content']) api_processor.run_apis()if __name__ == '__main__': pywebio.start_server(main, port=8080, debug=True, cdn=False, auto_open_webbrowser=True) Demo下面是最后的效果，每个API之间sleep了一秒。 Project Src","link":"/2022/02/27/2022/02/2022-02-27-APITool-V1/"},{"title":"最近的一些思考","text":"Thinking最近的心态总算恢复了一些了,大概6、7成吧，但有时候想到一些事还是很难受。 去年真的发生太多的事了。 去年出事了之后，那几个月心态已经不好了，但我以为自己没事，因为上班的时候我觉得自己感觉还是挺好的。 那一两个月没睡好觉，不知道怎么面对这件事，晚上很难受痛苦，反而白天进入了工作状态就还好。 回想起来，一直是工作在治愈我，让我不用想太多的事情。 几十万可能对有些人不算多少钱吧，但对我来说太伤了，最主要是我也是直接亲历者，从没想过这咱种事会发生在自己身边。但最后我还是决定共同面对，还是不忍心，难道真的不管了么。 不过去年心态崩了之后，以前不开心的事也都涌上心头，我反而见怪不怪了吧。 记得那年高三，从来不舍得花钱的爸妈给我买了一个手机，真的也是像电影情节似的。后面的剧情也很俗套，就是沉迷小说，消耗了精力，虽然时间不长，但是让自己心散了，再加上自责与焦虑，直接心态崩了，完成没办法学习。人生因此直接改变了，尤其是埋下了心态崩溃的种子和丧失了十多年以来，专注学习的能力。哈哈哈，不想了，那时真的是哭笑不得。 后来上了大学，工作，我都和自己说，你了解你自己的，不要给自己大太的压力，好好做一份工作。 尤其是14年回了道富之后，我对自己说有啥干啥，尤其不要自己换组了，你永远不知道“下一颗糖是什么味道“，但我还是没做到。 还有和EX在一起没几个月，父亲就重病了，这期间家人还在催我结婚，我就觉得很可笑，让我怎么说得出口，谁知道情况父亲的病情需要投入多少下去。我心里还是觉得有些愧疚，是我不够好，后面两个人的生活轨迹已经不一样了，她可以拥有更好的生活，更美好的未来，谁不是父母的宝贝呢。谈恋爱还是要趁早，与爱人相处也是要学习的，人只有交往的过程中，才能认清自己，知道自己的缺点，学会爱人。 我发现每次生活出事了的时候，我就会想要改变，然后自己的心态出问题，一切都回不到从前了。 那一年我就想着，自己是不是还可以拼一下，在自己能力范围内多赚点钱，于是我就又忘记了自己说过的话，呆得还好的时候，不要乱动。 曾经我以为自己换了工作后悔是因为自己选择的原因，到现在我才明白，还是要看自己适不适合。 我需要工作来治愈自己，生活已经狼狈不堪，只有工作才能让我找到自己的价值，让我忙碌起来。 工作中我需要和同事的交流，我需要知道自己在做什么，让自己觉得是在做有意义的事。 你会离开自己呆着开心的项目组么，你会和自己喜欢的人说分手么。我会。 就像小时候有人问我，你喜欢学习么，我说喜欢。那你喜欢打游戏么，不喜欢。 有人对我说，人要为自己而活，自己怎么舒服怎么来，不要管别人怎么想。 就像高中和大学的时候，我真的觉得当班长没什么好处，应该要关注在好好学习。 但是当别人要我当的时候，我总是不会拒绝，想着为同学和老师服务吧，最后又有时候耽误了自己的事。 说实话，我会为自己曾经的一些选择而后悔和遗憾，但是生活总是向前看的。我从来不后悔离开那个组，因为我自由了，我不用再考虑校友，不用考虑自己的行为会不会让他为难，不用再忍着那些SB了。如果正常的情况下，对于SB我他妈直接就怼了。的确，当初我刚去的时候，应该不用管他的，觉得不合适，不爽就直接可以走了，可能就没有后面的事了。 我人生中有很多次刷新三观的时候。我有一个“秘密”从来没有和别人说过，我本来以为自己早就忘记了，但是去年心态崩了的时候又想起来了。中学时的一个学科竞赛的时候，老师竟然让我们几个人“作弊”。我一开始不会拒绝，但是心里一直很不安，觉得这是不对的事情。最后我还是没有干，最后被说了。我那时候起就对自己说，这辈子不要去干违背自己良心的事了。 我对别人比对自己宽容，这也是我又做回测试的一个原因，虽然这个想法不一定对。 我去年本来就想给生活做减法的，结果发生了那么多的事。。。。。。 但是世事难料呀，就像当年刚买了车的时候，还是临牌，去滨江找朋友玩，结果在小巷子里不小心碰到了自行车横在道上的屁股,一个女生受伤了。那也是我第一次处理这种事，我让自己冷静下来，让EX先去朋友家，自己陪着伤者去医院拍片，虽然其实就脚那边一点擦伤，但是女生的对象还是要求去检查骨头。也是应该的吧，最后也为了省事，还是给了200块私了，其实我这样处理还是有点风险的，现在肯定不会这么干了。第二天我自己去修车，走在4S店外面的路上去找公交回家，还是忍不住情绪崩溃了，因为车的事，因为父亲的事，因为种种。有时候还是会想，买个小破车还要贷款，当年如果早点买了房子的话，生活是不是就完全不一样了。当年和同学去看房子，给父母打电话，我刚表达是不是有可能想办法买个房子的，他们就直接又是老一套说辞，好好工作，过几年就可以买了。父母有他们的局限性，我也有，有些事还是要看开。再难过,过几分钟就好了，生活总是这样的。 有人说我对工作看的太重了，或许吧，我只是想把工作做好，工作真的能使人快乐，而不应该是痛苦。 没有办法，我的人生只能一步步试错，一步步改正，一步步和解。 如果人真的是上帝创造的人工智能的话，每个人的硬件都是天生的，而软件（性格）却是与原生家庭息息相关。我知道自己有很多bug，但我已经很努力的在自我改良和适应了。 我知道问心无愧很难，但有时候还是要保持初心，我不想别人因为我而受到伤害。 现在最重要的，就是做好自己现在的工作，调整好自己的心态。生活总是一步步向前看的。加油吧！","link":"/2022/02/27/2022/02/2022-02-27-Recent-Thought/"},{"title":"Browser Screen Recording","text":"Overview前两天看到一个原生 JS 用浏览器录制屏幕的方法，挺有意思的，今天试着想把声音也录制下来，看了一天，还是没有解决，网上的方法都不对。。 核心代码主要是利用navigator.mediaDevices.getDisplayMedia来实现视频录制。 尝试过使用navigator.mediaDevices.getUserMedia来获取音频（麦克风），但是暂时没有办法和视频合在一起。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Recording&lt;/title&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;style&gt; body { font-family: Arial; margin: 4vh auto; width: 90vw; max-width: 600px; text-align: center; } #controls { text-align: center; } .record-btn { margin: 10px 5px; padding: 15px; background-color: #2bcbba; border: none; color: white; font-weight: bold; border-radius: 6px; outline: none; font-size: 1.2em; width: 120px; height: 50px; } .record-btn:hover { background-color: #26de81; cursor: hand; } .record-btn:disabled { background-color: #2bcbba80; } #stop { background-color: #fc5c65; } #video { margin-top: 10px; margin-bottom: 20px; border: 12px solid #a5adb0; border-radius: 15px; outline: none; width: 100%; height: 400px; background-color: black; } h1 { color: #2bcbba; letter-spacing: -2.5px; line-height: 30px; } .created { color: lightgrey; letter-spacing: -0.7px; font-size: 1em; margin-top: 40px; } .created &gt; a { color: #4b7bec; text-decoration: none; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;&lt;u style=&quot;color:#fc5c65&quot;&gt;Recording&lt;/u&gt;&lt;/h1&gt; &lt;video class=&quot;video&quot; width=&quot;600px&quot; controls&gt;&lt;/video&gt; &lt;button class=&quot;record-btn&quot;&gt;record&lt;/button&gt; &lt;!-- &lt;script src=&quot;./index.js&quot;&gt;&lt;/script&gt; --&gt; &lt;/body&gt; &lt;script&gt; let btn = document.querySelector(&quot;.record-btn&quot;); btn.addEventListener(&quot;click&quot;, async function () { // let audioStream = await navigator.mediaDevices.getUserMedia({ // // video: { width: 1280, height: 720 }, // audio: true // }); let videoStream = await navigator.mediaDevices.getDisplayMedia({ video: true, // audio: true, //not support cursor: &quot;always&quot;, }); // videoStream.addTrack(audioStream.getAudioTracks()[0]); const mime = MediaRecorder.isTypeSupported(&quot;video/webm; codecs=vp9&quot;) ? &quot;video/webm; codecs=vp9&quot; : &quot;video/webm&quot;; let mediaRecorder = new MediaRecorder(videoStream, { mimeType: mime, }); let chunks = []; mediaRecorder.addEventListener(&quot;dataavailable&quot;, function (e) { console.log(&quot;dataavailable&quot;, e.data); chunks.push(e.data); }); mediaRecorder.addEventListener(&quot;stop&quot;, function () { console.log(&quot;stop&quot;); let blob = new Blob(chunks, { type: chunks[0].type, //,&quot;video/mp4&quot; }); let url = URL.createObjectURL(blob); let video = document.querySelector(&quot;video&quot;); video.src = url; let a = document.createElement(&quot;a&quot;); a.href = url; a.download = &quot;video.webm&quot;; //video.mp4 a.click(); }); mediaRecorder.start(); }); &lt;/script&gt;&lt;/html&gt; Demo 参考用JS创建一个录屏功能用网页来录制视频","link":"/2022/03/05/2022/03/2022-03-05-Browser-Screen-Record/"},{"title":"Merge CSV to Excel","text":"Overview周五老板说有没有用python把csv合并到excel中的例子，用python还是比较方便的。 这次主要用到的库： 12345678et-xmlfile==1.1.0numpy==1.22.2openpyxl==3.0.9pandas==1.4.1python-dateutil==2.8.2pytz==2021.3six==1.16.0XlsxWriter==3.0.3 配置文件使用配置文件config.ini来管理，可以支持目录，分隔符，有没有带headerr，和输出文件的配置。 12345678910111213141516171819[config];folder store csvsfolder=./data-normal/;separatorsep=,;csv with header: 1, no header: 0header=1;output header0output=./output.normal.xlsx;folder=./data-noheader/;sep=;;header=0;output=./output.noheader.xlsx; folder=./data-sep-tab/; sep=\\t; header=0; output=./output.tab.xlsx 代码下面是主要的代码: 123456789101112131415161718192021222324252627282930313233343536373839404142import configparserimport globimport osimport pandas as pdpd.io.formats.excel.ExcelFormatter.header_style = NoneCONFIG_PATH = './config.ini'class Config: def __init__(self): cf = configparser.ConfigParser() cf.read(CONFIG_PATH) self.folder = cf.get(&quot;config&quot;, &quot;folder&quot;) self.sep = cf.get(&quot;config&quot;, &quot;sep&quot;) self.header = True if int(cf.get(&quot;config&quot;, &quot;header&quot;)) == 1 else False self.output = cf.get(&quot;config&quot;, &quot;output&quot;)def read_csvs(config): folder = config.folder csv_file_paths = glob.glob(f&quot;{folder}/*.csv&quot;, recursive=True) if not csv_file_paths: print('No csv files found, please check the config!') exit() with pd.ExcelWriter(config.output, engine=&quot;openpyxl&quot;) as writer: for csv_file_path in csv_file_paths: print(f'reading {csv_file_path}') df = pd.read_csv(csv_file_path, sep=config.sep, engine='python', header=0 if config.header else None) filename = os.path.splitext(os.path.basename(csv_file_path))[0] df.to_excel(writer, sheet_name=filename, index=None, header=True if config.header else None) print(f'write to {config.output} done')if __name__ == '__main__': config = Config() read_csvs(config) 其中有几个细节，pandas默认输出的excel里header是加粗并且带线的，如果想要去掉就需要配置一下 1pd.io.formats.excel.ExcelFormatter.header_style = None 默认的方法是不支持\\t这样的分隔符来表示的，需要加上engine=&quot;openpyxl&quot;。 12with pd.ExcelWriter(config.output, engine=&quot;openpyxl&quot;) as writer: pass Demo下面是简单的演示 Merge CSV to Excel演示地址 源码merge-csv-to-excel.zip 参考详解pandas的read_csv方法Pands Input/output","link":"/2022/03/06/2022/03/2022-03-06-merge-csv-to-excel/"}],"tags":[{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"network","slug":"network","link":"/tags/network/"},{"name":"vmware","slug":"vmware","link":"/tags/vmware/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"big data","slug":"big-data","link":"/tags/big-data/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"csv","slug":"csv","link":"/tags/csv/"},{"name":"pytz","slug":"pytz","link":"/tags/pytz/"},{"name":"openpyxl","slug":"openpyxl","link":"/tags/openpyxl/"},{"name":"excel","slug":"excel","link":"/tags/excel/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"tkinter","slug":"tkinter","link":"/tags/tkinter/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"db","slug":"db","link":"/tags/db/"},{"name":"bigdata","slug":"bigdata","link":"/tags/bigdata/"},{"name":"private","slug":"private","link":"/tags/private/"},{"name":"dairy","slug":"dairy","link":"/tags/dairy/"},{"name":"diary","slug":"diary","link":"/tags/diary/"},{"name":"2019","slug":"2019","link":"/tags/2019/"},{"name":"pyodbc","slug":"pyodbc","link":"/tags/pyodbc/"},{"name":"unicode","slug":"unicode","link":"/tags/unicode/"},{"name":"regex","slug":"regex","link":"/tags/regex/"},{"name":"acm","slug":"acm","link":"/tags/acm/"},{"name":"log","slug":"log","link":"/tags/log/"},{"name":"performance","slug":"performance","link":"/tags/performance/"},{"name":"test","slug":"test","link":"/tags/test/"},{"name":"goal","slug":"goal","link":"/tags/goal/"},{"name":"monitor","slug":"monitor","link":"/tags/monitor/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"txt","slug":"txt","link":"/tags/txt/"},{"name":"console","slug":"console","link":"/tags/console/"},{"name":"cpu","slug":"cpu","link":"/tags/cpu/"},{"name":"memory","slug":"memory","link":"/tags/memory/"},{"name":"colorlog","slug":"colorlog","link":"/tags/colorlog/"},{"name":"chartjs","slug":"chartjs","link":"/tags/chartjs/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"chart","slug":"chart","link":"/tags/chart/"},{"name":"tasklist","slug":"tasklist","link":"/tags/tasklist/"},{"name":"powershell","slug":"powershell","link":"/tags/powershell/"},{"name":"email","slug":"email","link":"/tags/email/"},{"name":"training","slug":"training","link":"/tags/training/"},{"name":"spider","slug":"spider","link":"/tags/spider/"},{"name":"data structure","slug":"data-structure","link":"/tags/data-structure/"},{"name":"io","slug":"io","link":"/tags/io/"},{"name":"file","slug":"file","link":"/tags/file/"},{"name":"class","slug":"class","link":"/tags/class/"},{"name":"RESTful","slug":"RESTful","link":"/tags/RESTful/"},{"name":"flask-restful","slug":"flask-restful","link":"/tags/flask-restful/"},{"name":"progress bar","slug":"progress-bar","link":"/tags/progress-bar/"},{"name":"xpath","slug":"xpath","link":"/tags/xpath/"},{"name":"apidoc","slug":"apidoc","link":"/tags/apidoc/"},{"name":"yaml","slug":"yaml","link":"/tags/yaml/"},{"name":"config","slug":"config","link":"/tags/config/"},{"name":"dokuwiki","slug":"dokuwiki","link":"/tags/dokuwiki/"},{"name":"php","slug":"php","link":"/tags/php/"},{"name":"httpd","slug":"httpd","link":"/tags/httpd/"},{"name":"design patterns","slug":"design-patterns","link":"/tags/design-patterns/"},{"name":"proxy","slug":"proxy","link":"/tags/proxy/"},{"name":"swagger","slug":"swagger","link":"/tags/swagger/"},{"name":"gRPC","slug":"gRPC","link":"/tags/gRPC/"},{"name":"RPC","slug":"RPC","link":"/tags/RPC/"},{"name":"springboot","slug":"springboot","link":"/tags/springboot/"},{"name":"algorithms","slug":"algorithms","link":"/tags/algorithms/"},{"name":"pywin32","slug":"pywin32","link":"/tags/pywin32/"},{"name":"imageio","slug":"imageio","link":"/tags/imageio/"},{"name":"PIL","slug":"PIL","link":"/tags/PIL/"},{"name":"database","slug":"database","link":"/tags/database/"},{"name":"E-R","slug":"E-R","link":"/tags/E-R/"},{"name":"work","slug":"work","link":"/tags/work/"},{"name":"json","slug":"json","link":"/tags/json/"},{"name":"annotation","slug":"annotation","link":"/tags/annotation/"},{"name":"reflect","slug":"reflect","link":"/tags/reflect/"},{"name":"thread","slug":"thread","link":"/tags/thread/"},{"name":"lock","slug":"lock","link":"/tags/lock/"},{"name":"download","slug":"download","link":"/tags/download/"},{"name":"sqlalchemy","slug":"sqlalchemy","link":"/tags/sqlalchemy/"},{"name":"orm","slug":"orm","link":"/tags/orm/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Diary","slug":"Diary","link":"/tags/Diary/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","link":"/tags/HDFS/"},{"name":"moviepy","slug":"moviepy","link":"/tags/moviepy/"},{"name":"tiktok","slug":"tiktok","link":"/tags/tiktok/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"jdbc","slug":"jdbc","link":"/tags/jdbc/"},{"name":"life","slug":"life","link":"/tags/life/"},{"name":"box","slug":"box","link":"/tags/box/"},{"name":"sdk","slug":"sdk","link":"/tags/sdk/"},{"name":"data analysis","slug":"data-analysis","link":"/tags/data-analysis/"},{"name":"numpy","slug":"numpy","link":"/tags/numpy/"},{"name":"matplotlib","slug":"matplotlib","link":"/tags/matplotlib/"},{"name":"manual","slug":"manual","link":"/tags/manual/"},{"name":"alibaba","slug":"alibaba","link":"/tags/alibaba/"},{"name":"node.js","slug":"node-js","link":"/tags/node-js/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Apache","slug":"Apache","link":"/tags/Apache/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"audio","slug":"audio","link":"/tags/audio/"},{"name":"vedio","slug":"vedio","link":"/tags/vedio/"},{"name":"mkdocs","slug":"mkdocs","link":"/tags/mkdocs/"},{"name":"ftp","slug":"ftp","link":"/tags/ftp/"},{"name":"pyftpdlib","slug":"pyftpdlib","link":"/tags/pyftpdlib/"},{"name":"ffmpeg","slug":"ffmpeg","link":"/tags/ffmpeg/"},{"name":"video","slug":"video","link":"/tags/video/"},{"name":"pywebio","slug":"pywebio","link":"/tags/pywebio/"},{"name":"getDisplayMedia","slug":"getDisplayMedia","link":"/tags/getDisplayMedia/"},{"name":"getUserMedia","slug":"getUserMedia","link":"/tags/getUserMedia/"},{"name":"merge","slug":"merge","link":"/tags/merge/"}],"categories":[{"name":"Learn","slug":"Learn","link":"/categories/Learn/"},{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"Programs","slug":"Programs","link":"/categories/Programs/"},{"name":"Diary","slug":"Diary","link":"/categories/Diary/"},{"name":"Knowledge","slug":"Knowledge","link":"/categories/Knowledge/"}]}